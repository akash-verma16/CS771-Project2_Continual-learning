{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T16:25:57.028743Z",
     "iopub.status.busy": "2024-11-26T16:25:57.028296Z",
     "iopub.status.idle": "2024-11-26T17:45:09.193751Z",
     "shell.execute_reply": "2024-11-26T17:45:09.192686Z",
     "shell.execute_reply.started": "2024-11-26T16:25:57.028691Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reached_path_to_datasets'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/1878221273.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_dict = torch.load(pth_path)\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 173MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'started'}\n",
      "{0}\n",
      "{1}\n",
      "{2}\n",
      "{3}\n",
      "{4}\n",
      "{5}\n",
      "{6}\n",
      "{7}\n",
      "{8}\n",
      "{9}\n",
      "{10}\n",
      "{11}\n",
      "{12}\n",
      "{13}\n",
      "{14}\n",
      "{15}\n",
      "{16}\n",
      "Saved initial model and prototypes.\n",
      "{91.52}\n",
      "hello\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Saved model after self-training on dataset 2.\n",
      "Prototypes updated using weighted average.\n",
      "started_accuracy_computation\n",
      "{92.2}\n",
      "{91.24}\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Saved model after self-training on dataset 3.\n",
      "Prototypes updated using weighted average.\n",
      "started_accuracy_computation\n",
      "{91.72}\n",
      "{91.56}\n",
      "{90.84}\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Saved model after self-training on dataset 4.\n",
      "Prototypes updated using weighted average.\n",
      "started_accuracy_computation\n",
      "{92.0}\n",
      "{91.0}\n",
      "{90.76}\n",
      "{90.96}\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Saved model after self-training on dataset 5.\n",
      "Prototypes updated using weighted average.\n",
      "started_accuracy_computation\n",
      "{91.92}\n",
      "{90.92}\n",
      "{90.84}\n",
      "{91.04}\n",
      "{90.76}\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Saved model after self-training on dataset 6.\n",
      "Prototypes updated using weighted average.\n",
      "started_accuracy_computation\n",
      "{91.88}\n",
      "{91.0}\n",
      "{90.84}\n",
      "{91.08}\n",
      "{90.4}\n",
      "{91.12}\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Saved model after self-training on dataset 7.\n",
      "Prototypes updated using weighted average.\n",
      "started_accuracy_computation\n",
      "{91.76}\n",
      "{90.92}\n",
      "{91.0}\n",
      "{90.76}\n",
      "{90.52}\n",
      "{91.16}\n",
      "{90.12}\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Saved model after self-training on dataset 8.\n",
      "Prototypes updated using weighted average.\n",
      "started_accuracy_computation\n",
      "{92.32}\n",
      "{91.2}\n",
      "{91.04}\n",
      "{91.12}\n",
      "{90.52}\n",
      "{91.36}\n",
      "{90.24}\n",
      "{91.76}\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Saved model after self-training on dataset 9.\n",
      "Prototypes updated using weighted average.\n",
      "started_accuracy_computation\n",
      "{92.24}\n",
      "{90.96}\n",
      "{90.68}\n",
      "{91.32}\n",
      "{90.48}\n",
      "{91.36}\n",
      "{90.92}\n",
      "{91.64}\n",
      "{90.36}\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Saved model after self-training on dataset 10.\n",
      "Prototypes updated using weighted average.\n",
      "started_accuracy_computation\n",
      "{92.08}\n",
      "{91.4}\n",
      "{91.48}\n",
      "{91.04}\n",
      "{91.0}\n",
      "{91.76}\n",
      "{90.92}\n",
      "{92.32}\n",
      "{90.8}\n",
      "{91.48}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import copy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "feature_dir = '/kaggle/working/files'\n",
    "os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Distillation Loss Function\n",
    "def distillation_loss(student_outputs, teacher_outputs, temperature=2.0):\n",
    "    teacher_probs = F.softmax(teacher_outputs / temperature, dim=1)\n",
    "    student_probs = F.log_softmax(student_outputs / temperature, dim=1)\n",
    "    return F.kl_div(student_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, targets=None, transform=None):\n",
    "        self.images = images\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        target = self.targets[idx] if self.targets is not None else -1\n",
    "        return image, target\n",
    "\n",
    "    \n",
    "# Load Data from .pth file\n",
    "def load_data_from_pth(pth_path, has_targets=True):\n",
    "    data_dict = torch.load(pth_path)\n",
    "    data = data_dict['data']\n",
    "    targets = data_dict['targets'] if has_targets else None\n",
    "    return data, targets\n",
    "\n",
    "# Paths to Datasets\n",
    "#edit these paths to run on your machine\n",
    "d1_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/1_train_data.tar.pth'\n",
    "d1hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/1_eval_data.tar.pth'\n",
    "d2_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/2_train_data.tar.pth'\n",
    "d2hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/2_eval_data.tar.pth'\n",
    "d3_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/3_train_data.tar.pth'\n",
    "d3hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/3_eval_data.tar.pth'\n",
    "\n",
    "d4_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/4_train_data.tar.pth'\n",
    "d4hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/4_eval_data.tar.pth'\n",
    "\n",
    "d5_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/5_train_data.tar.pth'\n",
    "d5hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/5_eval_data.tar.pth'\n",
    "\n",
    "d6_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/6_train_data.tar.pth'\n",
    "d6hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/6_eval_data.tar.pth'\n",
    "\n",
    "d7_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/7_train_data.tar.pth'\n",
    "d7hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/7_eval_data.tar.pth'\n",
    "\n",
    "d8_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/8_train_data.tar.pth'\n",
    "d8hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/8_eval_data.tar.pth'\n",
    "\n",
    "d9_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/9_train_data.tar.pth'\n",
    "d9hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/9_eval_data.tar.pth'\n",
    "\n",
    "d10_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/10_train_data.tar.pth'\n",
    "d10hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/10_eval_data.tar.pth'\n",
    "\n",
    "d11_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/1_train_data.tar.pth'\n",
    "d11hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/1_eval_data.tar.pth'\n",
    "\n",
    "d12_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/2_train_data.tar.pth'\n",
    "d12hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/2_eval_data.tar.pth'\n",
    "\n",
    "d13_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/3_train_data.tar.pth'\n",
    "d13hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/3_eval_data.tar.pth'\n",
    "\n",
    "d14_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/4_train_data.tar.pth'\n",
    "d14hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/4_eval_data.tar.pth'\n",
    "\n",
    "d15_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/5_train_data.tar.pth'\n",
    "d15hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/5_eval_data.tar.pth'\n",
    "\n",
    "d16_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/6_train_data.tar.pth'\n",
    "d16hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/6_eval_data.tar.pth'\n",
    "\n",
    "d17_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/7_train_data.tar.pth'\n",
    "d17hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/7_eval_data.tar.pth'\n",
    "\n",
    "d18_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/8_train_data.tar.pth'\n",
    "d18hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/8_eval_data.tar.pth'\n",
    "\n",
    "d19_path = '//kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/9_train_data.tar.pth'\n",
    "d19hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/9_eval_data.tar.pth'\n",
    "\n",
    "d20_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/10_train_data.tar.pth'\n",
    "d20hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/10_eval_data.tar.pth'\n",
    "\n",
    "print({\"reached_path_to_datasets\"})\n",
    "\n",
    "# Load Datasets\n",
    "d1_data, d1_targets = load_data_from_pth(d1_path)\n",
    "d1hat_data, d1hat_targets = load_data_from_pth(d1hat_path)\n",
    "d2_data, _ = load_data_from_pth(d2_path, has_targets=False)\n",
    "d2hat_data, d2hat_targets = load_data_from_pth(d2hat_path)\n",
    "\n",
    "d3_data, _ = load_data_from_pth(d3_path, has_targets=False)\n",
    "d3hat_data, d3hat_targets = load_data_from_pth(d3hat_path)\n",
    "\n",
    "d4_data, _ = load_data_from_pth(d4_path, has_targets=False)\n",
    "d4hat_data, d4hat_targets = load_data_from_pth(d4hat_path)\n",
    "\n",
    "d5_data, _ = load_data_from_pth(d5_path, has_targets=False)\n",
    "d5hat_data, d5hat_targets = load_data_from_pth(d5hat_path)\n",
    "\n",
    "d6_data, _ = load_data_from_pth(d6_path, has_targets=False)\n",
    "d6hat_data, d6hat_targets = load_data_from_pth(d6hat_path)\n",
    "\n",
    "d7_data, _ = load_data_from_pth(d7_path, has_targets=False)\n",
    "d7hat_data, d7hat_targets = load_data_from_pth(d7hat_path)\n",
    "\n",
    "d8_data, _ = load_data_from_pth(d8_path, has_targets=False)\n",
    "d8hat_data, d8hat_targets = load_data_from_pth(d8hat_path)\n",
    "\n",
    "d9_data, _ = load_data_from_pth(d9_path, has_targets=False)\n",
    "d9hat_data, d9hat_targets = load_data_from_pth(d9hat_path)\n",
    "\n",
    "d10_data, _ = load_data_from_pth(d10_path, has_targets=False)\n",
    "d10hat_data, d10hat_targets = load_data_from_pth(d10hat_path)\n",
    "\n",
    "d11_data, _ = load_data_from_pth(d11_path, has_targets=False)\n",
    "d11hat_data, d11hat_targets = load_data_from_pth(d11hat_path)\n",
    "\n",
    "d12_data, _ = load_data_from_pth(d12_path, has_targets=False)\n",
    "d12hat_data, d12hat_targets = load_data_from_pth(d12hat_path)\n",
    "\n",
    "d13_data, _ = load_data_from_pth(d13_path, has_targets=False)\n",
    "d13hat_data, d13hat_targets = load_data_from_pth(d13hat_path)\n",
    "\n",
    "d14_data, _ = load_data_from_pth(d14_path, has_targets=False)\n",
    "d14hat_data, d14hat_targets = load_data_from_pth(d14hat_path)\n",
    "\n",
    "d15_data, _ = load_data_from_pth(d15_path, has_targets=False)\n",
    "d15hat_data, d15hat_targets = load_data_from_pth(d15hat_path)\n",
    "\n",
    "d16_data, _ = load_data_from_pth(d16_path, has_targets=False)\n",
    "d16hat_data, d16hat_targets = load_data_from_pth(d16hat_path)\n",
    "\n",
    "d17_data, _ = load_data_from_pth(d17_path, has_targets=False)\n",
    "d17hat_data, d17hat_targets = load_data_from_pth(d17hat_path)\n",
    "\n",
    "d18_data, _ = load_data_from_pth(d18_path, has_targets=False)\n",
    "d18hat_data, d18hat_targets = load_data_from_pth(d18hat_path)\n",
    "\n",
    "d19_data, _ = load_data_from_pth(d19_path, has_targets=False)\n",
    "d19hat_data, d19hat_targets = load_data_from_pth(d19hat_path)\n",
    "\n",
    "d20_data, _ = load_data_from_pth(d20_path, has_targets=False)\n",
    "d20hat_data, d20hat_targets = load_data_from_pth(d20hat_path)\n",
    "\n",
    "\n",
    "# Data Transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = CustomImageDataset(d1_data, d1_targets, transform=train_transform)\n",
    "d1hat_dataset = CustomImageDataset(d1hat_data, d1hat_targets, transform=test_transform)\n",
    "d2hat_dataset = CustomImageDataset(d2hat_data, d2hat_targets, transform=test_transform)\n",
    "d3hat_dataset = CustomImageDataset(d3hat_data, d3hat_targets, transform=test_transform)\n",
    "d4hat_dataset = CustomImageDataset(d4hat_data, d4hat_targets, transform=test_transform)\n",
    "d5hat_dataset = CustomImageDataset(d5hat_data, d5hat_targets, transform=test_transform)\n",
    "d6hat_dataset = CustomImageDataset(d6hat_data, d6hat_targets, transform=test_transform)\n",
    "d7hat_dataset = CustomImageDataset(d7hat_data, d7hat_targets, transform=test_transform)\n",
    "d8hat_dataset = CustomImageDataset(d8hat_data, d8hat_targets, transform=test_transform)\n",
    "d9hat_dataset = CustomImageDataset(d9hat_data, d9hat_targets, transform=test_transform)\n",
    "d10hat_dataset = CustomImageDataset(d10hat_data, d10hat_targets, transform=test_transform)\n",
    "d11hat_dataset = CustomImageDataset(d11hat_data, d11hat_targets, transform=test_transform)\n",
    "d12hat_dataset = CustomImageDataset(d12hat_data, d12hat_targets, transform=test_transform)\n",
    "d13hat_dataset = CustomImageDataset(d13hat_data, d13hat_targets, transform=test_transform)\n",
    "\n",
    "# For 14\n",
    "d14hat_dataset = CustomImageDataset(d14hat_data, d14hat_targets, transform=test_transform)\n",
    "# For 15\n",
    "d15hat_dataset = CustomImageDataset(d15hat_data, d15hat_targets, transform=test_transform)\n",
    "# For 16\n",
    "d16hat_dataset = CustomImageDataset(d16hat_data, d16hat_targets, transform=test_transform)\n",
    "# For 17\n",
    "d17hat_dataset = CustomImageDataset(d17hat_data, d17hat_targets, transform=test_transform)\n",
    "# For 18\n",
    "d18hat_dataset = CustomImageDataset(d18hat_data, d18hat_targets, transform=test_transform)\n",
    "# For 19\n",
    "d19hat_dataset = CustomImageDataset(d19hat_data, d19hat_targets, transform=test_transform)\n",
    "# For 20\n",
    "d20hat_dataset = CustomImageDataset(d20hat_data, d20hat_targets, transform=test_transform)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "d1hat_loader = DataLoader(d1hat_dataset, batch_size=32, shuffle=False)\n",
    "d2hat_loader = DataLoader(d2hat_dataset, batch_size=32, shuffle=False)\n",
    "d3hat_loader = DataLoader(d3hat_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "d4hat_loader = DataLoader(d4hat_dataset, batch_size=32, shuffle=False)\n",
    "d5hat_loader = DataLoader(d5hat_dataset, batch_size=32, shuffle=False)\n",
    "d6hat_loader = DataLoader(d6hat_dataset, batch_size=32, shuffle=False)\n",
    "d7hat_loader = DataLoader(d7hat_dataset, batch_size=32, shuffle=False)\n",
    "d8hat_loader = DataLoader(d8hat_dataset, batch_size=32, shuffle=False)\n",
    "d9hat_loader = DataLoader(d9hat_dataset, batch_size=32, shuffle=False)\n",
    "d10hat_loader = DataLoader(d10hat_dataset, batch_size=32, shuffle=False)\n",
    "d11hat_loader = DataLoader(d11hat_dataset, batch_size=32, shuffle=False)\n",
    "d12hat_loader = DataLoader(d12hat_dataset, batch_size=32, shuffle=False)\n",
    "d13hat_loader = DataLoader(d13hat_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# For 14\n",
    "d14hat_loader = DataLoader(d14hat_dataset, batch_size=32, shuffle=False)\n",
    "# For 15\n",
    "d15hat_loader = DataLoader(d15hat_dataset, batch_size=32, shuffle=False)\n",
    "# For 16\n",
    "d16hat_loader = DataLoader(d16hat_dataset, batch_size=32, shuffle=False)\n",
    "# For 17\n",
    "d17hat_loader = DataLoader(d17hat_dataset, batch_size=32, shuffle=False)\n",
    "# For 18\n",
    "d18hat_loader = DataLoader(d18hat_dataset, batch_size=32, shuffle=False)\n",
    "# For 19\n",
    "d19hat_loader = DataLoader(d19hat_dataset, batch_size=32, shuffle=False)\n",
    "# For 20\n",
    "d20hat_loader = DataLoader(d20hat_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Feature Extractor: ResNet50\n",
    "class ResNet50Extractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50Extractor, self).__init__()\n",
    "        resnet_model = models.resnet50(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet_model.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "    \n",
    "# Learning with Prototypes Model\n",
    "class LearningWithPrototypes(nn.Module):\n",
    "    def __init__(self, feature_extractor, num_classes, prototype_dim=2048):\n",
    "        super(LearningWithPrototypes, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.fc = nn.Linear(prototype_dim, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "        self.prototypes = nn.Parameter(torch.randn(num_classes, prototype_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        logits = self.fc(features)\n",
    "        return logits, features\n",
    "\n",
    "    def prototype_loss(self, features, labels):\n",
    "        distances = torch.cdist(features, self.prototypes)\n",
    "        return F.cross_entropy(-distances, labels)\n",
    "    \n",
    "    def update_prototypes(self, features, labels):\n",
    "        for class_idx in range(self.num_classes):\n",
    "            class_features = features[labels == class_idx]\n",
    "            if class_features.size(0) > 0:\n",
    "                self.prototypes.data[class_idx] = class_features.mean(dim=0)\n",
    "\n",
    "    def update_prototypes_with_ratp(self, features, labels, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Update prototypes with new high-confidence samples.\n",
    "\n",
    "        Parameters:\n",
    "        - features: Tensor of shape (batch_size, feature_dim), feature representations of the dataset.\n",
    "        - labels: Tensor of shape (batch_size,), corresponding pseudo-labels.\n",
    "        - alpha: Weight for blending old and new prototypes (default: 0.5).\n",
    "        \"\"\"\n",
    "        for class_idx in range(self.num_classes):\n",
    "            # Select features belonging to the current class\n",
    "            class_mask = (labels == class_idx)\n",
    "            class_features = features[class_mask]  # Shape: (num_samples_in_class, feature_dim)\n",
    "            \n",
    "            if class_features.size(0) > 0:\n",
    "                # Compute new centroid for this class (average over batch dimension)\n",
    "                new_centroid = class_features.mean(dim=0)  # Shape: (feature_dim,)\n",
    "                \n",
    "                # Blend the old and new prototypes\n",
    "                self.prototypes.data[class_idx] = (\n",
    "                    alpha * self.prototypes.data[class_idx] + (1 - alpha) * new_centroid\n",
    "                )\n",
    "                \n",
    "                \n",
    "                \n",
    "# Initialize Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "feature_extractor = ResNet50Extractor().to(device)\n",
    "model = LearningWithPrototypes(feature_extractor, num_classes=len(set(d1_targets))).to(device)\n",
    "\n",
    "\n",
    "print({\"started\"})\n",
    "\n",
    "# Load Pretrained Prototypes if Available\n",
    "if os.path.exists('/kaggle/input/weight-files/prototypes.pth'):\n",
    "    model.prototypes.data = torch.load('/kaggle/input/weight-files/prototypes.pth')\n",
    "    print(\"Loaded saved prototypes.\")\n",
    "\n",
    "# Loss Function, Optimizer, and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "alpha = 0.1\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "\n",
    "# Initial Training on D1\n",
    "if os.path.exists('/kaggle/input/weight-files/initial_model_weights.pth'):\n",
    "    model.load_state_dict(torch.load('/kaggle/input/weight-files/initial_model_weights.pth'))\n",
    "    print(\"Loaded pretrained model.\")\n",
    "else:\n",
    "    model.train()\n",
    "    for epoch in range(17):\n",
    "        print({epoch})\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, features = model(images)\n",
    "            ce_loss = criterion(outputs, labels)\n",
    "            proto_loss = model.prototype_loss(features, labels)\n",
    "            loss = ce_loss + alpha * proto_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        scheduler.step()\n",
    "        with torch.no_grad():\n",
    "            all_features = []\n",
    "            all_labels = []\n",
    "            for images, labels in train_loader:\n",
    "                images = images.to(device)\n",
    "                features = model.feature_extractor(images)\n",
    "                all_features.append(features.view(features.size(0), -1))\n",
    "                all_labels.append(labels.to(device))\n",
    "            all_features = torch.cat(all_features, dim=0)\n",
    "            all_labels = torch.cat(all_labels, dim=0)\n",
    "            model.update_prototypes(all_features, all_labels)\n",
    "\n",
    "    torch.save(model.state_dict(), '/kaggle/working/files/initial_model_weights.pth')\n",
    "    torch.save(model.prototypes.data, '/kaggle/working/files/prototypes.pth')\n",
    "    print(\"Saved initial model and prototypes.\")\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Self-Training with Distillation\n",
    "# def self_train_update(model, unlabeled_data,num, num_epochs=12, temperature=2.0, alpha=0.5, beta=0.5):\n",
    "#     pseudo_labels = []\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for images, _ in DataLoader(CustomImageDataset(unlabeled_data, transform=test_transform), batch_size=32):\n",
    "#             images = images.to(device)\n",
    "#             outputs, _ = model(images)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             pseudo_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "#     pseudo_dataset = CustomImageDataset(unlabeled_data, np.array(pseudo_labels), transform=train_transform)\n",
    "#     pseudo_loader = DataLoader(pseudo_dataset, batch_size=32, shuffle=True)\n",
    "#     new_model = copy.deepcopy(model)\n",
    "#     optimizer = optim.AdamW(new_model.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "#     scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print({epoch})\n",
    "#         for images, labels in pseudo_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             student_outputs, features = new_model(images)\n",
    "#             with torch.no_grad():\n",
    "#                 teacher_outputs, _ = model(images)\n",
    "#             ce_loss = criterion(student_outputs, labels)\n",
    "#             proto_loss = new_model.prototype_loss(features, labels)\n",
    "#             dist_loss = distillation_loss(student_outputs, teacher_outputs, temperature)\n",
    "#             loss = ce_loss + alpha * proto_loss + beta * dist_loss\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#     torch.save(new_model.state_dict(), f'/kaggle/working/files/model_f{num}_weights.pth')\n",
    "#     print(\"Saved model after self-training.\")\n",
    "#     return new_model\n",
    "\n",
    "\n",
    "\n",
    "def self_train_update(\n",
    "    old_model,\n",
    "    unlabeled_data,\n",
    "    num,\n",
    "    num_epochs=15,\n",
    "    temperature=2.0,\n",
    "    alpha=0.5,\n",
    "    beta=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Self-training with pseudo-labels, distillation, and prototype updating.\n",
    "\n",
    "    Parameters:\n",
    "    - old_model: The model from the previous step, whose prototypes will be blended.\n",
    "    - unlabeled_data: Unlabeled dataset to be pseudo-labeled and used for training.\n",
    "    - num: Current dataset number for tracking.\n",
    "    - num_epochs: Number of epochs for fine-tuning.\n",
    "    - temperature: Temperature parameter for distillation.\n",
    "    - alpha: Weight for the prototype loss.\n",
    "    - beta: Weight for the distillation loss.\n",
    "\n",
    "    Returns:\n",
    "    - new_model: Updated model after training.\n",
    "    \"\"\"\n",
    "    pseudo_labels = []\n",
    "    old_model.eval()\n",
    "    \n",
    "    # Generate pseudo-labels using the old model\n",
    "    with torch.no_grad():\n",
    "        for images, _ in DataLoader(CustomImageDataset(unlabeled_data, transform=test_transform), batch_size=32):\n",
    "            images = images.to(device)\n",
    "            outputs, _ = old_model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            pseudo_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Create a pseudo-labeled dataset\n",
    "    pseudo_dataset = CustomImageDataset(unlabeled_data, np.array(pseudo_labels), transform=train_transform)\n",
    "    pseudo_loader = DataLoader(pseudo_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Initialize a new model as a copy of the old one\n",
    "    new_model = copy.deepcopy(old_model)\n",
    "    optimizer = optim.AdamW(new_model.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    # Train the new model with the pseudo-labeled data\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for images, labels in pseudo_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the student (new) model\n",
    "            student_outputs, features = new_model(images)\n",
    "\n",
    "            # Get teacher outputs for distillation\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs, _ = old_model(images)\n",
    "\n",
    "            # Calculate losses\n",
    "            ce_loss = criterion(student_outputs, labels)\n",
    "            proto_loss = new_model.prototype_loss(features, labels)\n",
    "            dist_loss = distillation_loss(student_outputs, teacher_outputs, temperature)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = ce_loss + alpha * proto_loss + beta * dist_loss\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Save the new model after training\n",
    "    torch.save(new_model.state_dict(), f'/kaggle/working/files/model_f{num}_weights.pth')\n",
    "    print(f\"Saved model after self-training on dataset {num}.\")\n",
    "\n",
    "    # Update prototypes using weighted average of old and new model prototypes\n",
    "    with torch.no_grad():\n",
    "        weight_old = (num - 1) / num\n",
    "        weight_new = 1 / num\n",
    "        new_model.prototypes.data = (\n",
    "            weight_old * old_model.prototypes.data + weight_new * new_model.prototypes.data\n",
    "        )\n",
    "        print(\"Prototypes updated using weighted average.\")\n",
    "\n",
    "    return new_model\n",
    "\n",
    "# Function to Calculate Accuracy\n",
    "def calculate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs, _ = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "accuracy_matrix = []\n",
    "\n",
    "\n",
    "f1_acc = calculate_accuracy(model,d1hat_loader)\n",
    "accuracy_matrix.append([f1_acc,None,None,None,None,None,None,None,None,None])\n",
    "\n",
    "print({f1_acc})\n",
    "\n",
    "print(\"hello\")\n",
    "#Train on Unlabeled Data (D2) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f2_weights.pth'):\n",
    "    model_f2 = copy.deepcopy(model)\n",
    "    model_f2.load_state_dict(torch.load('/kaggle/input/weight-files/model_f2_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f2 = self_train_update(model, d2_data,2)\n",
    "    \n",
    "\n",
    "print(\"started_accuracy_computation\")\n",
    "f2_acc_d1hat = calculate_accuracy(model_f2,d1hat_loader)\n",
    "f2_acc_d2hat = calculate_accuracy(model_f2,d2hat_loader)\n",
    "\n",
    "accuracy_matrix.append([f2_acc_d1hat,f2_acc_d2hat,None,None,None,None,None,None,None,None])\n",
    "print({f2_acc_d1hat})\n",
    "print({f2_acc_d2hat})\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D3) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f3_weights.pth'):\n",
    "    model_f3 = copy.deepcopy(model)\n",
    "    model_f3.load_state_dict(torch.load('/kaggle/input/weight-files/model_f3_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f3 = self_train_update(model_f2, d3_data,3)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f3_acc_d1hat = calculate_accuracy(model_f3,d1hat_loader)\n",
    "f3_acc_d2hat = calculate_accuracy(model_f3,d2hat_loader)\n",
    "f3_acc_d3hat = calculate_accuracy(model_f3,d3hat_loader)\n",
    "\n",
    "accuracy_matrix.append([f3_acc_d1hat,f3_acc_d2hat,f3_acc_d3hat,None,None,None,None,None,None,None])\n",
    "print({f3_acc_d1hat})\n",
    "print({f3_acc_d2hat})\n",
    "print({f3_acc_d3hat})\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D4) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f4_weights.pth'):\n",
    "    model_f4 = copy.deepcopy(model)\n",
    "    model_f4.load_state_dict(torch.load('/kaggle/input/weight-files/model_f4_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f4 = self_train_update(model_f3, d4_data,4)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f4_acc_d1hat = calculate_accuracy(model_f4,d1hat_loader)\n",
    "f4_acc_d2hat = calculate_accuracy(model_f4,d2hat_loader)\n",
    "f4_acc_d3hat = calculate_accuracy(model_f4,d3hat_loader)\n",
    "f4_acc_d4hat = calculate_accuracy(model_f4,d4hat_loader)\n",
    "\n",
    "accuracy_matrix.append([f4_acc_d1hat,f4_acc_d2hat,f4_acc_d3hat,f4_acc_d4hat,None,None,None,None,None,None])\n",
    "print({f4_acc_d1hat})\n",
    "print({f4_acc_d2hat})\n",
    "print({f4_acc_d3hat})\n",
    "print({f4_acc_d4hat})\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D5) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f5_weights.pth'):\n",
    "    model_f5 = copy.deepcopy(model)\n",
    "    model_f5.load_state_dict(torch.load('/kaggle/input/weight-files/model_f5_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f5 = self_train_update(model_f4, d5_data,5)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f5_acc_d1hat = calculate_accuracy(model_f5,d1hat_loader)\n",
    "f5_acc_d2hat = calculate_accuracy(model_f5,d2hat_loader)\n",
    "f5_acc_d3hat = calculate_accuracy(model_f5,d3hat_loader)\n",
    "f5_acc_d4hat = calculate_accuracy(model_f5,d4hat_loader)\n",
    "f5_acc_d5hat = calculate_accuracy(model_f5,d5hat_loader)\n",
    "\n",
    "accuracy_matrix.append([f5_acc_d1hat,f5_acc_d2hat,f5_acc_d3hat,f5_acc_d4hat,f5_acc_d5hat,None,None,None,None,None])\n",
    "print({f5_acc_d1hat})\n",
    "print({f5_acc_d2hat})\n",
    "print({f5_acc_d3hat})\n",
    "print({f5_acc_d4hat})\n",
    "print({f5_acc_d5hat})\n",
    "\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D6) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f6_weights.pth'):\n",
    "    model_f6 = copy.deepcopy(model)\n",
    "    model_f6.load_state_dict(torch.load('/kaggle/input/weight-files/model_f6_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f6 = self_train_update(model_f5, d6_data,6)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f6_acc_d1hat = calculate_accuracy(model_f6,d1hat_loader)\n",
    "f6_acc_d2hat = calculate_accuracy(model_f6,d2hat_loader)\n",
    "f6_acc_d3hat = calculate_accuracy(model_f6,d3hat_loader)\n",
    "f6_acc_d4hat = calculate_accuracy(model_f6,d4hat_loader)\n",
    "f6_acc_d5hat = calculate_accuracy(model_f6,d5hat_loader)\n",
    "f6_acc_d6hat = calculate_accuracy(model_f6,d6hat_loader)\n",
    "\n",
    "accuracy_matrix.append([f6_acc_d1hat,f6_acc_d2hat,f6_acc_d3hat,f6_acc_d4hat,f6_acc_d5hat,f6_acc_d6hat,None,None,None,None])\n",
    "print({f6_acc_d1hat})\n",
    "print({f6_acc_d2hat})\n",
    "print({f6_acc_d3hat})\n",
    "print({f6_acc_d4hat})\n",
    "print({f6_acc_d5hat})\n",
    "print({f6_acc_d6hat})\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D7) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f7_weights.pth'):\n",
    "    model_f7 = copy.deepcopy(model)\n",
    "    model_f7.load_state_dict(torch.load('/kaggle/input/weight-files/model_f7_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f7 = self_train_update(model_f6, d7_data,7)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f7_acc_d1hat = calculate_accuracy(model_f7,d1hat_loader)\n",
    "f7_acc_d2hat = calculate_accuracy(model_f7,d2hat_loader)\n",
    "f7_acc_d3hat = calculate_accuracy(model_f7,d3hat_loader)\n",
    "f7_acc_d4hat = calculate_accuracy(model_f7,d4hat_loader)\n",
    "f7_acc_d5hat = calculate_accuracy(model_f7,d5hat_loader)\n",
    "f7_acc_d6hat = calculate_accuracy(model_f7,d6hat_loader)\n",
    "f7_acc_d7hat = calculate_accuracy(model_f7,d7hat_loader)\n",
    "\n",
    "accuracy_matrix.append([f7_acc_d1hat,f7_acc_d2hat,f7_acc_d3hat,f7_acc_d4hat,f7_acc_d5hat,f7_acc_d6hat,f7_acc_d7hat,None,None,None])\n",
    "print({f7_acc_d1hat})\n",
    "print({f7_acc_d2hat})\n",
    "print({f7_acc_d3hat})\n",
    "print({f7_acc_d4hat})\n",
    "print({f7_acc_d5hat})\n",
    "print({f7_acc_d6hat})\n",
    "print({f7_acc_d7hat})\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D8) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f8_weights.pth'):\n",
    "    model_f8 = copy.deepcopy(model)\n",
    "    model_f8.load_state_dict(torch.load('/kaggle/input/weight-files/model_f8_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f8 = self_train_update(model_f7, d8_data,8)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f8_acc_d1hat = calculate_accuracy(model_f8,d1hat_loader)\n",
    "f8_acc_d2hat = calculate_accuracy(model_f8,d2hat_loader)\n",
    "f8_acc_d3hat = calculate_accuracy(model_f8,d3hat_loader)\n",
    "f8_acc_d4hat = calculate_accuracy(model_f8,d4hat_loader)\n",
    "f8_acc_d5hat = calculate_accuracy(model_f8,d5hat_loader)\n",
    "f8_acc_d6hat = calculate_accuracy(model_f8,d6hat_loader)\n",
    "f8_acc_d7hat = calculate_accuracy(model_f8,d7hat_loader)\n",
    "f8_acc_d8hat = calculate_accuracy(model_f8,d8hat_loader)\n",
    "\n",
    "accuracy_matrix.append([f8_acc_d1hat,f8_acc_d2hat,f8_acc_d3hat,f8_acc_d4hat,f8_acc_d5hat,f8_acc_d6hat,f8_acc_d7hat,f8_acc_d8hat,None,None])\n",
    "print({f8_acc_d1hat})\n",
    "print({f8_acc_d2hat})\n",
    "print({f8_acc_d3hat})\n",
    "print({f8_acc_d4hat})\n",
    "print({f8_acc_d5hat})\n",
    "print({f8_acc_d6hat})\n",
    "print({f8_acc_d7hat})\n",
    "print({f8_acc_d8hat})\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D9) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f9_weights.pth'):\n",
    "    model_f9 = copy.deepcopy(model)\n",
    "    model_f9.load_state_dict(torch.load('/kaggle/input/weight-files/model_f9_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f9 = self_train_update(model_f8, d9_data,9)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f9_acc_d1hat = calculate_accuracy(model_f9,d1hat_loader)\n",
    "f9_acc_d2hat = calculate_accuracy(model_f9,d2hat_loader)\n",
    "f9_acc_d3hat = calculate_accuracy(model_f9,d3hat_loader)\n",
    "f9_acc_d4hat = calculate_accuracy(model_f9,d4hat_loader)\n",
    "f9_acc_d5hat = calculate_accuracy(model_f9,d5hat_loader)\n",
    "f9_acc_d6hat = calculate_accuracy(model_f8,d6hat_loader)\n",
    "f9_acc_d7hat = calculate_accuracy(model_f9,d7hat_loader)\n",
    "f9_acc_d8hat = calculate_accuracy(model_f9,d8hat_loader)\n",
    "f9_acc_d9hat = calculate_accuracy(model_f9,d9hat_loader)\n",
    "\n",
    "accuracy_matrix.append([f9_acc_d1hat,f9_acc_d2hat,f9_acc_d3hat,f9_acc_d4hat,f9_acc_d5hat,f9_acc_d6hat,f9_acc_d7hat,f9_acc_d8hat,f9_acc_d9hat,None])\n",
    "print({f9_acc_d1hat})\n",
    "print({f9_acc_d2hat})\n",
    "print({f9_acc_d3hat})\n",
    "print({f9_acc_d4hat})\n",
    "print({f9_acc_d5hat})\n",
    "print({f9_acc_d6hat})\n",
    "print({f9_acc_d7hat})\n",
    "print({f9_acc_d8hat})\n",
    "print({f9_acc_d9hat})\n",
    "\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D10) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f10_weights.pth'):\n",
    "    model_f10 = copy.deepcopy(model)\n",
    "    model_f10.load_state_dict(torch.load('/kaggle/input/weight-files/model_f10_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f10 = self_train_update(model_f9, d10_data,10)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f10_acc_d1hat = calculate_accuracy(model_f10,d1hat_loader)\n",
    "print({f10_acc_d1hat})\n",
    "f10_acc_d2hat = calculate_accuracy(model_f10,d2hat_loader)\n",
    "print({f10_acc_d2hat})\n",
    "f10_acc_d3hat = calculate_accuracy(model_f10,d3hat_loader)\n",
    "print({f10_acc_d3hat})\n",
    "f10_acc_d4hat = calculate_accuracy(model_f10,d4hat_loader)\n",
    "print({f10_acc_d4hat})\n",
    "f10_acc_d5hat = calculate_accuracy(model_f10,d5hat_loader)\n",
    "print({f10_acc_d5hat})\n",
    "f10_acc_d6hat = calculate_accuracy(model_f10,d6hat_loader)\n",
    "print({f10_acc_d6hat})\n",
    "f10_acc_d7hat = calculate_accuracy(model_f10,d7hat_loader)\n",
    "print({f10_acc_d7hat})\n",
    "f10_acc_d8hat = calculate_accuracy(model_f10,d8hat_loader)\n",
    "print({f10_acc_d8hat})\n",
    "f10_acc_d9hat = calculate_accuracy(model_f10,d9hat_loader)\n",
    "print({f10_acc_d9hat})\n",
    "f10_acc_d10hat = calculate_accuracy(model_f10,d10hat_loader)\n",
    "print({f10_acc_d10hat})\n",
    "\n",
    "accuracy_matrix.append([f10_acc_d1hat,f10_acc_d2hat,f10_acc_d3hat,f10_acc_d4hat,f10_acc_d5hat,f10_acc_d6hat,f10_acc_d7hat,f10_acc_d8hat,f10_acc_d9hat,f10_acc_d10hat])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rand Mix using Ada In "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6049882,
     "sourceId": 9858288,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6051933,
     "sourceId": 9860877,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
