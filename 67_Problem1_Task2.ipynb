{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T15:40:23.384648Z",
     "iopub.status.busy": "2024-11-26T15:40:23.384212Z",
     "iopub.status.idle": "2024-11-26T16:20:07.979124Z",
     "shell.execute_reply": "2024-11-26T16:20:07.978221Z",
     "shell.execute_reply.started": "2024-11-26T15:40:23.384606Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reached_path_to_datasets'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/3106921013.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_dict = torch.load(pth_path)\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 123MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'started'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/3106921013.py:485: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_f10.load_state_dict(torch.load('/kaggle/input/d10-weights/model_f10_weights.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded self-trained model. for model 10\n",
      "Epoch 1/15\n",
      "Epoch 0 has a loss of 0.4599204659461975\n",
      "Epoch 0 has a loss of 0.8011131286621094\n",
      "Epoch 0 has a loss of 0.5198723673820496\n",
      "Epoch 0 has a loss of 0.4204876720905304\n",
      "Epoch 0 has a loss of 0.49412858486175537\n",
      "Epoch 0 has a loss of 0.4465569257736206\n",
      "Epoch 0 has a loss of 0.3646828234195709\n",
      "Epoch 0 has a loss of 0.40532320737838745\n",
      "Epoch 0 has a loss of 0.2633451223373413\n",
      "Epoch 0 has a loss of 0.3439815938472748\n",
      "Epoch 0 has a loss of 0.3762035667896271\n",
      "Epoch 0 has a loss of 0.18445393443107605\n",
      "Epoch 1, Loss: 0.4290723604361216\n",
      "Epoch 2/15\n",
      "Epoch 1 has a loss of 0.31932738423347473\n",
      "Epoch 1 has a loss of 0.21292632818222046\n",
      "Epoch 1 has a loss of 0.1085902601480484\n",
      "Epoch 1 has a loss of 0.3186531066894531\n",
      "Epoch 1 has a loss of 0.2185685783624649\n",
      "Epoch 1 has a loss of 0.3829090893268585\n",
      "Epoch 1 has a loss of 0.32523059844970703\n",
      "Epoch 1 has a loss of 0.3222225308418274\n",
      "Epoch 1 has a loss of 0.4065302610397339\n",
      "Epoch 1 has a loss of 0.2006652057170868\n",
      "Epoch 1 has a loss of 0.1216338574886322\n",
      "Epoch 1 has a loss of 0.22984255850315094\n",
      "Epoch 2, Loss: 0.26474295802911124\n",
      "Epoch 3/15\n",
      "Epoch 2 has a loss of 0.14992831647396088\n",
      "Epoch 2 has a loss of 0.14215174317359924\n",
      "Epoch 2 has a loss of 0.19759279489517212\n",
      "Epoch 2 has a loss of 0.1637447625398636\n",
      "Epoch 2 has a loss of 0.24868528544902802\n",
      "Epoch 2 has a loss of 0.4386640787124634\n",
      "Epoch 2 has a loss of 0.08720231056213379\n",
      "Epoch 2 has a loss of 0.12413918972015381\n",
      "Epoch 2 has a loss of 0.15285803377628326\n",
      "Epoch 2 has a loss of 0.14721331000328064\n",
      "Epoch 2 has a loss of 0.19000403583049774\n",
      "Epoch 2 has a loss of 0.8767803907394409\n",
      "Epoch 3, Loss: 0.2280422201156616\n",
      "Epoch 4/15\n",
      "Epoch 3 has a loss of 0.26918894052505493\n",
      "Epoch 3 has a loss of 0.17042213678359985\n",
      "Epoch 3 has a loss of 0.21140089631080627\n",
      "Epoch 3 has a loss of 0.12001220881938934\n",
      "Epoch 3 has a loss of 0.13628141582012177\n",
      "Epoch 3 has a loss of 0.2140098512172699\n",
      "Epoch 3 has a loss of 0.3574709892272949\n",
      "Epoch 3 has a loss of 0.2341255247592926\n",
      "Epoch 3 has a loss of 0.38484612107276917\n",
      "Epoch 3 has a loss of 0.15979433059692383\n",
      "Epoch 3 has a loss of 0.15525031089782715\n",
      "Epoch 3 has a loss of 0.15478812158107758\n",
      "Epoch 4, Loss: 0.21538617074489594\n",
      "Epoch 5/15\n",
      "Epoch 4 has a loss of 0.15458141267299652\n",
      "Epoch 4 has a loss of 0.14240574836730957\n",
      "Epoch 4 has a loss of 0.15090109407901764\n",
      "Epoch 4 has a loss of 0.13098950684070587\n",
      "Epoch 4 has a loss of 0.17369909584522247\n",
      "Epoch 4 has a loss of 0.32468289136886597\n",
      "Epoch 4 has a loss of 0.13403281569480896\n",
      "Epoch 4 has a loss of 0.12258318811655045\n",
      "Epoch 4 has a loss of 0.09583882987499237\n",
      "Epoch 4 has a loss of 0.48976707458496094\n",
      "Epoch 4 has a loss of 0.10847961902618408\n",
      "Epoch 4 has a loss of 0.12876608967781067\n",
      "Epoch 5, Loss: 0.18095034909248353\n",
      "Epoch 6/15\n",
      "Epoch 5 has a loss of 0.2709445357322693\n",
      "Epoch 5 has a loss of 0.08807861059904099\n",
      "Epoch 5 has a loss of 0.11041495949029922\n",
      "Epoch 5 has a loss of 0.5668043494224548\n",
      "Epoch 5 has a loss of 0.17571189999580383\n",
      "Epoch 5 has a loss of 0.19742660224437714\n",
      "Epoch 5 has a loss of 0.28303754329681396\n",
      "Epoch 5 has a loss of 0.11399199068546295\n",
      "Epoch 5 has a loss of 0.31684496998786926\n",
      "Epoch 5 has a loss of 0.16561023890972137\n",
      "Epoch 5 has a loss of 0.12471480667591095\n",
      "Epoch 5 has a loss of 0.18377861380577087\n",
      "Epoch 6, Loss: 0.2172306249141693\n",
      "Epoch 7/15\n",
      "Epoch 6 has a loss of 0.21885696053504944\n",
      "Epoch 6 has a loss of 0.2020762860774994\n",
      "Epoch 6 has a loss of 0.07628080248832703\n",
      "Epoch 6 has a loss of 0.09750933945178986\n",
      "Epoch 6 has a loss of 0.15611380338668823\n",
      "Epoch 6 has a loss of 0.1279381364583969\n",
      "Epoch 6 has a loss of 0.6219717860221863\n",
      "Epoch 6 has a loss of 0.7310808897018433\n",
      "Epoch 6 has a loss of 0.27683788537979126\n",
      "Epoch 6 has a loss of 0.08240467309951782\n",
      "Epoch 6 has a loss of 0.12323135882616043\n",
      "Epoch 6 has a loss of 0.26291990280151367\n",
      "Epoch 7, Loss: 0.2477461846669515\n",
      "Epoch 8/15\n",
      "Epoch 7 has a loss of 0.13587254285812378\n",
      "Epoch 7 has a loss of 0.1683269739151001\n",
      "Epoch 7 has a loss of 0.3368428349494934\n",
      "Epoch 7 has a loss of 0.10271215438842773\n",
      "Epoch 7 has a loss of 0.1769220530986786\n",
      "Epoch 7 has a loss of 0.20269477367401123\n",
      "Epoch 7 has a loss of 0.08604826033115387\n",
      "Epoch 7 has a loss of 0.1046181246638298\n",
      "Epoch 7 has a loss of 0.11929292976856232\n",
      "Epoch 7 has a loss of 0.09599268436431885\n",
      "Epoch 7 has a loss of 0.258556604385376\n",
      "Epoch 7 has a loss of 0.6660038828849792\n",
      "Epoch 8, Loss: 0.1934139927228292\n",
      "Epoch 9/15\n",
      "Epoch 8 has a loss of 0.30808672308921814\n",
      "Epoch 8 has a loss of 0.13431382179260254\n",
      "Epoch 8 has a loss of 0.27263447642326355\n",
      "Epoch 8 has a loss of 0.22535738348960876\n",
      "Epoch 8 has a loss of 0.22660189867019653\n",
      "Epoch 8 has a loss of 0.26003938913345337\n",
      "Epoch 8 has a loss of 0.07849177718162537\n",
      "Epoch 8 has a loss of 0.4118099808692932\n",
      "Epoch 8 has a loss of 0.2193586230278015\n",
      "Epoch 8 has a loss of 0.13648772239685059\n",
      "Epoch 8 has a loss of 0.11880303174257278\n",
      "Epoch 8 has a loss of 0.5377166867256165\n",
      "Epoch 9, Loss: 0.23709599542617799\n",
      "Epoch 10/15\n",
      "Epoch 9 has a loss of 0.1738341599702835\n",
      "Epoch 9 has a loss of 0.6225430965423584\n",
      "Epoch 9 has a loss of 0.11442601680755615\n",
      "Epoch 9 has a loss of 0.08774018287658691\n",
      "Epoch 9 has a loss of 0.1500299870967865\n",
      "Epoch 9 has a loss of 0.10162992775440216\n",
      "Epoch 9 has a loss of 0.2557012736797333\n",
      "Epoch 9 has a loss of 0.15871863067150116\n",
      "Epoch 9 has a loss of 0.10758516192436218\n",
      "Epoch 9 has a loss of 0.19344836473464966\n",
      "Epoch 9 has a loss of 0.18288956582546234\n",
      "Epoch 9 has a loss of 0.0706319510936737\n",
      "Epoch 10, Loss: 0.18767471639315286\n",
      "Epoch 11/15\n",
      "Epoch 10 has a loss of 0.1031324714422226\n",
      "Epoch 10 has a loss of 0.12040668725967407\n",
      "Epoch 10 has a loss of 0.4409518837928772\n",
      "Epoch 10 has a loss of 0.0962575227022171\n",
      "Epoch 10 has a loss of 0.1429237574338913\n",
      "Epoch 10 has a loss of 0.18358270823955536\n",
      "Epoch 10 has a loss of 0.18305866420269012\n",
      "Epoch 10 has a loss of 0.0756300538778305\n",
      "Epoch 10 has a loss of 0.3949204087257385\n",
      "Epoch 10 has a loss of 0.1542862206697464\n",
      "Epoch 10 has a loss of 0.10926788300275803\n",
      "Epoch 10 has a loss of 0.1758190393447876\n",
      "Epoch 11, Loss: 0.18182725938161215\n",
      "Epoch 12/15\n",
      "Epoch 11 has a loss of 0.18964770436286926\n",
      "Epoch 11 has a loss of 0.3638322353363037\n",
      "Epoch 11 has a loss of 0.17517973482608795\n",
      "Epoch 11 has a loss of 0.21893265843391418\n",
      "Epoch 11 has a loss of 0.24991801381111145\n",
      "Epoch 11 has a loss of 0.19750788807868958\n",
      "Epoch 11 has a loss of 0.159494549036026\n",
      "Epoch 11 has a loss of 0.10869938135147095\n",
      "Epoch 11 has a loss of 0.5337867736816406\n",
      "Epoch 11 has a loss of 0.17552852630615234\n",
      "Epoch 11 has a loss of 0.1327572911977768\n",
      "Epoch 11 has a loss of 0.2896654009819031\n",
      "Epoch 12, Loss: 0.23155044380823772\n",
      "Epoch 13/15\n",
      "Epoch 12 has a loss of 0.11087044328451157\n",
      "Epoch 12 has a loss of 0.16606679558753967\n",
      "Epoch 12 has a loss of 0.1496194452047348\n",
      "Epoch 12 has a loss of 0.13745777308940887\n",
      "Epoch 12 has a loss of 0.13397854566574097\n",
      "Epoch 12 has a loss of 0.07991467416286469\n",
      "Epoch 12 has a loss of 0.17207278311252594\n",
      "Epoch 12 has a loss of 0.20184257626533508\n",
      "Epoch 12 has a loss of 0.2587134838104248\n",
      "Epoch 12 has a loss of 0.12015914916992188\n",
      "Epoch 12 has a loss of 0.2831389605998993\n",
      "Epoch 12 has a loss of 0.14169779419898987\n",
      "Epoch 13, Loss: 0.1634713531335195\n",
      "Epoch 14/15\n",
      "Epoch 13 has a loss of 0.14705777168273926\n",
      "Epoch 13 has a loss of 0.24461449682712555\n",
      "Epoch 13 has a loss of 0.4407421052455902\n",
      "Epoch 13 has a loss of 0.15097299218177795\n",
      "Epoch 13 has a loss of 0.04667998477816582\n",
      "Epoch 13 has a loss of 0.1461944282054901\n",
      "Epoch 13 has a loss of 0.14517727494239807\n",
      "Epoch 13 has a loss of 0.10001911222934723\n",
      "Epoch 13 has a loss of 0.16233958303928375\n",
      "Epoch 13 has a loss of 0.0888293981552124\n",
      "Epoch 13 has a loss of 0.13031859695911407\n",
      "Epoch 13 has a loss of 0.15980714559555054\n",
      "Epoch 14, Loss: 0.16365287510553997\n",
      "Epoch 15/15\n",
      "Epoch 14 has a loss of 0.17449192702770233\n",
      "Epoch 14 has a loss of 0.06612697243690491\n",
      "Epoch 14 has a loss of 0.17384639382362366\n",
      "Epoch 14 has a loss of 0.13238266110420227\n",
      "Epoch 14 has a loss of 0.22529111802577972\n",
      "Epoch 14 has a loss of 0.10598856210708618\n",
      "Epoch 14 has a loss of 0.3636387884616852\n",
      "Epoch 14 has a loss of 0.12549394369125366\n",
      "Epoch 14 has a loss of 0.17064620554447174\n",
      "Epoch 14 has a loss of 0.09405892342329025\n",
      "Epoch 14 has a loss of 0.07239539176225662\n",
      "Epoch 14 has a loss of 0.1524326056241989\n",
      "Epoch 15, Loss: 0.15478799553712208\n",
      "Prototypes updated using weighted average.\n",
      "Saved model after self-training with T2PL.\n",
      "started_accuracy_computation\n",
      "{91.28}\n",
      "{90.52}\n",
      "{90.4}\n",
      "{90.56}\n",
      "{90.36}\n",
      "{90.76}\n",
      "{90.44}\n",
      "{91.64}\n",
      "{90.72}\n",
      "{90.92}\n",
      "{88.96}\n",
      "Epoch 1/15\n",
      "Epoch 0 has a loss of 3.7148380279541016\n",
      "Epoch 0 has a loss of 3.5974926948547363\n",
      "Epoch 0 has a loss of 3.8425984382629395\n",
      "Epoch 0 has a loss of 4.152933120727539\n",
      "Epoch 0 has a loss of 3.648550510406494\n",
      "Epoch 0 has a loss of 4.402130126953125\n",
      "Epoch 0 has a loss of 4.552995681762695\n",
      "Epoch 0 has a loss of 3.2737045288085938\n",
      "Epoch 0 has a loss of 4.285702705383301\n",
      "Epoch 0 has a loss of 3.507338047027588\n",
      "Epoch 0 has a loss of 2.7579245567321777\n",
      "Epoch 0 has a loss of 3.6950221061706543\n",
      "Epoch 1, Loss: 3.7881178092956542\n",
      "Epoch 2/15\n",
      "Epoch 1 has a loss of 4.082614898681641\n",
      "Epoch 1 has a loss of 2.7377636432647705\n",
      "Epoch 1 has a loss of 2.971449851989746\n",
      "Epoch 1 has a loss of 2.834777355194092\n",
      "Epoch 1 has a loss of 2.3582329750061035\n",
      "Epoch 1 has a loss of 2.7161731719970703\n",
      "Epoch 1 has a loss of 3.4168801307678223\n",
      "Epoch 1 has a loss of 2.516249179840088\n",
      "Epoch 1 has a loss of 1.4517319202423096\n",
      "Epoch 1 has a loss of 1.6960787773132324\n",
      "Epoch 1 has a loss of 1.3613673448562622\n",
      "Epoch 1 has a loss of 1.604797124862671\n",
      "Epoch 2, Loss: 2.4999907995859783\n",
      "Epoch 3/15\n",
      "Epoch 2 has a loss of 2.078131675720215\n",
      "Epoch 2 has a loss of 1.2859985828399658\n",
      "Epoch 2 has a loss of 0.7199274301528931\n",
      "Epoch 2 has a loss of 1.5369150638580322\n",
      "Epoch 2 has a loss of 1.9314172267913818\n",
      "Epoch 2 has a loss of 1.934990644454956\n",
      "Epoch 2 has a loss of 1.2610971927642822\n",
      "Epoch 2 has a loss of 1.2659766674041748\n",
      "Epoch 2 has a loss of 2.353389263153076\n",
      "Epoch 2 has a loss of 1.9329502582550049\n",
      "Epoch 2 has a loss of 2.696246862411499\n",
      "Epoch 2 has a loss of 1.7215936183929443\n",
      "Epoch 3, Loss: 1.726671895980835\n",
      "Epoch 4/15\n",
      "Epoch 3 has a loss of 1.5460362434387207\n",
      "Epoch 3 has a loss of 1.7516875267028809\n",
      "Epoch 3 has a loss of 0.9233424067497253\n",
      "Epoch 3 has a loss of 1.0429811477661133\n",
      "Epoch 3 has a loss of 1.6272790431976318\n",
      "Epoch 3 has a loss of 1.143913984298706\n",
      "Epoch 3 has a loss of 0.8717246055603027\n",
      "Epoch 3 has a loss of 1.9152553081512451\n",
      "Epoch 3 has a loss of 1.020158052444458\n",
      "Epoch 3 has a loss of 1.0780222415924072\n",
      "Epoch 3 has a loss of 1.24845290184021\n",
      "Epoch 3 has a loss of 0.8347841501235962\n",
      "Epoch 4, Loss: 1.260275589942932\n",
      "Epoch 5/15\n",
      "Epoch 4 has a loss of 1.2601587772369385\n",
      "Epoch 4 has a loss of 0.9577165842056274\n",
      "Epoch 4 has a loss of 0.943511426448822\n",
      "Epoch 4 has a loss of 0.767488956451416\n",
      "Epoch 4 has a loss of 0.8220442533493042\n",
      "Epoch 4 has a loss of 2.585096836090088\n",
      "Epoch 4 has a loss of 0.8408705592155457\n",
      "Epoch 4 has a loss of 2.2422726154327393\n",
      "Epoch 4 has a loss of 0.7050803899765015\n",
      "Epoch 4 has a loss of 0.6218869090080261\n",
      "Epoch 4 has a loss of 1.812253475189209\n",
      "Epoch 4 has a loss of 0.3725817799568176\n",
      "Epoch 5, Loss: 1.1798335092862446\n",
      "Epoch 6/15\n",
      "Epoch 5 has a loss of 1.16688871383667\n",
      "Epoch 5 has a loss of 0.8475174307823181\n",
      "Epoch 5 has a loss of 0.6046435832977295\n",
      "Epoch 5 has a loss of 1.149673581123352\n",
      "Epoch 5 has a loss of 0.5232853889465332\n",
      "Epoch 5 has a loss of 0.8986639976501465\n",
      "Epoch 5 has a loss of 1.2853138446807861\n",
      "Epoch 5 has a loss of 1.059770107269287\n",
      "Epoch 5 has a loss of 0.7481808662414551\n",
      "Epoch 5 has a loss of 0.7910042405128479\n",
      "Epoch 5 has a loss of 0.7725280523300171\n",
      "Epoch 5 has a loss of 0.9476982951164246\n",
      "Epoch 6, Loss: 0.8984429189364116\n",
      "Epoch 7/15\n",
      "Epoch 6 has a loss of 1.0640933513641357\n",
      "Epoch 6 has a loss of 0.7263203263282776\n",
      "Epoch 6 has a loss of 0.5995403528213501\n",
      "Epoch 6 has a loss of 0.7577295899391174\n",
      "Epoch 6 has a loss of 0.7309010028839111\n",
      "Epoch 6 has a loss of 0.6703068017959595\n",
      "Epoch 6 has a loss of 1.0445150136947632\n",
      "Epoch 6 has a loss of 0.7013542652130127\n",
      "Epoch 6 has a loss of 0.7178885340690613\n",
      "Epoch 6 has a loss of 0.7363412380218506\n",
      "Epoch 6 has a loss of 0.8085113167762756\n",
      "Epoch 6 has a loss of 0.5768246650695801\n",
      "Epoch 7, Loss: 0.7656187324523925\n",
      "Epoch 8/15\n",
      "Epoch 7 has a loss of 1.179821252822876\n",
      "Epoch 7 has a loss of 0.9698200821876526\n",
      "Epoch 7 has a loss of 0.7113341093063354\n",
      "Epoch 7 has a loss of 0.6851705312728882\n",
      "Epoch 7 has a loss of 0.5230677723884583\n",
      "Epoch 7 has a loss of 0.8439942598342896\n",
      "Epoch 7 has a loss of 0.7060592770576477\n",
      "Epoch 7 has a loss of 1.093496322631836\n",
      "Epoch 7 has a loss of 0.7129539251327515\n",
      "Epoch 7 has a loss of 0.6138476133346558\n",
      "Epoch 7 has a loss of 0.34865236282348633\n",
      "Epoch 7 has a loss of 0.5255684852600098\n",
      "Epoch 8, Loss: 0.7480294278462728\n",
      "Epoch 9/15\n",
      "Epoch 8 has a loss of 0.4435635209083557\n",
      "Epoch 8 has a loss of 0.6270623207092285\n",
      "Epoch 8 has a loss of 0.6353657245635986\n",
      "Epoch 8 has a loss of 0.7894625663757324\n",
      "Epoch 8 has a loss of 0.5605984330177307\n",
      "Epoch 8 has a loss of 0.526832103729248\n",
      "Epoch 8 has a loss of 0.6951904296875\n",
      "Epoch 8 has a loss of 0.6700007915496826\n",
      "Epoch 8 has a loss of 0.46966758370399475\n",
      "Epoch 8 has a loss of 0.6943507194519043\n",
      "Epoch 8 has a loss of 0.8519843816757202\n",
      "Epoch 8 has a loss of 0.5950828790664673\n",
      "Epoch 9, Loss: 0.6307664550145468\n",
      "Epoch 10/15\n",
      "Epoch 9 has a loss of 0.6974382400512695\n",
      "Epoch 9 has a loss of 1.3149611949920654\n",
      "Epoch 9 has a loss of 0.6965471506118774\n",
      "Epoch 9 has a loss of 0.43791669607162476\n",
      "Epoch 9 has a loss of 0.9868925213813782\n",
      "Epoch 9 has a loss of 0.4955967366695404\n",
      "Epoch 9 has a loss of 0.7051172852516174\n",
      "Epoch 9 has a loss of 0.3177826404571533\n",
      "Epoch 9 has a loss of 0.4230803847312927\n",
      "Epoch 9 has a loss of 1.4686249494552612\n",
      "Epoch 9 has a loss of 0.544378936290741\n",
      "Epoch 9 has a loss of 0.5467333793640137\n",
      "Epoch 10, Loss: 0.723737715403239\n",
      "Epoch 11/15\n",
      "Epoch 10 has a loss of 0.7834032773971558\n",
      "Epoch 10 has a loss of 0.4636702537536621\n",
      "Epoch 10 has a loss of 0.4278770983219147\n",
      "Epoch 10 has a loss of 0.881772518157959\n",
      "Epoch 10 has a loss of 0.6625256538391113\n",
      "Epoch 10 has a loss of 1.2031407356262207\n",
      "Epoch 10 has a loss of 1.0163633823394775\n",
      "Epoch 10 has a loss of 0.3600192070007324\n",
      "Epoch 10 has a loss of 0.6538844108581543\n",
      "Epoch 10 has a loss of 0.6506575345993042\n",
      "Epoch 10 has a loss of 0.9917809963226318\n",
      "Epoch 10 has a loss of 1.0053482055664062\n",
      "Epoch 11, Loss: 0.7524428024291993\n",
      "Epoch 12/15\n",
      "Epoch 11 has a loss of 1.0252615213394165\n",
      "Epoch 11 has a loss of 0.7835192680358887\n",
      "Epoch 11 has a loss of 0.39661285281181335\n",
      "Epoch 11 has a loss of 0.5644456744194031\n",
      "Epoch 11 has a loss of 0.6999509930610657\n",
      "Epoch 11 has a loss of 0.6923873424530029\n",
      "Epoch 11 has a loss of 0.3500044047832489\n",
      "Epoch 11 has a loss of 0.6215312480926514\n",
      "Epoch 11 has a loss of 0.4663759171962738\n",
      "Epoch 11 has a loss of 0.7431957721710205\n",
      "Epoch 11 has a loss of 0.6600103378295898\n",
      "Epoch 11 has a loss of 0.7685543298721313\n",
      "Epoch 12, Loss: 0.6447525339126587\n",
      "Epoch 13/15\n",
      "Epoch 12 has a loss of 0.8377269506454468\n",
      "Epoch 12 has a loss of 0.449022501707077\n",
      "Epoch 12 has a loss of 1.0106227397918701\n",
      "Epoch 12 has a loss of 0.5706900358200073\n",
      "Epoch 12 has a loss of 1.309279203414917\n",
      "Epoch 12 has a loss of 0.5153589844703674\n",
      "Epoch 12 has a loss of 0.5944055914878845\n",
      "Epoch 12 has a loss of 0.3689427077770233\n",
      "Epoch 12 has a loss of 0.49161258339881897\n",
      "Epoch 12 has a loss of 1.0760493278503418\n",
      "Epoch 12 has a loss of 1.093314290046692\n",
      "Epoch 12 has a loss of 0.5980936884880066\n",
      "Epoch 13, Loss: 0.7464025390942891\n",
      "Epoch 14/15\n",
      "Epoch 13 has a loss of 0.9678114056587219\n",
      "Epoch 13 has a loss of 0.45934221148490906\n",
      "Epoch 13 has a loss of 0.7130082845687866\n",
      "Epoch 13 has a loss of 0.3909832835197449\n",
      "Epoch 13 has a loss of 1.0900732278823853\n",
      "Epoch 13 has a loss of 0.41570132970809937\n",
      "Epoch 13 has a loss of 0.9505816698074341\n",
      "Epoch 13 has a loss of 0.4715898633003235\n",
      "Epoch 13 has a loss of 0.8370023965835571\n",
      "Epoch 13 has a loss of 0.6128886938095093\n",
      "Epoch 13 has a loss of 0.5639446377754211\n",
      "Epoch 13 has a loss of 0.9066269397735596\n",
      "Epoch 14, Loss: 0.6932962233225505\n",
      "Epoch 15/15\n",
      "Epoch 14 has a loss of 0.5233938097953796\n",
      "Epoch 14 has a loss of 0.7234984636306763\n",
      "Epoch 14 has a loss of 0.6443645358085632\n",
      "Epoch 14 has a loss of 0.9078643321990967\n",
      "Epoch 14 has a loss of 0.5238546133041382\n",
      "Epoch 14 has a loss of 0.7583202123641968\n",
      "Epoch 14 has a loss of 0.42878544330596924\n",
      "Epoch 14 has a loss of 0.4032597839832306\n",
      "Epoch 14 has a loss of 0.784984290599823\n",
      "Epoch 14 has a loss of 0.4612239599227905\n",
      "Epoch 14 has a loss of 0.8392041921615601\n",
      "Epoch 14 has a loss of 0.3152085840702057\n",
      "Epoch 15, Loss: 0.6165597701867421\n",
      "Prototypes updated using weighted average.\n",
      "Saved model after self-training with T2PL.\n",
      "started_accuracy_computation\n",
      "{87.84}\n",
      "{87.48}\n",
      "{86.48}\n",
      "{86.32}\n",
      "{87.0}\n",
      "{87.32}\n",
      "{86.16}\n",
      "{88.08}\n",
      "{86.52}\n",
      "{88.0}\n",
      "{75.64}\n",
      "{70.88}\n",
      "Epoch 1/15\n",
      "Epoch 0 has a loss of 0.7765091061592102\n",
      "Epoch 0 has a loss of 1.05193293094635\n",
      "Epoch 0 has a loss of 1.079073429107666\n",
      "Epoch 0 has a loss of 1.2497076988220215\n",
      "Epoch 0 has a loss of 0.8388405442237854\n",
      "Epoch 0 has a loss of 0.4008151888847351\n",
      "Epoch 0 has a loss of 0.730441153049469\n",
      "Epoch 0 has a loss of 0.5409663915634155\n",
      "Epoch 0 has a loss of 0.46393710374832153\n",
      "Epoch 0 has a loss of 1.0248607397079468\n",
      "Epoch 0 has a loss of 0.9767926931381226\n",
      "Epoch 0 has a loss of 2.374314069747925\n",
      "Epoch 1, Loss: 0.9250487651824951\n",
      "Epoch 2/15\n",
      "Epoch 1 has a loss of 0.703920841217041\n",
      "Epoch 1 has a loss of 0.44195663928985596\n",
      "Epoch 1 has a loss of 0.35410189628601074\n",
      "Epoch 1 has a loss of 0.33526214957237244\n",
      "Epoch 1 has a loss of 1.1053324937820435\n",
      "Epoch 1 has a loss of 0.45582857728004456\n",
      "Epoch 1 has a loss of 0.6537699699401855\n",
      "Epoch 1 has a loss of 0.4564310908317566\n",
      "Epoch 1 has a loss of 0.47355419397354126\n",
      "Epoch 1 has a loss of 1.4857261180877686\n",
      "Epoch 1 has a loss of 0.21919330954551697\n",
      "Epoch 1 has a loss of 0.2269342541694641\n",
      "Epoch 2, Loss: 0.5843785621325175\n",
      "Epoch 3/15\n",
      "Epoch 2 has a loss of 0.5693820118904114\n",
      "Epoch 2 has a loss of 0.45583540201187134\n",
      "Epoch 2 has a loss of 0.3386022448539734\n",
      "Epoch 2 has a loss of 0.6564685702323914\n",
      "Epoch 2 has a loss of 0.29786252975463867\n",
      "Epoch 2 has a loss of 0.14001095294952393\n",
      "Epoch 2 has a loss of 0.16293956339359283\n",
      "Epoch 2 has a loss of 0.6056776642799377\n",
      "Epoch 2 has a loss of 0.27468714118003845\n",
      "Epoch 2 has a loss of 0.3873128294944763\n",
      "Epoch 2 has a loss of 0.3150772154331207\n",
      "Epoch 2 has a loss of 0.7225887775421143\n",
      "Epoch 3, Loss: 0.4030478343963623\n",
      "Epoch 4/15\n",
      "Epoch 3 has a loss of 0.2603752017021179\n",
      "Epoch 3 has a loss of 0.3022626042366028\n",
      "Epoch 3 has a loss of 0.3986806869506836\n",
      "Epoch 3 has a loss of 0.567370593547821\n",
      "Epoch 3 has a loss of 0.2856036126613617\n",
      "Epoch 3 has a loss of 0.3426779508590698\n",
      "Epoch 3 has a loss of 0.09508191049098969\n",
      "Epoch 3 has a loss of 0.6677180528640747\n",
      "Epoch 3 has a loss of 0.13318082690238953\n",
      "Epoch 3 has a loss of 0.26272812485694885\n",
      "Epoch 3 has a loss of 0.6383974552154541\n",
      "Epoch 3 has a loss of 0.39393848180770874\n",
      "Epoch 4, Loss: 0.36157613261540733\n",
      "Epoch 5/15\n",
      "Epoch 4 has a loss of 0.6445520520210266\n",
      "Epoch 4 has a loss of 0.2923484444618225\n",
      "Epoch 4 has a loss of 0.29660195112228394\n",
      "Epoch 4 has a loss of 0.25191980600357056\n",
      "Epoch 4 has a loss of 0.21229620277881622\n",
      "Epoch 4 has a loss of 0.1829010397195816\n",
      "Epoch 4 has a loss of 0.14717727899551392\n",
      "Epoch 4 has a loss of 0.3252504765987396\n",
      "Epoch 4 has a loss of 0.10875903815031052\n",
      "Epoch 4 has a loss of 0.29774031043052673\n",
      "Epoch 4 has a loss of 0.2243238091468811\n",
      "Epoch 4 has a loss of 0.31713828444480896\n",
      "Epoch 5, Loss: 0.2740747563838959\n",
      "Epoch 6/15\n",
      "Epoch 5 has a loss of 0.22559161484241486\n",
      "Epoch 5 has a loss of 0.40053969621658325\n",
      "Epoch 5 has a loss of 0.17269974946975708\n",
      "Epoch 5 has a loss of 0.16593165695667267\n",
      "Epoch 5 has a loss of 0.14374321699142456\n",
      "Epoch 5 has a loss of 0.1359158605337143\n",
      "Epoch 5 has a loss of 0.07541355490684509\n",
      "Epoch 5 has a loss of 0.16094206273555756\n",
      "Epoch 5 has a loss of 0.34522002935409546\n",
      "Epoch 5 has a loss of 0.46604788303375244\n",
      "Epoch 5 has a loss of 0.1765250861644745\n",
      "Epoch 5 has a loss of 0.20076695084571838\n",
      "Epoch 6, Loss: 0.22296504807472228\n",
      "Epoch 7/15\n",
      "Epoch 6 has a loss of 0.4850536286830902\n",
      "Epoch 6 has a loss of 0.16101773083209991\n",
      "Epoch 6 has a loss of 0.21065476536750793\n",
      "Epoch 6 has a loss of 0.14049850404262543\n",
      "Epoch 6 has a loss of 0.20691853761672974\n",
      "Epoch 6 has a loss of 0.18002408742904663\n",
      "Epoch 6 has a loss of 0.31049615144729614\n",
      "Epoch 6 has a loss of 0.18113788962364197\n",
      "Epoch 6 has a loss of 0.18051455914974213\n",
      "Epoch 6 has a loss of 0.2377985417842865\n",
      "Epoch 6 has a loss of 0.17430400848388672\n",
      "Epoch 6 has a loss of 0.49264997243881226\n",
      "Epoch 7, Loss: 0.24085423549016316\n",
      "Epoch 8/15\n",
      "Epoch 7 has a loss of 0.12302786111831665\n",
      "Epoch 7 has a loss of 0.10113438218832016\n",
      "Epoch 7 has a loss of 0.29477354884147644\n",
      "Epoch 7 has a loss of 0.31539714336395264\n",
      "Epoch 7 has a loss of 0.7958254814147949\n",
      "Epoch 7 has a loss of 0.23098471760749817\n",
      "Epoch 7 has a loss of 0.15840254724025726\n",
      "Epoch 7 has a loss of 0.3748825490474701\n",
      "Epoch 7 has a loss of 0.15557748079299927\n",
      "Epoch 7 has a loss of 0.19741269946098328\n",
      "Epoch 7 has a loss of 0.1749543994665146\n",
      "Epoch 7 has a loss of 0.2893700897693634\n",
      "Epoch 8, Loss: 0.2671238453388214\n",
      "Epoch 9/15\n",
      "Epoch 8 has a loss of 0.10552974790334702\n",
      "Epoch 8 has a loss of 0.20497551560401917\n",
      "Epoch 8 has a loss of 0.4754217863082886\n",
      "Epoch 8 has a loss of 0.21651281416416168\n",
      "Epoch 8 has a loss of 0.20956926047801971\n",
      "Epoch 8 has a loss of 0.355679988861084\n",
      "Epoch 8 has a loss of 0.26547399163246155\n",
      "Epoch 8 has a loss of 0.1906939595937729\n",
      "Epoch 8 has a loss of 0.13255123794078827\n",
      "Epoch 8 has a loss of 0.3030468225479126\n",
      "Epoch 8 has a loss of 0.16366052627563477\n",
      "Epoch 8 has a loss of 0.7168789505958557\n",
      "Epoch 9, Loss: 0.2678077778816223\n",
      "Epoch 10/15\n",
      "Epoch 9 has a loss of 0.17690159380435944\n",
      "Epoch 9 has a loss of 0.11355240643024445\n",
      "Epoch 9 has a loss of 0.6004089713096619\n",
      "Epoch 9 has a loss of 0.12064303457736969\n",
      "Epoch 9 has a loss of 0.130018413066864\n",
      "Epoch 9 has a loss of 0.22227472066879272\n",
      "Epoch 9 has a loss of 0.23917575180530548\n",
      "Epoch 9 has a loss of 0.07514671981334686\n",
      "Epoch 9 has a loss of 0.21237324178218842\n",
      "Epoch 9 has a loss of 0.4277589023113251\n",
      "Epoch 9 has a loss of 0.1456717550754547\n",
      "Epoch 9 has a loss of 0.3471757769584656\n",
      "Epoch 10, Loss: 0.23154842456181843\n",
      "Epoch 11/15\n",
      "Epoch 10 has a loss of 0.13776807487010956\n",
      "Epoch 10 has a loss of 0.4775882959365845\n",
      "Epoch 10 has a loss of 0.1962931752204895\n",
      "Epoch 10 has a loss of 0.16522298753261566\n",
      "Epoch 10 has a loss of 0.14133712649345398\n",
      "Epoch 10 has a loss of 0.8995476365089417\n",
      "Epoch 10 has a loss of 0.1415606141090393\n",
      "Epoch 10 has a loss of 0.1868928074836731\n",
      "Epoch 10 has a loss of 0.11819520592689514\n",
      "Epoch 10 has a loss of 0.23100513219833374\n",
      "Epoch 10 has a loss of 0.061573006212711334\n",
      "Epoch 10 has a loss of 0.29335182905197144\n",
      "Epoch 11, Loss: 0.25325488551457725\n",
      "Epoch 12/15\n",
      "Epoch 11 has a loss of 0.27971917390823364\n",
      "Epoch 11 has a loss of 0.14150741696357727\n",
      "Epoch 11 has a loss of 0.5419014096260071\n",
      "Epoch 11 has a loss of 0.274320513010025\n",
      "Epoch 11 has a loss of 0.06955823302268982\n",
      "Epoch 11 has a loss of 0.27527737617492676\n",
      "Epoch 11 has a loss of 0.18590176105499268\n",
      "Epoch 11 has a loss of 0.1931811422109604\n",
      "Epoch 11 has a loss of 0.20129676163196564\n",
      "Epoch 11 has a loss of 0.08578463643789291\n",
      "Epoch 11 has a loss of 0.14453089237213135\n",
      "Epoch 11 has a loss of 0.15932074189186096\n",
      "Epoch 12, Loss: 0.2139725738366445\n",
      "Epoch 13/15\n",
      "Epoch 12 has a loss of 0.2164956033229828\n",
      "Epoch 12 has a loss of 0.12061367928981781\n",
      "Epoch 12 has a loss of 0.3263053297996521\n",
      "Epoch 12 has a loss of 0.16867318749427795\n",
      "Epoch 12 has a loss of 0.19683422148227692\n",
      "Epoch 12 has a loss of 0.16108381748199463\n",
      "Epoch 12 has a loss of 0.22492384910583496\n",
      "Epoch 12 has a loss of 0.14751729369163513\n",
      "Epoch 12 has a loss of 0.484485387802124\n",
      "Epoch 12 has a loss of 0.30195581912994385\n",
      "Epoch 12 has a loss of 0.13526733219623566\n",
      "Epoch 12 has a loss of 0.14593973755836487\n",
      "Epoch 13, Loss: 0.2209322416782379\n",
      "Epoch 14/15\n",
      "Epoch 13 has a loss of 0.14714457094669342\n",
      "Epoch 13 has a loss of 0.1439751833677292\n",
      "Epoch 13 has a loss of 0.3254045844078064\n",
      "Epoch 13 has a loss of 0.1533033698797226\n",
      "Epoch 13 has a loss of 0.18903253972530365\n",
      "Epoch 13 has a loss of 0.12805160880088806\n",
      "Epoch 13 has a loss of 0.12307789921760559\n",
      "Epoch 13 has a loss of 0.1332874596118927\n",
      "Epoch 13 has a loss of 0.19018274545669556\n",
      "Epoch 13 has a loss of 0.12485739588737488\n",
      "Epoch 13 has a loss of 0.2906537353992462\n",
      "Epoch 13 has a loss of 0.19725115597248077\n",
      "Epoch 14, Loss: 0.17841027081012725\n",
      "Epoch 15/15\n",
      "Epoch 14 has a loss of 0.1588028520345688\n",
      "Epoch 14 has a loss of 0.2489652782678604\n",
      "Epoch 14 has a loss of 0.24057312309741974\n",
      "Epoch 14 has a loss of 0.10554109513759613\n",
      "Epoch 14 has a loss of 0.15477079153060913\n",
      "Epoch 14 has a loss of 0.3217087388038635\n",
      "Epoch 14 has a loss of 0.21263767778873444\n",
      "Epoch 14 has a loss of 0.14106903970241547\n",
      "Epoch 14 has a loss of 0.225828617811203\n",
      "Epoch 14 has a loss of 0.18267282843589783\n",
      "Epoch 14 has a loss of 0.1420636624097824\n",
      "Epoch 14 has a loss of 0.20028647780418396\n",
      "Epoch 15, Loss: 0.19443964680035908\n",
      "Prototypes updated using weighted average.\n",
      "Saved model after self-training with T2PL.\n",
      "started_accuracy_computation\n",
      "{90.08}\n",
      "{90.44}\n",
      "{89.4}\n",
      "{89.0}\n",
      "{89.96}\n",
      "{89.68}\n",
      "{89.12}\n",
      "{90.48}\n",
      "{89.8}\n",
      "{90.0}\n",
      "{80.8}\n",
      "{65.72}\n",
      "{81.64}\n",
      "Epoch 1/15\n",
      "Epoch 0 has a loss of 0.1886838972568512\n",
      "Epoch 0 has a loss of 0.1295587569475174\n",
      "Epoch 0 has a loss of 0.3373225927352905\n",
      "Epoch 0 has a loss of 0.1186252310872078\n",
      "Epoch 0 has a loss of 0.11012127250432968\n",
      "Epoch 0 has a loss of 0.214161217212677\n",
      "Epoch 0 has a loss of 0.16731581091880798\n",
      "Epoch 0 has a loss of 0.17802850902080536\n",
      "Epoch 0 has a loss of 0.5112764835357666\n",
      "Epoch 0 has a loss of 0.2172226905822754\n",
      "Epoch 0 has a loss of 0.11412277817726135\n",
      "Epoch 0 has a loss of 0.21567033231258392\n",
      "Epoch 1, Loss: 0.2083372621933619\n",
      "Epoch 2/15\n",
      "Epoch 1 has a loss of 0.37772107124328613\n",
      "Epoch 1 has a loss of 0.29923245310783386\n",
      "Epoch 1 has a loss of 0.16928476095199585\n",
      "Epoch 1 has a loss of 0.11338960379362106\n",
      "Epoch 1 has a loss of 0.23515024781227112\n",
      "Epoch 1 has a loss of 0.1647823303937912\n",
      "Epoch 1 has a loss of 0.18592095375061035\n",
      "Epoch 1 has a loss of 0.09548041224479675\n",
      "Epoch 1 has a loss of 0.22591377794742584\n",
      "Epoch 1 has a loss of 0.35343438386917114\n",
      "Epoch 1 has a loss of 0.20146247744560242\n",
      "Epoch 1 has a loss of 0.14983485639095306\n",
      "Epoch 2, Loss: 0.2158477888504664\n",
      "Epoch 3/15\n",
      "Epoch 2 has a loss of 0.20920196175575256\n",
      "Epoch 2 has a loss of 0.07602007687091827\n",
      "Epoch 2 has a loss of 0.1931886225938797\n",
      "Epoch 2 has a loss of 0.11379770189523697\n",
      "Epoch 2 has a loss of 0.1435084193944931\n",
      "Epoch 2 has a loss of 0.10852216929197311\n",
      "Epoch 2 has a loss of 0.13641326129436493\n",
      "Epoch 2 has a loss of 0.1754334270954132\n",
      "Epoch 2 has a loss of 0.07765956223011017\n",
      "Epoch 2 has a loss of 0.17826758325099945\n",
      "Epoch 2 has a loss of 0.06182611733675003\n",
      "Epoch 2 has a loss of 0.15981726348400116\n",
      "Epoch 3, Loss: 0.13556971188386283\n",
      "Epoch 4/15\n",
      "Epoch 3 has a loss of 0.07527536153793335\n",
      "Epoch 3 has a loss of 0.16710825264453888\n",
      "Epoch 3 has a loss of 0.23375003039836884\n",
      "Epoch 3 has a loss of 0.07641246169805527\n",
      "Epoch 3 has a loss of 0.19007733464241028\n",
      "Epoch 3 has a loss of 0.13645318150520325\n",
      "Epoch 3 has a loss of 0.07885171473026276\n",
      "Epoch 3 has a loss of 0.1346834897994995\n",
      "Epoch 3 has a loss of 0.16580288112163544\n",
      "Epoch 3 has a loss of 0.08610978722572327\n",
      "Epoch 3 has a loss of 0.12029816210269928\n",
      "Epoch 3 has a loss of 0.055813103914260864\n",
      "Epoch 4, Loss: 0.12842140380541484\n",
      "Epoch 5/15\n",
      "Epoch 4 has a loss of 0.08398320525884628\n",
      "Epoch 4 has a loss of 0.2865702211856842\n",
      "Epoch 4 has a loss of 0.15764424204826355\n",
      "Epoch 4 has a loss of 0.16100402176380157\n",
      "Epoch 4 has a loss of 0.12310291081666946\n",
      "Epoch 4 has a loss of 0.1756184697151184\n",
      "Epoch 4 has a loss of 0.12928690016269684\n",
      "Epoch 4 has a loss of 0.11118806153535843\n",
      "Epoch 4 has a loss of 0.35588663816452026\n",
      "Epoch 4 has a loss of 0.09656161814928055\n",
      "Epoch 4 has a loss of 0.07144617289304733\n",
      "Epoch 4 has a loss of 0.11702798306941986\n",
      "Epoch 5, Loss: 0.1567066730260849\n",
      "Epoch 6/15\n",
      "Epoch 5 has a loss of 0.09646326303482056\n",
      "Epoch 5 has a loss of 0.07397454231977463\n",
      "Epoch 5 has a loss of 0.09714776277542114\n",
      "Epoch 5 has a loss of 0.17352481186389923\n",
      "Epoch 5 has a loss of 0.16355572640895844\n",
      "Epoch 5 has a loss of 0.13602016866207123\n",
      "Epoch 5 has a loss of 0.44487375020980835\n",
      "Epoch 5 has a loss of 0.12366526573896408\n",
      "Epoch 5 has a loss of 0.10730462521314621\n",
      "Epoch 5 has a loss of 0.3379793167114258\n",
      "Epoch 5 has a loss of 0.09109627455472946\n",
      "Epoch 5 has a loss of 0.09710478782653809\n",
      "Epoch 6, Loss: 0.1634474302927653\n",
      "Epoch 7/15\n",
      "Epoch 6 has a loss of 0.4591083228588104\n",
      "Epoch 6 has a loss of 0.07147186994552612\n",
      "Epoch 6 has a loss of 0.06634864956140518\n",
      "Epoch 6 has a loss of 0.0764613077044487\n",
      "Epoch 6 has a loss of 0.09637583792209625\n",
      "Epoch 6 has a loss of 0.1688907891511917\n",
      "Epoch 6 has a loss of 0.10202547162771225\n",
      "Epoch 6 has a loss of 0.16910088062286377\n",
      "Epoch 6 has a loss of 0.1200934499502182\n",
      "Epoch 6 has a loss of 0.13670049607753754\n",
      "Epoch 6 has a loss of 0.06920438259840012\n",
      "Epoch 6 has a loss of 0.07480856031179428\n",
      "Epoch 7, Loss: 0.13564160945018133\n",
      "Epoch 8/15\n",
      "Epoch 7 has a loss of 0.12668360769748688\n",
      "Epoch 7 has a loss of 0.07585424929857254\n",
      "Epoch 7 has a loss of 0.05500687658786774\n",
      "Epoch 7 has a loss of 0.0897088572382927\n",
      "Epoch 7 has a loss of 0.1489606648683548\n",
      "Epoch 7 has a loss of 0.25351816415786743\n",
      "Epoch 7 has a loss of 0.10161706805229187\n",
      "Epoch 7 has a loss of 0.06413126736879349\n",
      "Epoch 7 has a loss of 0.07341264188289642\n",
      "Epoch 7 has a loss of 0.09923845529556274\n",
      "Epoch 7 has a loss of 0.11205313354730606\n",
      "Epoch 7 has a loss of 0.06708178669214249\n",
      "Epoch 8, Loss: 0.10653013505538304\n",
      "Epoch 9/15\n",
      "Epoch 8 has a loss of 0.1171332523226738\n",
      "Epoch 8 has a loss of 0.1104857325553894\n",
      "Epoch 8 has a loss of 0.1127617210149765\n",
      "Epoch 8 has a loss of 0.1451754868030548\n",
      "Epoch 8 has a loss of 0.09637514501810074\n",
      "Epoch 8 has a loss of 0.105101577937603\n",
      "Epoch 8 has a loss of 0.06255679577589035\n",
      "Epoch 8 has a loss of 0.0885419249534607\n",
      "Epoch 8 has a loss of 0.05515369027853012\n",
      "Epoch 8 has a loss of 0.039480552077293396\n",
      "Epoch 8 has a loss of 0.16055595874786377\n",
      "Epoch 8 has a loss of 0.28123998641967773\n",
      "Epoch 9, Loss: 0.11054618263244628\n",
      "Epoch 10/15\n",
      "Epoch 9 has a loss of 0.0850883200764656\n",
      "Epoch 9 has a loss of 0.0720742791891098\n",
      "Epoch 9 has a loss of 0.08178652077913284\n",
      "Epoch 9 has a loss of 0.11398617178201675\n",
      "Epoch 9 has a loss of 0.17223459482192993\n",
      "Epoch 9 has a loss of 0.23475566506385803\n",
      "Epoch 9 has a loss of 0.10300450026988983\n",
      "Epoch 9 has a loss of 0.1150599792599678\n",
      "Epoch 9 has a loss of 0.07392960786819458\n",
      "Epoch 9 has a loss of 0.08966080844402313\n",
      "Epoch 9 has a loss of 0.11500327289104462\n",
      "Epoch 9 has a loss of 0.08347706496715546\n",
      "Epoch 10, Loss: 0.11234840412934621\n",
      "Epoch 11/15\n",
      "Epoch 10 has a loss of 0.12547355890274048\n",
      "Epoch 10 has a loss of 0.12498964369297028\n",
      "Epoch 10 has a loss of 0.04971484839916229\n",
      "Epoch 10 has a loss of 0.09836538881063461\n",
      "Epoch 10 has a loss of 0.10577976703643799\n",
      "Epoch 10 has a loss of 0.20316331088542938\n",
      "Epoch 10 has a loss of 0.11211962252855301\n",
      "Epoch 10 has a loss of 0.06439279019832611\n",
      "Epoch 10 has a loss of 0.08039577305316925\n",
      "Epoch 10 has a loss of 0.11560273170471191\n",
      "Epoch 10 has a loss of 0.07096173614263535\n",
      "Epoch 10 has a loss of 0.03743155300617218\n",
      "Epoch 11, Loss: 0.10051098453998565\n",
      "Epoch 12/15\n",
      "Epoch 11 has a loss of 0.09870121628046036\n",
      "Epoch 11 has a loss of 0.06567095220088959\n",
      "Epoch 11 has a loss of 0.21291527152061462\n",
      "Epoch 11 has a loss of 0.12769468128681183\n",
      "Epoch 11 has a loss of 0.14159265160560608\n",
      "Epoch 11 has a loss of 0.1511622667312622\n",
      "Epoch 11 has a loss of 0.07181835919618607\n",
      "Epoch 11 has a loss of 0.07746857404708862\n",
      "Epoch 11 has a loss of 0.16642946004867554\n",
      "Epoch 11 has a loss of 0.077661894261837\n",
      "Epoch 11 has a loss of 0.10477931052446365\n",
      "Epoch 11 has a loss of 0.15340913832187653\n",
      "Epoch 12, Loss: 0.11999210290114085\n",
      "Epoch 13/15\n",
      "Epoch 12 has a loss of 0.0838879644870758\n",
      "Epoch 12 has a loss of 0.1232522651553154\n",
      "Epoch 12 has a loss of 0.042959533631801605\n",
      "Epoch 12 has a loss of 0.09782487899065018\n",
      "Epoch 12 has a loss of 0.18139350414276123\n",
      "Epoch 12 has a loss of 0.13823795318603516\n",
      "Epoch 12 has a loss of 0.111538365483284\n",
      "Epoch 12 has a loss of 0.10808446258306503\n",
      "Epoch 12 has a loss of 0.0795585885643959\n",
      "Epoch 12 has a loss of 0.27654263377189636\n",
      "Epoch 12 has a loss of 0.10385146737098694\n",
      "Epoch 12 has a loss of 0.2260361760854721\n",
      "Epoch 13, Loss: 0.1288187834819158\n",
      "Epoch 14/15\n",
      "Epoch 13 has a loss of 0.08256953954696655\n",
      "Epoch 13 has a loss of 0.29681363701820374\n",
      "Epoch 13 has a loss of 0.09008059650659561\n",
      "Epoch 13 has a loss of 0.48862531781196594\n",
      "Epoch 13 has a loss of 0.05953581631183624\n",
      "Epoch 13 has a loss of 0.12952938675880432\n",
      "Epoch 13 has a loss of 0.06585751473903656\n",
      "Epoch 13 has a loss of 0.042830102145671844\n",
      "Epoch 13 has a loss of 0.23334844410419464\n",
      "Epoch 13 has a loss of 0.20848029851913452\n",
      "Epoch 13 has a loss of 0.1802024394273758\n",
      "Epoch 13 has a loss of 0.06717763841152191\n",
      "Epoch 14, Loss: 0.16436539908250172\n",
      "Epoch 15/15\n",
      "Epoch 14 has a loss of 0.07032754272222519\n",
      "Epoch 14 has a loss of 0.11425498127937317\n",
      "Epoch 14 has a loss of 0.2887243926525116\n",
      "Epoch 14 has a loss of 0.16700479388237\n",
      "Epoch 14 has a loss of 0.06447076052427292\n",
      "Epoch 14 has a loss of 0.05737406015396118\n",
      "Epoch 14 has a loss of 0.1514236032962799\n",
      "Epoch 14 has a loss of 0.1404993087053299\n",
      "Epoch 14 has a loss of 0.09747417271137238\n",
      "Epoch 14 has a loss of 0.1137004941701889\n",
      "Epoch 14 has a loss of 0.07056127488613129\n",
      "Epoch 14 has a loss of 0.1385648548603058\n",
      "Epoch 15, Loss: 0.12248822395006816\n",
      "Prototypes updated using weighted average.\n",
      "Saved model after self-training with T2PL.\n",
      "started_accuracy_computation\n",
      "{89.52}\n",
      "{89.52}\n",
      "{88.32}\n",
      "{87.84}\n",
      "{88.6}\n",
      "{89.12}\n",
      "{89.04}\n",
      "{89.08}\n",
      "{88.96}\n",
      "{88.36}\n",
      "{83.0}\n",
      "{63.88}\n",
      "{78.88}\n",
      "{88.84}\n",
      "Epoch 1/15\n",
      "Epoch 0 has a loss of 0.16940513253211975\n",
      "Epoch 0 has a loss of 0.36501890420913696\n",
      "Epoch 0 has a loss of 0.12230105698108673\n",
      "Epoch 0 has a loss of 0.1335938274860382\n",
      "Epoch 0 has a loss of 0.07601960748434067\n",
      "Epoch 0 has a loss of 0.11520363390445709\n",
      "Epoch 0 has a loss of 0.10838731378316879\n",
      "Epoch 0 has a loss of 0.11580706387758255\n",
      "Epoch 0 has a loss of 0.788521409034729\n",
      "Epoch 0 has a loss of 0.051128827035427094\n",
      "Epoch 0 has a loss of 0.08564674109220505\n",
      "Epoch 0 has a loss of 0.5006241798400879\n",
      "Epoch 1, Loss: 0.2125531431833903\n",
      "Epoch 2/15\n",
      "Epoch 1 has a loss of 0.13493558764457703\n",
      "Epoch 1 has a loss of 0.0580667220056057\n",
      "Epoch 1 has a loss of 0.27426230907440186\n",
      "Epoch 1 has a loss of 0.08033803105354309\n",
      "Epoch 1 has a loss of 0.08777156472206116\n",
      "Epoch 1 has a loss of 0.25138944387435913\n",
      "Epoch 1 has a loss of 0.029389498755335808\n",
      "Epoch 1 has a loss of 0.029810821637511253\n",
      "Epoch 1 has a loss of 0.07198874652385712\n",
      "Epoch 1 has a loss of 0.2185906022787094\n",
      "Epoch 1 has a loss of 0.10776487737894058\n",
      "Epoch 1 has a loss of 0.1319633424282074\n",
      "Epoch 2, Loss: 0.1228080518245697\n",
      "Epoch 3/15\n",
      "Epoch 2 has a loss of 0.040976595133543015\n",
      "Epoch 2 has a loss of 0.03182118758559227\n",
      "Epoch 2 has a loss of 0.04403757303953171\n",
      "Epoch 2 has a loss of 0.09459002315998077\n",
      "Epoch 2 has a loss of 0.03251344710588455\n",
      "Epoch 2 has a loss of 0.10238451510667801\n",
      "Epoch 2 has a loss of 0.08648869395256042\n",
      "Epoch 2 has a loss of 0.0846213549375534\n",
      "Epoch 2 has a loss of 0.057111408561468124\n",
      "Epoch 2 has a loss of 0.034173112362623215\n",
      "Epoch 2 has a loss of 0.1075747087597847\n",
      "Epoch 2 has a loss of 0.14408737421035767\n",
      "Epoch 3, Loss: 0.06996099583307902\n",
      "Epoch 4/15\n",
      "Epoch 3 has a loss of 0.06909599155187607\n",
      "Epoch 3 has a loss of 0.09303219616413116\n",
      "Epoch 3 has a loss of 0.07029473036527634\n",
      "Epoch 3 has a loss of 0.14513449370861053\n",
      "Epoch 3 has a loss of 0.08950074017047882\n",
      "Epoch 3 has a loss of 0.05310196802020073\n",
      "Epoch 3 has a loss of 0.05145720764994621\n",
      "Epoch 3 has a loss of 0.13539399206638336\n",
      "Epoch 3 has a loss of 0.09445346146821976\n",
      "Epoch 3 has a loss of 0.04083921015262604\n",
      "Epoch 3 has a loss of 0.17158600687980652\n",
      "Epoch 3 has a loss of 0.12561529874801636\n",
      "Epoch 4, Loss: 0.09422301816940308\n",
      "Epoch 5/15\n",
      "Epoch 4 has a loss of 0.16327247023582458\n",
      "Epoch 4 has a loss of 0.07176212221384048\n",
      "Epoch 4 has a loss of 0.07414804399013519\n",
      "Epoch 4 has a loss of 0.05882536992430687\n",
      "Epoch 4 has a loss of 0.08849845081567764\n",
      "Epoch 4 has a loss of 0.11516250669956207\n",
      "Epoch 4 has a loss of 0.03246327117085457\n",
      "Epoch 4 has a loss of 0.17794761061668396\n",
      "Epoch 4 has a loss of 0.09674649685621262\n",
      "Epoch 4 has a loss of 0.07467096298933029\n",
      "Epoch 4 has a loss of 0.09223508089780807\n",
      "Epoch 4 has a loss of 0.03223612904548645\n",
      "Epoch 5, Loss: 0.09121297955513001\n",
      "Epoch 6/15\n",
      "Epoch 5 has a loss of 0.025832563638687134\n",
      "Epoch 5 has a loss of 0.04981377348303795\n",
      "Epoch 5 has a loss of 0.08609793335199356\n",
      "Epoch 5 has a loss of 0.07865414023399353\n",
      "Epoch 5 has a loss of 0.06442313641309738\n",
      "Epoch 5 has a loss of 0.06855736672878265\n",
      "Epoch 5 has a loss of 0.09803373366594315\n",
      "Epoch 5 has a loss of 0.11046382784843445\n",
      "Epoch 5 has a loss of 0.0983157530426979\n",
      "Epoch 5 has a loss of 0.07431894540786743\n",
      "Epoch 5 has a loss of 0.07563550025224686\n",
      "Epoch 5 has a loss of 0.040860071778297424\n",
      "Epoch 6, Loss: 0.07334526725610098\n",
      "Epoch 7/15\n",
      "Epoch 6 has a loss of 0.08533547818660736\n",
      "Epoch 6 has a loss of 0.048029251396656036\n",
      "Epoch 6 has a loss of 0.08272529393434525\n",
      "Epoch 6 has a loss of 0.13373039662837982\n",
      "Epoch 6 has a loss of 0.07609596103429794\n",
      "Epoch 6 has a loss of 0.1846570521593094\n",
      "Epoch 6 has a loss of 0.10408996045589447\n",
      "Epoch 6 has a loss of 0.029037194326519966\n",
      "Epoch 6 has a loss of 0.06843693554401398\n",
      "Epoch 6 has a loss of 0.0514165535569191\n",
      "Epoch 6 has a loss of 0.091708704829216\n",
      "Epoch 6 has a loss of 0.12036904692649841\n",
      "Epoch 7, Loss: 0.08889839227994283\n",
      "Epoch 8/15\n",
      "Epoch 7 has a loss of 0.09874050319194794\n",
      "Epoch 7 has a loss of 0.04222238063812256\n",
      "Epoch 7 has a loss of 0.0661468356847763\n",
      "Epoch 7 has a loss of 0.09655975550413132\n",
      "Epoch 7 has a loss of 0.06338749825954437\n",
      "Epoch 7 has a loss of 0.09122249484062195\n",
      "Epoch 7 has a loss of 0.03957238048315048\n",
      "Epoch 7 has a loss of 0.07647819072008133\n",
      "Epoch 7 has a loss of 0.08385968953371048\n",
      "Epoch 7 has a loss of 0.059547845274209976\n",
      "Epoch 7 has a loss of 0.08302076905965805\n",
      "Epoch 7 has a loss of 0.04438391327857971\n",
      "Epoch 8, Loss: 0.07105359196662903\n",
      "Epoch 9/15\n",
      "Epoch 8 has a loss of 0.09336043894290924\n",
      "Epoch 8 has a loss of 0.02507551573216915\n",
      "Epoch 8 has a loss of 0.16524948179721832\n",
      "Epoch 8 has a loss of 0.028935136273503304\n",
      "Epoch 8 has a loss of 0.024744372814893723\n",
      "Epoch 8 has a loss of 0.15420065820217133\n",
      "Epoch 8 has a loss of 0.13000771403312683\n",
      "Epoch 8 has a loss of 0.02944689616560936\n",
      "Epoch 8 has a loss of 0.05378294736146927\n",
      "Epoch 8 has a loss of 0.07126861065626144\n",
      "Epoch 8 has a loss of 0.059794336557388306\n",
      "Epoch 8 has a loss of 0.05800163000822067\n",
      "Epoch 9, Loss: 0.074884674568971\n",
      "Epoch 10/15\n",
      "Epoch 9 has a loss of 0.081600122153759\n",
      "Epoch 9 has a loss of 0.12784840166568756\n",
      "Epoch 9 has a loss of 0.0431174635887146\n",
      "Epoch 9 has a loss of 0.07268386334180832\n",
      "Epoch 9 has a loss of 0.04027819260954857\n",
      "Epoch 9 has a loss of 0.03219818323850632\n",
      "Epoch 9 has a loss of 0.10569694638252258\n",
      "Epoch 9 has a loss of 0.09010660648345947\n",
      "Epoch 9 has a loss of 0.09530600160360336\n",
      "Epoch 9 has a loss of 0.10605643689632416\n",
      "Epoch 9 has a loss of 0.06461719423532486\n",
      "Epoch 9 has a loss of 0.07844701409339905\n",
      "Epoch 10, Loss: 0.07815622003873189\n",
      "Epoch 11/15\n",
      "Epoch 10 has a loss of 0.08260875195264816\n",
      "Epoch 10 has a loss of 0.05904669314622879\n",
      "Epoch 10 has a loss of 0.07124878466129303\n",
      "Epoch 10 has a loss of 0.0512780100107193\n",
      "Epoch 10 has a loss of 0.12310240417718887\n",
      "Epoch 10 has a loss of 0.03539042919874191\n",
      "Epoch 10 has a loss of 0.01947956532239914\n",
      "Epoch 10 has a loss of 0.029935602098703384\n",
      "Epoch 10 has a loss of 0.026604028418660164\n",
      "Epoch 10 has a loss of 0.035513684153556824\n",
      "Epoch 10 has a loss of 0.053267017006874084\n",
      "Epoch 10 has a loss of 0.09252618998289108\n",
      "Epoch 11, Loss: 0.05580613710482915\n",
      "Epoch 12/15\n",
      "Epoch 11 has a loss of 0.050660159438848495\n",
      "Epoch 11 has a loss of 0.05931384861469269\n",
      "Epoch 11 has a loss of 0.12496092915534973\n",
      "Epoch 11 has a loss of 0.07884450256824493\n",
      "Epoch 11 has a loss of 0.06580974161624908\n",
      "Epoch 11 has a loss of 0.07920779287815094\n",
      "Epoch 11 has a loss of 0.060043297708034515\n",
      "Epoch 11 has a loss of 0.07695497572422028\n",
      "Epoch 11 has a loss of 0.0503738708794117\n",
      "Epoch 11 has a loss of 0.09345778077840805\n",
      "Epoch 11 has a loss of 0.05449282377958298\n",
      "Epoch 11 has a loss of 0.05955185741186142\n",
      "Epoch 12, Loss: 0.07141739696264267\n",
      "Epoch 13/15\n",
      "Epoch 12 has a loss of 0.032052796334028244\n",
      "Epoch 12 has a loss of 0.058620650321245193\n",
      "Epoch 12 has a loss of 0.15779200196266174\n",
      "Epoch 12 has a loss of 0.04906629025936127\n",
      "Epoch 12 has a loss of 0.07584959268569946\n",
      "Epoch 12 has a loss of 0.08818022906780243\n",
      "Epoch 12 has a loss of 0.1358652412891388\n",
      "Epoch 12 has a loss of 0.09192368388175964\n",
      "Epoch 12 has a loss of 0.046116624027490616\n",
      "Epoch 12 has a loss of 0.08572657406330109\n",
      "Epoch 12 has a loss of 0.0650806725025177\n",
      "Epoch 12 has a loss of 0.13190819323062897\n",
      "Epoch 13, Loss: 0.08371911426385244\n",
      "Epoch 14/15\n",
      "Epoch 13 has a loss of 0.10637135058641434\n",
      "Epoch 13 has a loss of 0.07563069462776184\n",
      "Epoch 13 has a loss of 0.06825327128171921\n",
      "Epoch 13 has a loss of 0.09861551225185394\n",
      "Epoch 13 has a loss of 0.05739229917526245\n",
      "Epoch 13 has a loss of 0.06265819072723389\n",
      "Epoch 13 has a loss of 0.03578764945268631\n",
      "Epoch 13 has a loss of 0.03584613651037216\n",
      "Epoch 13 has a loss of 0.1727796047925949\n",
      "Epoch 13 has a loss of 0.05886157602071762\n",
      "Epoch 13 has a loss of 0.11012205481529236\n",
      "Epoch 13 has a loss of 0.08549010753631592\n",
      "Epoch 14, Loss: 0.08053455829620361\n",
      "Epoch 15/15\n",
      "Epoch 14 has a loss of 0.058157239109277725\n",
      "Epoch 14 has a loss of 0.08000227808952332\n",
      "Epoch 14 has a loss of 0.1120385155081749\n",
      "Epoch 14 has a loss of 0.06364497542381287\n",
      "Epoch 14 has a loss of 0.029862582683563232\n",
      "Epoch 14 has a loss of 0.176506906747818\n",
      "Epoch 14 has a loss of 0.052630484104156494\n",
      "Epoch 14 has a loss of 0.06560459733009338\n",
      "Epoch 14 has a loss of 0.1694270372390747\n",
      "Epoch 14 has a loss of 0.06259430199861526\n",
      "Epoch 14 has a loss of 0.06730205565690994\n",
      "Epoch 14 has a loss of 0.050155483186244965\n",
      "Epoch 15, Loss: 0.08309932607412339\n",
      "Prototypes updated using weighted average.\n",
      "Saved model after self-training with T2PL.\n",
      "started_accuracy_computation\n",
      "{90.24}\n",
      "{90.32}\n",
      "{88.8}\n",
      "{89.0}\n",
      "{89.68}\n",
      "{89.6}\n",
      "{89.16}\n",
      "{90.2}\n",
      "{89.68}\n",
      "{89.76}\n",
      "{83.72}\n",
      "{60.12}\n",
      "{78.56}\n",
      "{88.4}\n",
      "{89.6}\n",
      "Epoch 1/15\n",
      "Epoch 0 has a loss of 0.4590979218482971\n",
      "Epoch 0 has a loss of 0.8532388210296631\n",
      "Epoch 0 has a loss of 0.1450655162334442\n",
      "Epoch 0 has a loss of 0.2500542104244232\n",
      "Epoch 0 has a loss of 0.24381014704704285\n",
      "Epoch 0 has a loss of 0.2786494493484497\n",
      "Epoch 0 has a loss of 0.12208885699510574\n",
      "Epoch 0 has a loss of 0.37167787551879883\n",
      "Epoch 0 has a loss of 0.08481835573911667\n",
      "Epoch 0 has a loss of 0.24573373794555664\n",
      "Epoch 0 has a loss of 0.3522329032421112\n",
      "Epoch 0 has a loss of 0.07958977669477463\n",
      "Epoch 1, Loss: 0.29556675817569095\n",
      "Epoch 2/15\n",
      "Epoch 1 has a loss of 0.2716132700443268\n",
      "Epoch 1 has a loss of 0.11045253276824951\n",
      "Epoch 1 has a loss of 0.08785001933574677\n",
      "Epoch 1 has a loss of 0.13878168165683746\n",
      "Epoch 1 has a loss of 0.0867711752653122\n",
      "Epoch 1 has a loss of 0.2401021122932434\n",
      "Epoch 1 has a loss of 0.22160600125789642\n",
      "Epoch 1 has a loss of 0.21323245763778687\n",
      "Epoch 1 has a loss of 0.15777549147605896\n",
      "Epoch 1 has a loss of 0.1532600224018097\n",
      "Epoch 1 has a loss of 0.13706718385219574\n",
      "Epoch 1 has a loss of 0.14606383442878723\n",
      "Epoch 2, Loss: 0.16413826807339987\n",
      "Epoch 3/15\n",
      "Epoch 2 has a loss of 0.12774425745010376\n",
      "Epoch 2 has a loss of 0.17965391278266907\n",
      "Epoch 2 has a loss of 0.06567121297121048\n",
      "Epoch 2 has a loss of 0.13125455379486084\n",
      "Epoch 2 has a loss of 0.14935895800590515\n",
      "Epoch 2 has a loss of 0.09975510835647583\n",
      "Epoch 2 has a loss of 0.17615850269794464\n",
      "Epoch 2 has a loss of 0.08327029645442963\n",
      "Epoch 2 has a loss of 0.06542561203241348\n",
      "Epoch 2 has a loss of 0.1260191798210144\n",
      "Epoch 2 has a loss of 0.1669674962759018\n",
      "Epoch 2 has a loss of 0.10452403873205185\n",
      "Epoch 3, Loss: 0.12342662344376246\n",
      "Epoch 4/15\n",
      "Epoch 3 has a loss of 0.06088294833898544\n",
      "Epoch 3 has a loss of 0.10191255062818527\n",
      "Epoch 3 has a loss of 0.22980189323425293\n",
      "Epoch 3 has a loss of 0.1107044667005539\n",
      "Epoch 3 has a loss of 0.10105752944946289\n",
      "Epoch 3 has a loss of 0.056612301617860794\n",
      "Epoch 3 has a loss of 0.07614254206418991\n",
      "Epoch 3 has a loss of 0.07397253811359406\n",
      "Epoch 3 has a loss of 0.07132742553949356\n",
      "Epoch 3 has a loss of 0.05167365446686745\n",
      "Epoch 3 has a loss of 0.1669614166021347\n",
      "Epoch 3 has a loss of 0.15361663699150085\n",
      "Epoch 4, Loss: 0.10337802449862163\n",
      "Epoch 5/15\n",
      "Epoch 4 has a loss of 0.05232108011841774\n",
      "Epoch 4 has a loss of 0.24796950817108154\n",
      "Epoch 4 has a loss of 0.05418330803513527\n",
      "Epoch 4 has a loss of 0.0957368016242981\n",
      "Epoch 4 has a loss of 0.07085397094488144\n",
      "Epoch 4 has a loss of 0.10106658935546875\n",
      "Epoch 4 has a loss of 0.19304922223091125\n",
      "Epoch 4 has a loss of 0.10102926194667816\n",
      "Epoch 4 has a loss of 0.09113158285617828\n",
      "Epoch 4 has a loss of 0.18007391691207886\n",
      "Epoch 4 has a loss of 0.07626061141490936\n",
      "Epoch 4 has a loss of 0.070556640625\n",
      "Epoch 5, Loss: 0.1121611467997233\n",
      "Epoch 6/15\n",
      "Epoch 5 has a loss of 0.26498615741729736\n",
      "Epoch 5 has a loss of 0.09406867623329163\n",
      "Epoch 5 has a loss of 0.2568913996219635\n",
      "Epoch 5 has a loss of 0.17972879111766815\n",
      "Epoch 5 has a loss of 0.11485528945922852\n",
      "Epoch 5 has a loss of 0.17474842071533203\n",
      "Epoch 5 has a loss of 0.1548963189125061\n",
      "Epoch 5 has a loss of 0.07876341044902802\n",
      "Epoch 5 has a loss of 0.06814774125814438\n",
      "Epoch 5 has a loss of 0.07376149296760559\n",
      "Epoch 5 has a loss of 0.08572715520858765\n",
      "Epoch 5 has a loss of 0.09139095991849899\n",
      "Epoch 6, Loss: 0.13757969969511033\n",
      "Epoch 7/15\n",
      "Epoch 6 has a loss of 0.08856216073036194\n",
      "Epoch 6 has a loss of 0.12089211493730545\n",
      "Epoch 6 has a loss of 0.21364682912826538\n",
      "Epoch 6 has a loss of 0.09306800365447998\n",
      "Epoch 6 has a loss of 0.10860007256269455\n",
      "Epoch 6 has a loss of 0.06087562069296837\n",
      "Epoch 6 has a loss of 0.11278235912322998\n",
      "Epoch 6 has a loss of 0.08723416924476624\n",
      "Epoch 6 has a loss of 0.08686553686857224\n",
      "Epoch 6 has a loss of 0.08429321646690369\n",
      "Epoch 6 has a loss of 0.22562876343727112\n",
      "Epoch 6 has a loss of 0.3348166346549988\n",
      "Epoch 7, Loss: 0.12997105518976848\n",
      "Epoch 8/15\n",
      "Epoch 7 has a loss of 0.10004694759845734\n",
      "Epoch 7 has a loss of 0.08802381157875061\n",
      "Epoch 7 has a loss of 0.13273000717163086\n",
      "Epoch 7 has a loss of 0.09285958111286163\n",
      "Epoch 7 has a loss of 0.10038511455059052\n",
      "Epoch 7 has a loss of 0.11750075221061707\n",
      "Epoch 7 has a loss of 0.07562775164842606\n",
      "Epoch 7 has a loss of 0.16547054052352905\n",
      "Epoch 7 has a loss of 0.06975604593753815\n",
      "Epoch 7 has a loss of 0.10321440547704697\n",
      "Epoch 7 has a loss of 0.09731464833021164\n",
      "Epoch 7 has a loss of 0.11641514301300049\n",
      "Epoch 8, Loss: 0.104670121828715\n",
      "Epoch 9/15\n",
      "Epoch 8 has a loss of 0.061453383415937424\n",
      "Epoch 8 has a loss of 0.0942121222615242\n",
      "Epoch 8 has a loss of 0.20659103989601135\n",
      "Epoch 8 has a loss of 0.08249732106924057\n",
      "Epoch 8 has a loss of 0.047840774059295654\n",
      "Epoch 8 has a loss of 0.07050860673189163\n",
      "Epoch 8 has a loss of 0.1016116663813591\n",
      "Epoch 8 has a loss of 0.081565260887146\n",
      "Epoch 8 has a loss of 0.0423082634806633\n",
      "Epoch 8 has a loss of 0.23898519575595856\n",
      "Epoch 8 has a loss of 0.0761215090751648\n",
      "Epoch 8 has a loss of 0.1421331763267517\n",
      "Epoch 9, Loss: 0.1028994870185852\n",
      "Epoch 10/15\n",
      "Epoch 9 has a loss of 0.04668516665697098\n",
      "Epoch 9 has a loss of 0.6197622418403625\n",
      "Epoch 9 has a loss of 0.044651422649621964\n",
      "Epoch 9 has a loss of 0.1535840630531311\n",
      "Epoch 9 has a loss of 0.05919541046023369\n",
      "Epoch 9 has a loss of 0.24884501099586487\n",
      "Epoch 9 has a loss of 0.1281351000070572\n",
      "Epoch 9 has a loss of 0.11358622461557388\n",
      "Epoch 9 has a loss of 0.09306299686431885\n",
      "Epoch 9 has a loss of 0.05442993715405464\n",
      "Epoch 9 has a loss of 0.07238392531871796\n",
      "Epoch 9 has a loss of 0.09404000639915466\n",
      "Epoch 10, Loss: 0.1452298883597056\n",
      "Epoch 11/15\n",
      "Epoch 10 has a loss of 0.11092420667409897\n",
      "Epoch 10 has a loss of 0.3096669316291809\n",
      "Epoch 10 has a loss of 0.07512073218822479\n",
      "Epoch 10 has a loss of 0.08015742152929306\n",
      "Epoch 10 has a loss of 0.05387723818421364\n",
      "Epoch 10 has a loss of 0.04266975820064545\n",
      "Epoch 10 has a loss of 0.07958753407001495\n",
      "Epoch 10 has a loss of 0.0513434074819088\n",
      "Epoch 10 has a loss of 0.06384997069835663\n",
      "Epoch 10 has a loss of 0.040720488876104355\n",
      "Epoch 10 has a loss of 0.0997069925069809\n",
      "Epoch 10 has a loss of 0.1732606291770935\n",
      "Epoch 11, Loss: 0.09661062479019165\n",
      "Epoch 12/15\n",
      "Epoch 11 has a loss of 0.07945004105567932\n",
      "Epoch 11 has a loss of 0.05903065204620361\n",
      "Epoch 11 has a loss of 0.06473325937986374\n",
      "Epoch 11 has a loss of 0.06623819470405579\n",
      "Epoch 11 has a loss of 0.16002953052520752\n",
      "Epoch 11 has a loss of 0.0952598974108696\n",
      "Epoch 11 has a loss of 0.1157585084438324\n",
      "Epoch 11 has a loss of 0.0807235911488533\n",
      "Epoch 11 has a loss of 0.08835064619779587\n",
      "Epoch 11 has a loss of 0.08517683297395706\n",
      "Epoch 11 has a loss of 0.11879079788923264\n",
      "Epoch 11 has a loss of 0.06423554569482803\n",
      "Epoch 12, Loss: 0.09042869335412979\n",
      "Epoch 13/15\n",
      "Epoch 12 has a loss of 0.0907430425286293\n",
      "Epoch 12 has a loss of 0.07378806918859482\n",
      "Epoch 12 has a loss of 0.10812272876501083\n",
      "Epoch 12 has a loss of 0.06837835162878036\n",
      "Epoch 12 has a loss of 0.06175963953137398\n",
      "Epoch 12 has a loss of 0.12402814626693726\n",
      "Epoch 12 has a loss of 0.0872822105884552\n",
      "Epoch 12 has a loss of 0.08407320082187653\n",
      "Epoch 12 has a loss of 0.08885155618190765\n",
      "Epoch 12 has a loss of 0.09206559509038925\n",
      "Epoch 12 has a loss of 0.03604312986135483\n",
      "Epoch 12 has a loss of 0.13515684008598328\n",
      "Epoch 13, Loss: 0.08638119673728943\n",
      "Epoch 14/15\n",
      "Epoch 13 has a loss of 0.08776004612445831\n",
      "Epoch 13 has a loss of 0.08083921670913696\n",
      "Epoch 13 has a loss of 0.06099855899810791\n",
      "Epoch 13 has a loss of 0.20337824523448944\n",
      "Epoch 13 has a loss of 0.0720815509557724\n",
      "Epoch 13 has a loss of 0.04894060268998146\n",
      "Epoch 13 has a loss of 0.23549845814704895\n",
      "Epoch 13 has a loss of 0.11152470111846924\n",
      "Epoch 13 has a loss of 0.0695192813873291\n",
      "Epoch 13 has a loss of 0.13776570558547974\n",
      "Epoch 13 has a loss of 0.12454219907522202\n",
      "Epoch 13 has a loss of 0.11754696071147919\n",
      "Epoch 14, Loss: 0.112412624557813\n",
      "Epoch 15/15\n",
      "Epoch 14 has a loss of 0.09056776762008667\n",
      "Epoch 14 has a loss of 0.10089589655399323\n",
      "Epoch 14 has a loss of 0.06769474595785141\n",
      "Epoch 14 has a loss of 0.12152405083179474\n",
      "Epoch 14 has a loss of 0.15351997315883636\n",
      "Epoch 14 has a loss of 0.05021434277296066\n",
      "Epoch 14 has a loss of 0.05074980854988098\n",
      "Epoch 14 has a loss of 0.0607999823987484\n",
      "Epoch 14 has a loss of 0.07660364359617233\n",
      "Epoch 14 has a loss of 0.07367611676454544\n",
      "Epoch 14 has a loss of 0.09628060460090637\n",
      "Epoch 14 has a loss of 0.21858477592468262\n",
      "Epoch 15, Loss: 0.09383549785614013\n",
      "Prototypes updated using weighted average.\n",
      "Saved model after self-training with T2PL.\n",
      "started_accuracy_computation\n",
      "{90.36}\n",
      "{90.2}\n",
      "{89.8}\n",
      "{88.68}\n",
      "{90.2}\n",
      "{89.52}\n",
      "{89.0}\n",
      "{90.48}\n",
      "{89.36}\n",
      "{90.28}\n",
      "{80.04}\n",
      "{65.32}\n",
      "{81.12}\n",
      "{88.24}\n",
      "{90.0}\n",
      "{81.24}\n",
      "Epoch 1/15\n",
      "Epoch 0 has a loss of 0.1855716109275818\n",
      "Epoch 0 has a loss of 0.35195180773735046\n",
      "Epoch 0 has a loss of 0.23306956887245178\n",
      "Epoch 0 has a loss of 0.1687539517879486\n",
      "Epoch 0 has a loss of 0.5902187824249268\n",
      "Epoch 0 has a loss of 0.3700747787952423\n",
      "Epoch 0 has a loss of 0.2093748152256012\n",
      "Epoch 0 has a loss of 0.4910182058811188\n",
      "Epoch 0 has a loss of 0.23136547207832336\n",
      "Epoch 0 has a loss of 0.10599794238805771\n",
      "Epoch 0 has a loss of 0.17154385149478912\n",
      "Epoch 0 has a loss of 0.4188770353794098\n",
      "Epoch 1, Loss: 0.29098740537961326\n",
      "Epoch 2/15\n",
      "Epoch 1 has a loss of 0.1779513955116272\n",
      "Epoch 1 has a loss of 0.1887434422969818\n",
      "Epoch 1 has a loss of 0.10740283876657486\n",
      "Epoch 1 has a loss of 0.3240901529788971\n",
      "Epoch 1 has a loss of 0.13725803792476654\n",
      "Epoch 1 has a loss of 0.19843646883964539\n",
      "Epoch 1 has a loss of 0.19167470932006836\n",
      "Epoch 1 has a loss of 0.0885935127735138\n",
      "Epoch 1 has a loss of 0.11997228860855103\n",
      "Epoch 1 has a loss of 0.12342136353254318\n",
      "Epoch 1 has a loss of 0.12500892579555511\n",
      "Epoch 1 has a loss of 0.22288477420806885\n",
      "Epoch 2, Loss: 0.1657814671198527\n",
      "Epoch 3/15\n",
      "Epoch 2 has a loss of 0.14297692477703094\n",
      "Epoch 2 has a loss of 0.134555846452713\n",
      "Epoch 2 has a loss of 0.08431392163038254\n",
      "Epoch 2 has a loss of 0.07026355713605881\n",
      "Epoch 2 has a loss of 0.19165246188640594\n",
      "Epoch 2 has a loss of 0.27887529134750366\n",
      "Epoch 2 has a loss of 0.0838356763124466\n",
      "Epoch 2 has a loss of 0.12989778816699982\n",
      "Epoch 2 has a loss of 0.16113388538360596\n",
      "Epoch 2 has a loss of 0.05820630118250847\n",
      "Epoch 2 has a loss of 0.054146651178598404\n",
      "Epoch 2 has a loss of 0.1979457139968872\n",
      "Epoch 3, Loss: 0.13074191252390543\n",
      "Epoch 4/15\n",
      "Epoch 3 has a loss of 0.08405035734176636\n",
      "Epoch 3 has a loss of 0.30370330810546875\n",
      "Epoch 3 has a loss of 0.165119007229805\n",
      "Epoch 3 has a loss of 0.20921331644058228\n",
      "Epoch 3 has a loss of 0.12633241713047028\n",
      "Epoch 3 has a loss of 0.1396385133266449\n",
      "Epoch 3 has a loss of 0.19865630567073822\n",
      "Epoch 3 has a loss of 0.18687783181667328\n",
      "Epoch 3 has a loss of 0.16878841817378998\n",
      "Epoch 3 has a loss of 0.11642750352621078\n",
      "Epoch 3 has a loss of 0.14697188138961792\n",
      "Epoch 3 has a loss of 0.5483463406562805\n",
      "Epoch 4, Loss: 0.19113837162653605\n",
      "Epoch 5/15\n",
      "Epoch 4 has a loss of 0.1296229511499405\n",
      "Epoch 4 has a loss of 0.11127182096242905\n",
      "Epoch 4 has a loss of 0.08722329139709473\n",
      "Epoch 4 has a loss of 0.08621954172849655\n",
      "Epoch 4 has a loss of 0.10336024314165115\n",
      "Epoch 4 has a loss of 0.17698457837104797\n",
      "Epoch 4 has a loss of 0.16772040724754333\n",
      "Epoch 4 has a loss of 0.07543287426233292\n",
      "Epoch 4 has a loss of 0.17909657955169678\n",
      "Epoch 4 has a loss of 0.13099142909049988\n",
      "Epoch 4 has a loss of 0.1664445996284485\n",
      "Epoch 4 has a loss of 0.12563882768154144\n",
      "Epoch 5, Loss: 0.128398611108462\n",
      "Epoch 6/15\n",
      "Epoch 5 has a loss of 0.1797611266374588\n",
      "Epoch 5 has a loss of 0.09755884855985641\n",
      "Epoch 5 has a loss of 0.09466192126274109\n",
      "Epoch 5 has a loss of 0.11340704560279846\n",
      "Epoch 5 has a loss of 0.1419193297624588\n",
      "Epoch 5 has a loss of 0.0804278701543808\n",
      "Epoch 5 has a loss of 0.09884185343980789\n",
      "Epoch 5 has a loss of 0.08846171945333481\n",
      "Epoch 5 has a loss of 0.17625273764133453\n",
      "Epoch 5 has a loss of 0.15651892125606537\n",
      "Epoch 5 has a loss of 0.10271502286195755\n",
      "Epoch 5 has a loss of 0.12414693087339401\n",
      "Epoch 6, Loss: 0.1211525976061821\n",
      "Epoch 7/15\n",
      "Epoch 6 has a loss of 0.08739860355854034\n",
      "Epoch 6 has a loss of 0.09032807499170303\n",
      "Epoch 6 has a loss of 0.08702026307582855\n",
      "Epoch 6 has a loss of 0.056030869483947754\n",
      "Epoch 6 has a loss of 0.18295210599899292\n",
      "Epoch 6 has a loss of 0.14054037630558014\n",
      "Epoch 6 has a loss of 0.10748937726020813\n",
      "Epoch 6 has a loss of 0.09138927608728409\n",
      "Epoch 6 has a loss of 0.07568120211362839\n",
      "Epoch 6 has a loss of 0.1450117975473404\n",
      "Epoch 6 has a loss of 0.1762773096561432\n",
      "Epoch 6 has a loss of 0.13764549791812897\n",
      "Epoch 7, Loss: 0.11426576705773671\n",
      "Epoch 8/15\n",
      "Epoch 7 has a loss of 0.08337332308292389\n",
      "Epoch 7 has a loss of 0.09743182361125946\n",
      "Epoch 7 has a loss of 0.14046858251094818\n",
      "Epoch 7 has a loss of 0.06781752407550812\n",
      "Epoch 7 has a loss of 0.07656006515026093\n",
      "Epoch 7 has a loss of 0.06696804612874985\n",
      "Epoch 7 has a loss of 0.10633373260498047\n",
      "Epoch 7 has a loss of 0.1644938588142395\n",
      "Epoch 7 has a loss of 0.1294340044260025\n",
      "Epoch 7 has a loss of 0.07765845954418182\n",
      "Epoch 7 has a loss of 0.08912473171949387\n",
      "Epoch 7 has a loss of 0.07537490874528885\n",
      "Epoch 8, Loss: 0.09846100201209386\n",
      "Epoch 9/15\n",
      "Epoch 8 has a loss of 0.14628851413726807\n",
      "Epoch 8 has a loss of 0.06584254652261734\n",
      "Epoch 8 has a loss of 0.08585412800312042\n",
      "Epoch 8 has a loss of 0.07452991604804993\n",
      "Epoch 8 has a loss of 0.24069830775260925\n",
      "Epoch 8 has a loss of 0.15903478860855103\n",
      "Epoch 8 has a loss of 0.049673501402139664\n",
      "Epoch 8 has a loss of 0.06284839659929276\n",
      "Epoch 8 has a loss of 0.21738901734352112\n",
      "Epoch 8 has a loss of 0.108292356133461\n",
      "Epoch 8 has a loss of 0.08384376764297485\n",
      "Epoch 8 has a loss of 0.1487065702676773\n",
      "Epoch 9, Loss: 0.11956719680627187\n",
      "Epoch 10/15\n",
      "Epoch 9 has a loss of 0.15529488027095795\n",
      "Epoch 9 has a loss of 0.08072856813669205\n",
      "Epoch 9 has a loss of 0.13525377213954926\n",
      "Epoch 9 has a loss of 0.08300508558750153\n",
      "Epoch 9 has a loss of 0.16607242822647095\n",
      "Epoch 9 has a loss of 0.1920909285545349\n",
      "Epoch 9 has a loss of 0.07407186180353165\n",
      "Epoch 9 has a loss of 0.12880903482437134\n",
      "Epoch 9 has a loss of 0.12455400824546814\n",
      "Epoch 9 has a loss of 0.11458664387464523\n",
      "Epoch 9 has a loss of 0.1236015111207962\n",
      "Epoch 9 has a loss of 0.16092978417873383\n",
      "Epoch 10, Loss: 0.12746555777390797\n",
      "Epoch 11/15\n",
      "Epoch 10 has a loss of 0.11025267839431763\n",
      "Epoch 10 has a loss of 0.07043512165546417\n",
      "Epoch 10 has a loss of 0.17663295567035675\n",
      "Epoch 10 has a loss of 0.0717611014842987\n",
      "Epoch 10 has a loss of 0.1546042263507843\n",
      "Epoch 10 has a loss of 0.19675619900226593\n",
      "Epoch 10 has a loss of 0.17141304910182953\n",
      "Epoch 10 has a loss of 0.15440906584262848\n",
      "Epoch 10 has a loss of 0.22184765338897705\n",
      "Epoch 10 has a loss of 0.04173526167869568\n",
      "Epoch 10 has a loss of 0.15663526952266693\n",
      "Epoch 10 has a loss of 0.06730923056602478\n",
      "Epoch 11, Loss: 0.13438814647992453\n",
      "Epoch 12/15\n",
      "Epoch 11 has a loss of 0.37711113691329956\n",
      "Epoch 11 has a loss of 0.198458731174469\n",
      "Epoch 11 has a loss of 0.2663169503211975\n",
      "Epoch 11 has a loss of 0.023677140474319458\n",
      "Epoch 11 has a loss of 0.129660964012146\n",
      "Epoch 11 has a loss of 0.14356404542922974\n",
      "Epoch 11 has a loss of 0.10457954555749893\n",
      "Epoch 11 has a loss of 0.1510152965784073\n",
      "Epoch 11 has a loss of 0.055652640759944916\n",
      "Epoch 11 has a loss of 0.16564646363258362\n",
      "Epoch 11 has a loss of 0.07028099149465561\n",
      "Epoch 11 has a loss of 0.10165897756814957\n",
      "Epoch 12, Loss: 0.15010400396585463\n",
      "Epoch 13/15\n",
      "Epoch 12 has a loss of 0.1422012597322464\n",
      "Epoch 12 has a loss of 0.21690328419208527\n",
      "Epoch 12 has a loss of 0.06676726043224335\n",
      "Epoch 12 has a loss of 0.08806858956813812\n",
      "Epoch 12 has a loss of 0.10535275936126709\n",
      "Epoch 12 has a loss of 0.08589981496334076\n",
      "Epoch 12 has a loss of 0.11644142121076584\n",
      "Epoch 12 has a loss of 0.0888773500919342\n",
      "Epoch 12 has a loss of 0.08651609718799591\n",
      "Epoch 12 has a loss of 0.1974766105413437\n",
      "Epoch 12 has a loss of 0.06097535789012909\n",
      "Epoch 12 has a loss of 0.18097896873950958\n",
      "Epoch 13, Loss: 0.11823432012399038\n",
      "Epoch 14/15\n",
      "Epoch 13 has a loss of 0.14175920188426971\n",
      "Epoch 13 has a loss of 0.10274805128574371\n",
      "Epoch 13 has a loss of 0.13045178353786469\n",
      "Epoch 13 has a loss of 0.14888006448745728\n",
      "Epoch 13 has a loss of 0.06679870188236237\n",
      "Epoch 13 has a loss of 0.07168281823396683\n",
      "Epoch 13 has a loss of 0.10188309103250504\n",
      "Epoch 13 has a loss of 0.11460838466882706\n",
      "Epoch 13 has a loss of 0.12438201904296875\n",
      "Epoch 13 has a loss of 0.13129441440105438\n",
      "Epoch 13 has a loss of 0.13167732954025269\n",
      "Epoch 13 has a loss of 0.13433951139450073\n",
      "Epoch 14, Loss: 0.11628564341862996\n",
      "Epoch 15/15\n",
      "Epoch 14 has a loss of 0.09306774288415909\n",
      "Epoch 14 has a loss of 0.09192158281803131\n",
      "Epoch 14 has a loss of 0.10114415735006332\n",
      "Epoch 14 has a loss of 0.13830049335956573\n",
      "Epoch 14 has a loss of 0.07797019183635712\n",
      "Epoch 14 has a loss of 0.21271976828575134\n",
      "Epoch 14 has a loss of 0.07431008666753769\n",
      "Epoch 14 has a loss of 0.06696050614118576\n",
      "Epoch 14 has a loss of 0.1342678964138031\n",
      "Epoch 14 has a loss of 0.05511107295751572\n",
      "Epoch 14 has a loss of 0.19185622036457062\n",
      "Epoch 14 has a loss of 0.06041846051812172\n",
      "Epoch 15, Loss: 0.10931673493981361\n",
      "Prototypes updated using weighted average.\n",
      "Saved model after self-training with T2PL.\n",
      "started_accuracy_computation\n",
      "{89.6}\n",
      "{89.4}\n",
      "{88.68}\n",
      "{88.16}\n",
      "{89.0}\n",
      "{88.52}\n",
      "{87.76}\n",
      "{89.0}\n",
      "{89.24}\n",
      "{88.96}\n",
      "{83.8}\n",
      "{60.6}\n",
      "{78.24}\n",
      "{88.72}\n",
      "{88.12}\n",
      "{77.32}\n",
      "{88.96}\n",
      "Epoch 1/15\n",
      "Epoch 0 has a loss of 0.1765146702528\n",
      "Epoch 0 has a loss of 0.7455039024353027\n",
      "Epoch 0 has a loss of 0.4263130724430084\n",
      "Epoch 0 has a loss of 0.5194759964942932\n",
      "Epoch 0 has a loss of 0.1572900265455246\n",
      "Epoch 0 has a loss of 0.5719144344329834\n",
      "Epoch 0 has a loss of 0.18560199439525604\n",
      "Epoch 0 has a loss of 0.2659018039703369\n",
      "Epoch 0 has a loss of 0.339101642370224\n",
      "Epoch 0 has a loss of 0.36044520139694214\n",
      "Epoch 0 has a loss of 0.22720132768154144\n",
      "Epoch 0 has a loss of 0.4335675835609436\n",
      "Epoch 1, Loss: 0.3658146793047587\n",
      "Epoch 2/15\n",
      "Epoch 1 has a loss of 0.57781583070755\n",
      "Epoch 1 has a loss of 0.13415178656578064\n",
      "Epoch 1 has a loss of 0.26957380771636963\n",
      "Epoch 1 has a loss of 0.5056260824203491\n",
      "Epoch 1 has a loss of 0.1523798108100891\n",
      "Epoch 1 has a loss of 0.18459364771842957\n",
      "Epoch 1 has a loss of 0.22452674806118011\n",
      "Epoch 1 has a loss of 0.16922879219055176\n",
      "Epoch 1 has a loss of 0.23647737503051758\n",
      "Epoch 1 has a loss of 0.15408366918563843\n",
      "Epoch 1 has a loss of 0.2628108859062195\n",
      "Epoch 1 has a loss of 0.16229483485221863\n",
      "Epoch 2, Loss: 0.25496898976961774\n",
      "Epoch 3/15\n",
      "Epoch 2 has a loss of 0.15011252462863922\n",
      "Epoch 2 has a loss of 0.17876450717449188\n",
      "Epoch 2 has a loss of 0.14944083988666534\n",
      "Epoch 2 has a loss of 0.3424881100654602\n",
      "Epoch 2 has a loss of 0.2963275611400604\n",
      "Epoch 2 has a loss of 0.31134486198425293\n",
      "Epoch 2 has a loss of 0.18700048327445984\n",
      "Epoch 2 has a loss of 0.20226120948791504\n",
      "Epoch 2 has a loss of 0.18731091916561127\n",
      "Epoch 2 has a loss of 0.18439295887947083\n",
      "Epoch 2 has a loss of 0.07971864938735962\n",
      "Epoch 2 has a loss of 0.6188360452651978\n",
      "Epoch 3, Loss: 0.23159048811594646\n",
      "Epoch 4/15\n",
      "Epoch 3 has a loss of 0.18980929255485535\n",
      "Epoch 3 has a loss of 0.3139399290084839\n",
      "Epoch 3 has a loss of 0.10050639510154724\n",
      "Epoch 3 has a loss of 0.33967703580856323\n",
      "Epoch 3 has a loss of 0.0939800813794136\n",
      "Epoch 3 has a loss of 0.2690051794052124\n",
      "Epoch 3 has a loss of 0.11924418807029724\n",
      "Epoch 3 has a loss of 0.10512538254261017\n",
      "Epoch 3 has a loss of 0.16965189576148987\n",
      "Epoch 3 has a loss of 0.22346176207065582\n",
      "Epoch 3 has a loss of 0.1555265188217163\n",
      "Epoch 3 has a loss of 0.23420247435569763\n",
      "Epoch 4, Loss: 0.19185157879193623\n",
      "Epoch 5/15\n",
      "Epoch 4 has a loss of 0.054814886301755905\n",
      "Epoch 4 has a loss of 0.16424517333507538\n",
      "Epoch 4 has a loss of 0.13983745872974396\n",
      "Epoch 4 has a loss of 0.11230963468551636\n",
      "Epoch 4 has a loss of 0.1039237305521965\n",
      "Epoch 4 has a loss of 0.1561635583639145\n",
      "Epoch 4 has a loss of 0.15305137634277344\n",
      "Epoch 4 has a loss of 0.20349150896072388\n",
      "Epoch 4 has a loss of 0.1196385845541954\n",
      "Epoch 4 has a loss of 0.09745839983224869\n",
      "Epoch 4 has a loss of 0.22431546449661255\n",
      "Epoch 4 has a loss of 0.22083880007266998\n",
      "Epoch 5, Loss: 0.14404076063632965\n",
      "Epoch 6/15\n",
      "Epoch 5 has a loss of 0.13936857879161835\n",
      "Epoch 5 has a loss of 0.19396105408668518\n",
      "Epoch 5 has a loss of 0.11040942370891571\n",
      "Epoch 5 has a loss of 0.1589808166027069\n",
      "Epoch 5 has a loss of 0.13497315347194672\n",
      "Epoch 5 has a loss of 0.2566065788269043\n",
      "Epoch 5 has a loss of 0.116755411028862\n",
      "Epoch 5 has a loss of 0.13066688179969788\n",
      "Epoch 5 has a loss of 0.09423916041851044\n",
      "Epoch 5 has a loss of 0.1272011548280716\n",
      "Epoch 5 has a loss of 0.35180991888046265\n",
      "Epoch 5 has a loss of 0.21041351556777954\n",
      "Epoch 6, Loss: 0.16778298425674437\n",
      "Epoch 7/15\n",
      "Epoch 6 has a loss of 0.15151990950107574\n",
      "Epoch 6 has a loss of 0.06312396377325058\n",
      "Epoch 6 has a loss of 0.19919459521770477\n",
      "Epoch 6 has a loss of 0.07719863951206207\n",
      "Epoch 6 has a loss of 0.1051803007721901\n",
      "Epoch 6 has a loss of 0.09557295590639114\n",
      "Epoch 6 has a loss of 0.11091341078281403\n",
      "Epoch 6 has a loss of 0.1143217384815216\n",
      "Epoch 6 has a loss of 0.13555364310741425\n",
      "Epoch 6 has a loss of 0.11799988150596619\n",
      "Epoch 6 has a loss of 0.21976351737976074\n",
      "Epoch 6 has a loss of 0.13154345750808716\n",
      "Epoch 7, Loss: 0.12671056350072224\n",
      "Epoch 8/15\n",
      "Epoch 7 has a loss of 0.24794995784759521\n",
      "Epoch 7 has a loss of 0.1324770599603653\n",
      "Epoch 7 has a loss of 0.25793251395225525\n",
      "Epoch 7 has a loss of 0.3246578574180603\n",
      "Epoch 7 has a loss of 0.12612436711788177\n",
      "Epoch 7 has a loss of 0.09370303153991699\n",
      "Epoch 7 has a loss of 0.07482387125492096\n",
      "Epoch 7 has a loss of 0.5226694345474243\n",
      "Epoch 7 has a loss of 0.09332875907421112\n",
      "Epoch 7 has a loss of 0.2375386357307434\n",
      "Epoch 7 has a loss of 0.17347021400928497\n",
      "Epoch 7 has a loss of 0.23837800323963165\n",
      "Epoch 8, Loss: 0.20957951080799103\n",
      "Epoch 9/15\n",
      "Epoch 8 has a loss of 0.14176476001739502\n",
      "Epoch 8 has a loss of 0.40361759066581726\n",
      "Epoch 8 has a loss of 0.1306075155735016\n",
      "Epoch 8 has a loss of 0.12178240716457367\n",
      "Epoch 8 has a loss of 0.1400795578956604\n",
      "Epoch 8 has a loss of 0.13123354315757751\n",
      "Epoch 8 has a loss of 0.10353461652994156\n",
      "Epoch 8 has a loss of 0.24108876287937164\n",
      "Epoch 8 has a loss of 0.2653786838054657\n",
      "Epoch 8 has a loss of 0.11214151978492737\n",
      "Epoch 8 has a loss of 0.22288081049919128\n",
      "Epoch 8 has a loss of 0.18771909177303314\n",
      "Epoch 9, Loss: 0.18338413782914478\n",
      "Epoch 10/15\n",
      "Epoch 9 has a loss of 0.16053810715675354\n",
      "Epoch 9 has a loss of 0.18158282339572906\n",
      "Epoch 9 has a loss of 0.1382833570241928\n",
      "Epoch 9 has a loss of 0.07553660124540329\n",
      "Epoch 9 has a loss of 0.1768416166305542\n",
      "Epoch 9 has a loss of 0.2623192071914673\n",
      "Epoch 9 has a loss of 0.09279974550008774\n",
      "Epoch 9 has a loss of 0.13220608234405518\n",
      "Epoch 9 has a loss of 0.1496822088956833\n",
      "Epoch 9 has a loss of 0.10307668149471283\n",
      "Epoch 9 has a loss of 0.15349459648132324\n",
      "Epoch 9 has a loss of 0.24793872237205505\n",
      "Epoch 10, Loss: 0.15398971597353617\n",
      "Epoch 11/15\n",
      "Epoch 10 has a loss of 0.2904278337955475\n",
      "Epoch 10 has a loss of 0.09334945678710938\n",
      "Epoch 10 has a loss of 0.12068681418895721\n",
      "Epoch 10 has a loss of 0.13085222244262695\n",
      "Epoch 10 has a loss of 0.1948738843202591\n",
      "Epoch 10 has a loss of 0.12994445860385895\n",
      "Epoch 10 has a loss of 0.28080958127975464\n",
      "Epoch 10 has a loss of 0.0671718418598175\n",
      "Epoch 10 has a loss of 0.1094478964805603\n",
      "Epoch 10 has a loss of 0.05138006806373596\n",
      "Epoch 10 has a loss of 0.24968425929546356\n",
      "Epoch 10 has a loss of 0.09665097296237946\n",
      "Epoch 11, Loss: 0.15258420940240225\n",
      "Epoch 12/15\n",
      "Epoch 11 has a loss of 0.1488289088010788\n",
      "Epoch 11 has a loss of 0.06775607168674469\n",
      "Epoch 11 has a loss of 0.2522128224372864\n",
      "Epoch 11 has a loss of 0.11843030154705048\n",
      "Epoch 11 has a loss of 0.08656620979309082\n",
      "Epoch 11 has a loss of 0.08307886123657227\n",
      "Epoch 11 has a loss of 0.11315091699361801\n",
      "Epoch 11 has a loss of 0.08487610518932343\n",
      "Epoch 11 has a loss of 0.07290218770503998\n",
      "Epoch 11 has a loss of 0.22915568947792053\n",
      "Epoch 11 has a loss of 0.10453712940216064\n",
      "Epoch 11 has a loss of 0.11011041700839996\n",
      "Epoch 12, Loss: 0.12293436300754547\n",
      "Epoch 13/15\n",
      "Epoch 12 has a loss of 0.13078682124614716\n",
      "Epoch 12 has a loss of 0.29690128564834595\n",
      "Epoch 12 has a loss of 0.15271726250648499\n",
      "Epoch 12 has a loss of 0.10271372646093369\n",
      "Epoch 12 has a loss of 0.17843961715698242\n",
      "Epoch 12 has a loss of 0.21282289922237396\n",
      "Epoch 12 has a loss of 0.05561910942196846\n",
      "Epoch 12 has a loss of 0.1424412578344345\n",
      "Epoch 12 has a loss of 0.16374824941158295\n",
      "Epoch 12 has a loss of 0.16996334493160248\n",
      "Epoch 12 has a loss of 0.1635819375514984\n",
      "Epoch 12 has a loss of 0.0929897278547287\n",
      "Epoch 13, Loss: 0.15672080028057098\n",
      "Epoch 14/15\n",
      "Epoch 13 has a loss of 0.1271371990442276\n",
      "Epoch 13 has a loss of 0.09745007753372192\n",
      "Epoch 13 has a loss of 0.09860818833112717\n",
      "Epoch 13 has a loss of 0.20732751488685608\n",
      "Epoch 13 has a loss of 0.17582762241363525\n",
      "Epoch 13 has a loss of 0.23361089825630188\n",
      "Epoch 13 has a loss of 0.11850535869598389\n",
      "Epoch 13 has a loss of 0.13636180758476257\n",
      "Epoch 13 has a loss of 0.13809579610824585\n",
      "Epoch 13 has a loss of 0.06613463163375854\n",
      "Epoch 13 has a loss of 0.10516688227653503\n",
      "Epoch 13 has a loss of 0.05987284705042839\n",
      "Epoch 14, Loss: 0.13203281796971958\n",
      "Epoch 15/15\n",
      "Epoch 14 has a loss of 0.09387963265180588\n",
      "Epoch 14 has a loss of 0.18246209621429443\n",
      "Epoch 14 has a loss of 0.17050272226333618\n",
      "Epoch 14 has a loss of 0.2106074094772339\n",
      "Epoch 14 has a loss of 0.13245923817157745\n",
      "Epoch 14 has a loss of 0.09426246583461761\n",
      "Epoch 14 has a loss of 0.06936970353126526\n",
      "Epoch 14 has a loss of 0.10421839356422424\n",
      "Epoch 14 has a loss of 0.1818152666091919\n",
      "Epoch 14 has a loss of 0.13081711530685425\n",
      "Epoch 14 has a loss of 0.0648798793554306\n",
      "Epoch 14 has a loss of 0.22457656264305115\n",
      "Epoch 15, Loss: 0.13625073726971945\n",
      "Prototypes updated using weighted average.\n",
      "Saved model after self-training with T2PL.\n",
      "started_accuracy_computation\n",
      "{88.28}\n",
      "{88.6}\n",
      "{87.48}\n",
      "{86.88}\n",
      "{87.88}\n",
      "{87.44}\n",
      "{87.04}\n",
      "{88.64}\n",
      "{88.08}\n",
      "{88.44}\n",
      "{76.56}\n",
      "{65.44}\n",
      "{80.0}\n",
      "{85.4}\n",
      "{88.68}\n",
      "{78.64}\n",
      "{86.72}\n",
      "{80.48}\n",
      "Epoch 1/15\n",
      "Epoch 0 has a loss of 1.1199662685394287\n",
      "Epoch 0 has a loss of 1.1124286651611328\n",
      "Epoch 0 has a loss of 1.6806917190551758\n",
      "Epoch 0 has a loss of 1.257025957107544\n",
      "Epoch 0 has a loss of 0.8012509346008301\n",
      "Epoch 0 has a loss of 0.6722933650016785\n",
      "Epoch 0 has a loss of 1.1226284503936768\n",
      "Epoch 0 has a loss of 0.917892336845398\n",
      "Epoch 0 has a loss of 0.5014288425445557\n",
      "Epoch 0 has a loss of 0.4628174901008606\n",
      "Epoch 0 has a loss of 0.6527082324028015\n",
      "Epoch 0 has a loss of 0.4306732416152954\n",
      "Epoch 1, Loss: 0.9054445784886678\n",
      "Epoch 2/15\n",
      "Epoch 1 has a loss of 0.286204069852829\n",
      "Epoch 1 has a loss of 0.25929078459739685\n",
      "Epoch 1 has a loss of 0.3916357159614563\n",
      "Epoch 1 has a loss of 0.9775046706199646\n",
      "Epoch 1 has a loss of 0.5312908291816711\n",
      "Epoch 1 has a loss of 0.2818942666053772\n",
      "Epoch 1 has a loss of 0.3114595115184784\n",
      "Epoch 1 has a loss of 0.42611750960350037\n",
      "Epoch 1 has a loss of 0.4418345093727112\n",
      "Epoch 1 has a loss of 0.4149232506752014\n",
      "Epoch 1 has a loss of 1.2699460983276367\n",
      "Epoch 1 has a loss of 0.3792027235031128\n",
      "Epoch 2, Loss: 0.5004504041671753\n",
      "Epoch 3/15\n",
      "Epoch 2 has a loss of 0.5662715435028076\n",
      "Epoch 2 has a loss of 0.5407362580299377\n",
      "Epoch 2 has a loss of 0.712838888168335\n",
      "Epoch 2 has a loss of 0.21292412281036377\n",
      "Epoch 2 has a loss of 0.3348497450351715\n",
      "Epoch 2 has a loss of 0.4075116515159607\n",
      "Epoch 2 has a loss of 0.43335121870040894\n",
      "Epoch 2 has a loss of 0.48090431094169617\n",
      "Epoch 2 has a loss of 0.4687243402004242\n",
      "Epoch 2 has a loss of 0.2493223398923874\n",
      "Epoch 2 has a loss of 0.37014833092689514\n",
      "Epoch 2 has a loss of 0.2252447009086609\n",
      "Epoch 3, Loss: 0.4215020696322123\n",
      "Epoch 4/15\n",
      "Epoch 3 has a loss of 0.38540035486221313\n",
      "Epoch 3 has a loss of 0.5890794396400452\n",
      "Epoch 3 has a loss of 0.28082096576690674\n",
      "Epoch 3 has a loss of 0.362423300743103\n",
      "Epoch 3 has a loss of 0.48860636353492737\n",
      "Epoch 3 has a loss of 0.23753906786441803\n",
      "Epoch 3 has a loss of 0.3661916255950928\n",
      "Epoch 3 has a loss of 0.47207707166671753\n",
      "Epoch 3 has a loss of 0.3294578492641449\n",
      "Epoch 3 has a loss of 0.3166346549987793\n",
      "Epoch 3 has a loss of 0.26338574290275574\n",
      "Epoch 3 has a loss of 0.37611833214759827\n",
      "Epoch 4, Loss: 0.3722198603153229\n",
      "Epoch 5/15\n",
      "Epoch 4 has a loss of 0.2726829946041107\n",
      "Epoch 4 has a loss of 0.28765374422073364\n",
      "Epoch 4 has a loss of 0.23932063579559326\n",
      "Epoch 4 has a loss of 0.28967636823654175\n",
      "Epoch 4 has a loss of 0.3526269197463989\n",
      "Epoch 4 has a loss of 0.257033109664917\n",
      "Epoch 4 has a loss of 0.22269149124622345\n",
      "Epoch 4 has a loss of 0.23543550074100494\n",
      "Epoch 4 has a loss of 0.24368250370025635\n",
      "Epoch 4 has a loss of 0.27470698952674866\n",
      "Epoch 4 has a loss of 0.18790903687477112\n",
      "Epoch 4 has a loss of 0.6569122076034546\n",
      "Epoch 5, Loss: 0.28463572851816815\n",
      "Epoch 6/15\n",
      "Epoch 5 has a loss of 0.2690731883049011\n",
      "Epoch 5 has a loss of 0.30429747700691223\n",
      "Epoch 5 has a loss of 0.2164171040058136\n",
      "Epoch 5 has a loss of 0.36413028836250305\n",
      "Epoch 5 has a loss of 0.2652505338191986\n",
      "Epoch 5 has a loss of 0.25103017687797546\n",
      "Epoch 5 has a loss of 0.5577714443206787\n",
      "Epoch 5 has a loss of 0.3813513517379761\n",
      "Epoch 5 has a loss of 0.34399205446243286\n",
      "Epoch 5 has a loss of 0.18927143514156342\n",
      "Epoch 5 has a loss of 0.23682960867881775\n",
      "Epoch 5 has a loss of 0.29777178168296814\n",
      "Epoch 6, Loss: 0.30664005382855736\n",
      "Epoch 7/15\n",
      "Epoch 6 has a loss of 0.48848944902420044\n",
      "Epoch 6 has a loss of 0.23105522990226746\n",
      "Epoch 6 has a loss of 0.172474205493927\n",
      "Epoch 6 has a loss of 0.3883587718009949\n",
      "Epoch 6 has a loss of 0.21416287124156952\n",
      "Epoch 6 has a loss of 0.18060564994812012\n",
      "Epoch 6 has a loss of 0.31069502234458923\n",
      "Epoch 6 has a loss of 0.2554123103618622\n",
      "Epoch 6 has a loss of 0.19171370565891266\n",
      "Epoch 6 has a loss of 0.3638531267642975\n",
      "Epoch 6 has a loss of 0.2117849588394165\n",
      "Epoch 6 has a loss of 0.2429981529712677\n",
      "Epoch 7, Loss: 0.27163820576667785\n",
      "Epoch 8/15\n",
      "Epoch 7 has a loss of 0.2413918673992157\n",
      "Epoch 7 has a loss of 0.1922808289527893\n",
      "Epoch 7 has a loss of 0.330441951751709\n",
      "Epoch 7 has a loss of 0.2468542754650116\n",
      "Epoch 7 has a loss of 0.29408693313598633\n",
      "Epoch 7 has a loss of 0.2598057985305786\n",
      "Epoch 7 has a loss of 0.6850461959838867\n",
      "Epoch 7 has a loss of 0.2554651200771332\n",
      "Epoch 7 has a loss of 0.26934733986854553\n",
      "Epoch 7 has a loss of 0.2806914746761322\n",
      "Epoch 7 has a loss of 0.2993415296077728\n",
      "Epoch 7 has a loss of 0.3015352189540863\n",
      "Epoch 8, Loss: 0.3047664430141449\n",
      "Epoch 9/15\n",
      "Epoch 8 has a loss of 0.21035364270210266\n",
      "Epoch 8 has a loss of 0.3396349251270294\n",
      "Epoch 8 has a loss of 0.426853209733963\n",
      "Epoch 8 has a loss of 0.3109341561794281\n",
      "Epoch 8 has a loss of 0.26903122663497925\n",
      "Epoch 8 has a loss of 0.25374770164489746\n",
      "Epoch 8 has a loss of 0.2428552508354187\n",
      "Epoch 8 has a loss of 0.22689953446388245\n",
      "Epoch 8 has a loss of 0.17635563015937805\n",
      "Epoch 8 has a loss of 0.20363017916679382\n",
      "Epoch 8 has a loss of 0.21454855799674988\n",
      "Epoch 8 has a loss of 0.1251714676618576\n",
      "Epoch 9, Loss: 0.25299720593293507\n",
      "Epoch 10/15\n",
      "Epoch 9 has a loss of 0.23009617626667023\n",
      "Epoch 9 has a loss of 0.19633512198925018\n",
      "Epoch 9 has a loss of 0.16603831946849823\n",
      "Epoch 9 has a loss of 0.2048483043909073\n",
      "Epoch 9 has a loss of 0.14277543127536774\n",
      "Epoch 9 has a loss of 0.29024001955986023\n",
      "Epoch 9 has a loss of 0.1689961552619934\n",
      "Epoch 9 has a loss of 0.2613970935344696\n",
      "Epoch 9 has a loss of 0.15250755846500397\n",
      "Epoch 9 has a loss of 0.3958248496055603\n",
      "Epoch 9 has a loss of 0.17013543844223022\n",
      "Epoch 9 has a loss of 0.13597768545150757\n",
      "Epoch 10, Loss: 0.2113645593325297\n",
      "Epoch 11/15\n",
      "Epoch 10 has a loss of 0.26035910844802856\n",
      "Epoch 10 has a loss of 0.29608726501464844\n",
      "Epoch 10 has a loss of 0.18911555409431458\n",
      "Epoch 10 has a loss of 0.4877837002277374\n",
      "Epoch 10 has a loss of 0.218405619263649\n",
      "Epoch 10 has a loss of 0.1876523792743683\n",
      "Epoch 10 has a loss of 0.2928425967693329\n",
      "Epoch 10 has a loss of 0.5617567300796509\n",
      "Epoch 10 has a loss of 0.2351987063884735\n",
      "Epoch 10 has a loss of 0.5077277421951294\n",
      "Epoch 10 has a loss of 0.2914496660232544\n",
      "Epoch 10 has a loss of 0.14942969381809235\n",
      "Epoch 11, Loss: 0.31025336833794914\n",
      "Epoch 12/15\n",
      "Epoch 11 has a loss of 0.24785804748535156\n",
      "Epoch 11 has a loss of 0.27338695526123047\n",
      "Epoch 11 has a loss of 0.25272947549819946\n",
      "Epoch 11 has a loss of 0.34338244795799255\n",
      "Epoch 11 has a loss of 0.23415769636631012\n",
      "Epoch 11 has a loss of 0.24924547970294952\n",
      "Epoch 11 has a loss of 0.3184489607810974\n",
      "Epoch 11 has a loss of 0.27397704124450684\n",
      "Epoch 11 has a loss of 0.20048177242279053\n",
      "Epoch 11 has a loss of 0.2562113404273987\n",
      "Epoch 11 has a loss of 0.15124092996120453\n",
      "Epoch 11 has a loss of 0.304568350315094\n",
      "Epoch 12, Loss: 0.2577091113726298\n",
      "Epoch 13/15\n",
      "Epoch 12 has a loss of 0.18552155792713165\n",
      "Epoch 12 has a loss of 0.2730216085910797\n",
      "Epoch 12 has a loss of 0.21583852171897888\n",
      "Epoch 12 has a loss of 0.4171801209449768\n",
      "Epoch 12 has a loss of 0.2646844983100891\n",
      "Epoch 12 has a loss of 0.1970897614955902\n",
      "Epoch 12 has a loss of 0.28082355856895447\n",
      "Epoch 12 has a loss of 0.2398114651441574\n",
      "Epoch 12 has a loss of 0.3418731391429901\n",
      "Epoch 12 has a loss of 0.310425728559494\n",
      "Epoch 12 has a loss of 0.21101078391075134\n",
      "Epoch 12 has a loss of 0.2631632685661316\n",
      "Epoch 13, Loss: 0.2667886373202006\n",
      "Epoch 14/15\n",
      "Epoch 13 has a loss of 0.23485144972801208\n",
      "Epoch 13 has a loss of 0.2985059916973114\n",
      "Epoch 13 has a loss of 0.2903669476509094\n",
      "Epoch 13 has a loss of 0.17155493795871735\n",
      "Epoch 13 has a loss of 0.21034488081932068\n",
      "Epoch 13 has a loss of 0.2280045449733734\n",
      "Epoch 13 has a loss of 0.2459665834903717\n",
      "Epoch 13 has a loss of 0.1936783343553543\n",
      "Epoch 13 has a loss of 0.29510703682899475\n",
      "Epoch 13 has a loss of 0.2342979609966278\n",
      "Epoch 13 has a loss of 0.1465485692024231\n",
      "Epoch 13 has a loss of 0.22961384057998657\n",
      "Epoch 14, Loss: 0.23161703983942666\n",
      "Epoch 15/15\n",
      "Epoch 14 has a loss of 0.42403358221054077\n",
      "Epoch 14 has a loss of 0.2443448156118393\n",
      "Epoch 14 has a loss of 0.2831125259399414\n",
      "Epoch 14 has a loss of 0.40079465508461\n",
      "Epoch 14 has a loss of 0.17049749195575714\n",
      "Epoch 14 has a loss of 0.24674639105796814\n",
      "Epoch 14 has a loss of 0.2850930690765381\n",
      "Epoch 14 has a loss of 0.17936691641807556\n",
      "Epoch 14 has a loss of 0.16220736503601074\n",
      "Epoch 14 has a loss of 0.265899658203125\n",
      "Epoch 14 has a loss of 0.5482418537139893\n",
      "Epoch 14 has a loss of 0.2891838252544403\n",
      "Epoch 15, Loss: 0.2916854782899221\n",
      "Prototypes updated using weighted average.\n",
      "Saved model after self-training with T2PL.\n",
      "started_accuracy_computation\n",
      "{87.48}\n",
      "{88.08}\n",
      "{86.12}\n",
      "{86.12}\n",
      "{87.0}\n",
      "{87.0}\n",
      "{86.44}\n",
      "{87.04}\n",
      "{87.08}\n",
      "{87.32}\n",
      "{76.96}\n",
      "{54.16}\n",
      "{74.68}\n",
      "{85.2}\n",
      "{86.8}\n",
      "{73.2}\n",
      "{85.12}\n",
      "{72.6}\n",
      "{78.8}\n",
      "Epoch 1/15\n",
      "Epoch 0 has a loss of 0.6664146184921265\n",
      "Epoch 0 has a loss of 0.1935003697872162\n",
      "Epoch 0 has a loss of 0.7224352955818176\n",
      "Epoch 0 has a loss of 0.3144306540489197\n",
      "Epoch 0 has a loss of 0.7567483186721802\n",
      "Epoch 0 has a loss of 0.3366732597351074\n",
      "Epoch 0 has a loss of 0.6758112907409668\n",
      "Epoch 0 has a loss of 0.36506980657577515\n",
      "Epoch 0 has a loss of 0.33592867851257324\n",
      "Epoch 0 has a loss of 0.2882918119430542\n",
      "Epoch 0 has a loss of 0.17287780344486237\n",
      "Epoch 0 has a loss of 0.4303267300128937\n",
      "Epoch 1, Loss: 0.4383982288837433\n",
      "Epoch 2/15\n",
      "Epoch 1 has a loss of 0.46307361125946045\n",
      "Epoch 1 has a loss of 0.18364202976226807\n",
      "Epoch 1 has a loss of 0.35271307826042175\n",
      "Epoch 1 has a loss of 0.18436914682388306\n",
      "Epoch 1 has a loss of 0.1362903118133545\n",
      "Epoch 1 has a loss of 0.20577295124530792\n",
      "Epoch 1 has a loss of 0.551390528678894\n",
      "Epoch 1 has a loss of 0.30553096532821655\n",
      "Epoch 1 has a loss of 0.10800288617610931\n",
      "Epoch 1 has a loss of 0.16491404175758362\n",
      "Epoch 1 has a loss of 0.17924009263515472\n",
      "Epoch 1 has a loss of 0.1325194388628006\n",
      "Epoch 2, Loss: 0.2500427085161209\n",
      "Epoch 3/15\n",
      "Epoch 2 has a loss of 0.14103521406650543\n",
      "Epoch 2 has a loss of 0.2322838306427002\n",
      "Epoch 2 has a loss of 0.2999418377876282\n",
      "Epoch 2 has a loss of 0.18041247129440308\n",
      "Epoch 2 has a loss of 0.1957404613494873\n",
      "Epoch 2 has a loss of 0.17742910981178284\n",
      "Epoch 2 has a loss of 0.3093973398208618\n",
      "Epoch 2 has a loss of 0.2565196454524994\n",
      "Epoch 2 has a loss of 0.12016379833221436\n",
      "Epoch 2 has a loss of 0.12289442867040634\n",
      "Epoch 2 has a loss of 0.10290341824293137\n",
      "Epoch 2 has a loss of 0.17479492723941803\n",
      "Epoch 3, Loss: 0.19322499493757883\n",
      "Epoch 4/15\n",
      "Epoch 3 has a loss of 0.17715205252170563\n",
      "Epoch 3 has a loss of 0.16276022791862488\n",
      "Epoch 3 has a loss of 0.17679136991500854\n",
      "Epoch 3 has a loss of 0.13550050556659698\n",
      "Epoch 3 has a loss of 0.11009301245212555\n",
      "Epoch 3 has a loss of 0.17712120711803436\n",
      "Epoch 3 has a loss of 0.11531655490398407\n",
      "Epoch 3 has a loss of 0.12898164987564087\n",
      "Epoch 3 has a loss of 0.10998115688562393\n",
      "Epoch 3 has a loss of 0.5341647267341614\n",
      "Epoch 3 has a loss of 0.12491389364004135\n",
      "Epoch 3 has a loss of 0.14446347951889038\n",
      "Epoch 4, Loss: 0.17549734258651734\n",
      "Epoch 5/15\n",
      "Epoch 4 has a loss of 0.38600704073905945\n",
      "Epoch 4 has a loss of 0.19386407732963562\n",
      "Epoch 4 has a loss of 0.21954208612442017\n",
      "Epoch 4 has a loss of 0.24118418991565704\n",
      "Epoch 4 has a loss of 0.10261284559965134\n",
      "Epoch 4 has a loss of 0.10165264457464218\n",
      "Epoch 4 has a loss of 0.13663393259048462\n",
      "Epoch 4 has a loss of 0.1283080279827118\n",
      "Epoch 4 has a loss of 0.11930397152900696\n",
      "Epoch 4 has a loss of 0.09814197570085526\n",
      "Epoch 4 has a loss of 0.17653800547122955\n",
      "Epoch 4 has a loss of 0.309946209192276\n",
      "Epoch 5, Loss: 0.18146667822202048\n",
      "Epoch 6/15\n",
      "Epoch 5 has a loss of 0.17704859375953674\n",
      "Epoch 5 has a loss of 0.2763407826423645\n",
      "Epoch 5 has a loss of 0.12597054243087769\n",
      "Epoch 5 has a loss of 0.10753146559000015\n",
      "Epoch 5 has a loss of 0.08274037390947342\n",
      "Epoch 5 has a loss of 0.1870112121105194\n",
      "Epoch 5 has a loss of 0.10285592079162598\n",
      "Epoch 5 has a loss of 0.183424711227417\n",
      "Epoch 5 has a loss of 0.1384422481060028\n",
      "Epoch 5 has a loss of 0.18061569333076477\n",
      "Epoch 5 has a loss of 0.1855948120355606\n",
      "Epoch 5 has a loss of 0.13342155516147614\n",
      "Epoch 6, Loss: 0.15730970442295075\n",
      "Epoch 7/15\n",
      "Epoch 6 has a loss of 0.19362889230251312\n",
      "Epoch 6 has a loss of 0.1446036398410797\n",
      "Epoch 6 has a loss of 0.09925161302089691\n",
      "Epoch 6 has a loss of 0.09232864528894424\n",
      "Epoch 6 has a loss of 0.11557598412036896\n",
      "Epoch 6 has a loss of 0.1168089359998703\n",
      "Epoch 6 has a loss of 0.1770218014717102\n",
      "Epoch 6 has a loss of 0.12895871698856354\n",
      "Epoch 6 has a loss of 0.12298387289047241\n",
      "Epoch 6 has a loss of 0.08388729393482208\n",
      "Epoch 6 has a loss of 0.11563148349523544\n",
      "Epoch 6 has a loss of 0.15021371841430664\n",
      "Epoch 7, Loss: 0.12788454310099284\n",
      "Epoch 8/15\n",
      "Epoch 7 has a loss of 0.09896154701709747\n",
      "Epoch 7 has a loss of 0.15626260638237\n",
      "Epoch 7 has a loss of 0.10948751121759415\n",
      "Epoch 7 has a loss of 0.14705678820610046\n",
      "Epoch 7 has a loss of 0.159348726272583\n",
      "Epoch 7 has a loss of 0.1883414089679718\n",
      "Epoch 7 has a loss of 0.20347943902015686\n",
      "Epoch 7 has a loss of 0.2348293662071228\n",
      "Epoch 7 has a loss of 0.09283140301704407\n",
      "Epoch 7 has a loss of 0.10352984070777893\n",
      "Epoch 7 has a loss of 0.08583731949329376\n",
      "Epoch 7 has a loss of 0.2162429839372635\n",
      "Epoch 8, Loss: 0.14808666463692982\n",
      "Epoch 9/15\n",
      "Epoch 8 has a loss of 0.12648905813694\n",
      "Epoch 8 has a loss of 0.123220294713974\n",
      "Epoch 8 has a loss of 0.20900976657867432\n",
      "Epoch 8 has a loss of 0.12901833653450012\n",
      "Epoch 8 has a loss of 0.07675962150096893\n",
      "Epoch 8 has a loss of 0.11362029612064362\n",
      "Epoch 8 has a loss of 0.1316993236541748\n",
      "Epoch 8 has a loss of 0.17586281895637512\n",
      "Epoch 8 has a loss of 0.42758437991142273\n",
      "Epoch 8 has a loss of 0.10492271184921265\n",
      "Epoch 8 has a loss of 0.08524324744939804\n",
      "Epoch 8 has a loss of 0.1693885326385498\n",
      "Epoch 9, Loss: 0.15574851099650064\n",
      "Epoch 10/15\n",
      "Epoch 9 has a loss of 0.06555911153554916\n",
      "Epoch 9 has a loss of 0.1607070118188858\n",
      "Epoch 9 has a loss of 0.2811398208141327\n",
      "Epoch 9 has a loss of 0.22933469712734222\n",
      "Epoch 9 has a loss of 0.14192131161689758\n",
      "Epoch 9 has a loss of 0.21206873655319214\n",
      "Epoch 9 has a loss of 0.14723971486091614\n",
      "Epoch 9 has a loss of 0.1444377452135086\n",
      "Epoch 9 has a loss of 0.13595353066921234\n",
      "Epoch 9 has a loss of 0.06865587830543518\n",
      "Epoch 9 has a loss of 0.05814347043633461\n",
      "Epoch 9 has a loss of 0.263566792011261\n",
      "Epoch 10, Loss: 0.15655250438054402\n",
      "Epoch 11/15\n",
      "Epoch 10 has a loss of 0.13548573851585388\n",
      "Epoch 10 has a loss of 0.12256009876728058\n",
      "Epoch 10 has a loss of 0.09864817559719086\n",
      "Epoch 10 has a loss of 0.0866600051522255\n",
      "Epoch 10 has a loss of 0.14460845291614532\n",
      "Epoch 10 has a loss of 0.10867949575185776\n",
      "Epoch 10 has a loss of 0.16522294282913208\n",
      "Epoch 10 has a loss of 0.36042681336402893\n",
      "Epoch 10 has a loss of 0.22547760605812073\n",
      "Epoch 10 has a loss of 0.0919172614812851\n",
      "Epoch 10 has a loss of 0.1107974499464035\n",
      "Epoch 10 has a loss of 0.23090073466300964\n",
      "Epoch 11, Loss: 0.15500321650505067\n",
      "Epoch 12/15\n",
      "Epoch 11 has a loss of 0.15196774899959564\n",
      "Epoch 11 has a loss of 0.09351909905672073\n",
      "Epoch 11 has a loss of 0.24992036819458008\n",
      "Epoch 11 has a loss of 0.08484910428524017\n",
      "Epoch 11 has a loss of 0.34199440479278564\n",
      "Epoch 11 has a loss of 0.22425802052021027\n",
      "Epoch 11 has a loss of 0.0561378076672554\n",
      "Epoch 11 has a loss of 0.2657712399959564\n",
      "Epoch 11 has a loss of 0.056035179644823074\n",
      "Epoch 11 has a loss of 0.17167574167251587\n",
      "Epoch 11 has a loss of 0.09822952747344971\n",
      "Epoch 11 has a loss of 0.08241873234510422\n",
      "Epoch 12, Loss: 0.1581735855937004\n",
      "Epoch 13/15\n",
      "Epoch 12 has a loss of 0.45837870240211487\n",
      "Epoch 12 has a loss of 0.10332828760147095\n",
      "Epoch 12 has a loss of 0.239141583442688\n",
      "Epoch 12 has a loss of 0.11135447025299072\n",
      "Epoch 12 has a loss of 0.07028592377901077\n",
      "Epoch 12 has a loss of 0.11021198332309723\n",
      "Epoch 12 has a loss of 0.23149514198303223\n",
      "Epoch 12 has a loss of 0.09599059075117111\n",
      "Epoch 12 has a loss of 0.055195994675159454\n",
      "Epoch 12 has a loss of 0.15956220030784607\n",
      "Epoch 12 has a loss of 0.07692673802375793\n",
      "Epoch 12 has a loss of 0.3430343270301819\n",
      "Epoch 13, Loss: 0.16711915000279745\n",
      "Epoch 14/15\n",
      "Epoch 13 has a loss of 0.08240159600973129\n",
      "Epoch 13 has a loss of 0.20995990931987762\n",
      "Epoch 13 has a loss of 0.13729055225849152\n",
      "Epoch 13 has a loss of 0.1214885339140892\n",
      "Epoch 13 has a loss of 0.10232125222682953\n",
      "Epoch 13 has a loss of 0.06907274574041367\n",
      "Epoch 13 has a loss of 0.314564973115921\n",
      "Epoch 13 has a loss of 0.09856629371643066\n",
      "Epoch 13 has a loss of 0.08474095165729523\n",
      "Epoch 13 has a loss of 0.15412510931491852\n",
      "Epoch 13 has a loss of 0.1968691647052765\n",
      "Epoch 13 has a loss of 0.11126191169023514\n",
      "Epoch 14, Loss: 0.1409169562458992\n",
      "Epoch 15/15\n",
      "Epoch 14 has a loss of 0.10368216782808304\n",
      "Epoch 14 has a loss of 0.11341740936040878\n",
      "Epoch 14 has a loss of 0.2673228979110718\n",
      "Epoch 14 has a loss of 0.11579921096563339\n",
      "Epoch 14 has a loss of 0.12652604281902313\n",
      "Epoch 14 has a loss of 0.07146978378295898\n",
      "Epoch 14 has a loss of 0.06023533642292023\n",
      "Epoch 14 has a loss of 0.07392630726099014\n",
      "Epoch 14 has a loss of 0.11914516240358353\n",
      "Epoch 14 has a loss of 0.12510982155799866\n",
      "Epoch 14 has a loss of 0.08852320909500122\n",
      "Epoch 14 has a loss of 0.11710641533136368\n",
      "Epoch 15, Loss: 0.11514262062311173\n",
      "Prototypes updated using weighted average.\n",
      "Saved model after self-training with T2PL.\n",
      "started_accuracy_computation\n",
      "{86.56}\n",
      "{86.56}\n",
      "{85.68}\n",
      "{85.32}\n",
      "{86.16}\n",
      "{85.68}\n",
      "{85.12}\n",
      "{86.16}\n",
      "{85.96}\n",
      "{85.84}\n",
      "{78.4}\n",
      "{49.52}\n",
      "{70.84}\n",
      "{82.72}\n",
      "{85.6}\n",
      "{71.56}\n",
      "{82.28}\n",
      "{69.24}\n",
      "{57.68}\n",
      "{86.56}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[91.28,\n",
       "  90.52,\n",
       "  90.4,\n",
       "  90.56,\n",
       "  90.36,\n",
       "  90.76,\n",
       "  90.44,\n",
       "  91.64,\n",
       "  90.72,\n",
       "  90.92,\n",
       "  88.96,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " [87.84,\n",
       "  87.48,\n",
       "  86.48,\n",
       "  86.32,\n",
       "  87.0,\n",
       "  87.32,\n",
       "  86.16,\n",
       "  88.08,\n",
       "  86.52,\n",
       "  88.0,\n",
       "  75.64,\n",
       "  70.88,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " [90.08,\n",
       "  90.44,\n",
       "  89.4,\n",
       "  89.0,\n",
       "  89.96,\n",
       "  89.68,\n",
       "  89.12,\n",
       "  90.48,\n",
       "  89.8,\n",
       "  90.0,\n",
       "  80.8,\n",
       "  65.72,\n",
       "  81.64,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " [89.52,\n",
       "  89.52,\n",
       "  88.32,\n",
       "  87.84,\n",
       "  88.6,\n",
       "  89.12,\n",
       "  89.04,\n",
       "  89.08,\n",
       "  88.96,\n",
       "  88.36,\n",
       "  83.0,\n",
       "  63.88,\n",
       "  78.88,\n",
       "  88.84,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " [90.24,\n",
       "  90.32,\n",
       "  88.8,\n",
       "  89.0,\n",
       "  89.68,\n",
       "  89.6,\n",
       "  89.16,\n",
       "  90.2,\n",
       "  89.68,\n",
       "  89.76,\n",
       "  83.72,\n",
       "  60.12,\n",
       "  78.56,\n",
       "  88.4,\n",
       "  89.6,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " [90.36,\n",
       "  90.2,\n",
       "  89.8,\n",
       "  88.68,\n",
       "  90.2,\n",
       "  89.52,\n",
       "  89.0,\n",
       "  90.48,\n",
       "  89.36,\n",
       "  90.28,\n",
       "  80.04,\n",
       "  65.32,\n",
       "  81.12,\n",
       "  88.24,\n",
       "  90.0,\n",
       "  81.24,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " [89.6,\n",
       "  89.4,\n",
       "  88.68,\n",
       "  88.16,\n",
       "  89.0,\n",
       "  88.52,\n",
       "  87.76,\n",
       "  89.0,\n",
       "  89.24,\n",
       "  88.96,\n",
       "  83.8,\n",
       "  60.6,\n",
       "  78.24,\n",
       "  88.72,\n",
       "  88.12,\n",
       "  77.32,\n",
       "  88.96,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " [88.28,\n",
       "  88.6,\n",
       "  87.48,\n",
       "  86.88,\n",
       "  87.88,\n",
       "  87.44,\n",
       "  87.04,\n",
       "  88.64,\n",
       "  88.08,\n",
       "  88.44,\n",
       "  76.56,\n",
       "  65.44,\n",
       "  80.0,\n",
       "  85.4,\n",
       "  88.68,\n",
       "  78.64,\n",
       "  86.72,\n",
       "  80.48,\n",
       "  None,\n",
       "  None],\n",
       " [87.48,\n",
       "  88.08,\n",
       "  86.12,\n",
       "  86.12,\n",
       "  87.0,\n",
       "  87.0,\n",
       "  86.44,\n",
       "  87.04,\n",
       "  87.08,\n",
       "  87.32,\n",
       "  76.96,\n",
       "  54.16,\n",
       "  74.68,\n",
       "  85.2,\n",
       "  86.8,\n",
       "  73.2,\n",
       "  85.12,\n",
       "  72.6,\n",
       "  78.8,\n",
       "  None],\n",
       " [86.56,\n",
       "  86.56,\n",
       "  85.68,\n",
       "  85.32,\n",
       "  86.16,\n",
       "  85.68,\n",
       "  85.12,\n",
       "  86.16,\n",
       "  85.96,\n",
       "  85.84,\n",
       "  78.4,\n",
       "  49.52,\n",
       "  70.84,\n",
       "  82.72,\n",
       "  85.6,\n",
       "  71.56,\n",
       "  82.28,\n",
       "  69.24,\n",
       "  57.68,\n",
       "  86.56]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import copy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "feature_dir = '/kaggle/working/files'\n",
    "os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "# Distillation Loss Function\n",
    "def distillation_loss(student_outputs, teacher_outputs, temperature=2.0):\n",
    "    teacher_probs = F.softmax(teacher_outputs / temperature, dim=1)\n",
    "    student_probs = F.log_softmax(student_outputs / temperature, dim=1)\n",
    "    return F.kl_div(student_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, targets=None, transform=None):\n",
    "        self.images = images\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        target = self.targets[idx] if self.targets is not None else -1\n",
    "        return image, target\n",
    "\n",
    "    \n",
    "# Load Data from .pth file\n",
    "def load_data_from_pth(pth_path, has_targets=True):\n",
    "    data_dict = torch.load(pth_path)\n",
    "    data = data_dict['data']\n",
    "    targets = data_dict['targets'] if has_targets else None\n",
    "    return data, targets\n",
    "\n",
    "# Paths to Datasets\n",
    "# Edit these paths to run the code on your machine\n",
    "d1_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/1_train_data.tar.pth'\n",
    "d1hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/1_eval_data.tar.pth'\n",
    "d2_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/2_train_data.tar.pth'\n",
    "d2hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/2_eval_data.tar.pth'\n",
    "d3_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/3_train_data.tar.pth'\n",
    "d3hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/3_eval_data.tar.pth'\n",
    "\n",
    "d4_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/4_train_data.tar.pth'\n",
    "d4hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/4_eval_data.tar.pth'\n",
    "\n",
    "d5_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/5_train_data.tar.pth'\n",
    "d5hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/5_eval_data.tar.pth'\n",
    "\n",
    "d6_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/6_train_data.tar.pth'\n",
    "d6hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/6_eval_data.tar.pth'\n",
    "\n",
    "d7_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/7_train_data.tar.pth'\n",
    "d7hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/7_eval_data.tar.pth'\n",
    "\n",
    "d8_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/8_train_data.tar.pth'\n",
    "d8hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/8_eval_data.tar.pth'\n",
    "\n",
    "d9_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/9_train_data.tar.pth'\n",
    "d9hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/9_eval_data.tar.pth'\n",
    "\n",
    "d10_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/train_data/10_train_data.tar.pth'\n",
    "d10hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_one_dataset/eval_data/10_eval_data.tar.pth'\n",
    "\n",
    "d11_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/1_train_data.tar.pth'\n",
    "d11hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/1_eval_data.tar.pth'\n",
    "\n",
    "d12_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/2_train_data.tar.pth'\n",
    "d12hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/2_eval_data.tar.pth'\n",
    "\n",
    "d13_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/3_train_data.tar.pth'\n",
    "d13hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/3_eval_data.tar.pth'\n",
    "\n",
    "d14_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/4_train_data.tar.pth'\n",
    "d14hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/4_eval_data.tar.pth'\n",
    "\n",
    "d15_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/5_train_data.tar.pth'\n",
    "d15hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/5_eval_data.tar.pth'\n",
    "\n",
    "d16_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/6_train_data.tar.pth'\n",
    "d16hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/6_eval_data.tar.pth'\n",
    "\n",
    "d17_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/7_train_data.tar.pth'\n",
    "d17hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/7_eval_data.tar.pth'\n",
    "\n",
    "d18_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/8_train_data.tar.pth'\n",
    "d18hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/8_eval_data.tar.pth'\n",
    "\n",
    "d19_path = '//kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/9_train_data.tar.pth'\n",
    "d19hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/9_eval_data.tar.pth'\n",
    "\n",
    "d20_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/train_data/10_train_data.tar.pth'\n",
    "d20hat_path = '/kaggle/input/dataset-for-proj-2/dataset/dataset/part_two_dataset/eval_data/10_eval_data.tar.pth'\n",
    "\n",
    "print({\"reached_path_to_datasets\"})\n",
    "\n",
    "# Load Datasets\n",
    "d1_data, d1_targets = load_data_from_pth(d1_path)\n",
    "d1hat_data, d1hat_targets = load_data_from_pth(d1hat_path)\n",
    "d2_data, _ = load_data_from_pth(d2_path, has_targets=False)\n",
    "d2hat_data, d2hat_targets = load_data_from_pth(d2hat_path)\n",
    "\n",
    "d3_data, _ = load_data_from_pth(d3_path, has_targets=False)\n",
    "d3hat_data, d3hat_targets = load_data_from_pth(d3hat_path)\n",
    "\n",
    "d4_data, _ = load_data_from_pth(d4_path, has_targets=False)\n",
    "d4hat_data, d4hat_targets = load_data_from_pth(d4hat_path)\n",
    "\n",
    "d5_data, _ = load_data_from_pth(d5_path, has_targets=False)\n",
    "d5hat_data, d5hat_targets = load_data_from_pth(d5hat_path)\n",
    "\n",
    "d6_data, _ = load_data_from_pth(d6_path, has_targets=False)\n",
    "d6hat_data, d6hat_targets = load_data_from_pth(d6hat_path)\n",
    "\n",
    "d7_data, _ = load_data_from_pth(d7_path, has_targets=False)\n",
    "d7hat_data, d7hat_targets = load_data_from_pth(d7hat_path)\n",
    "\n",
    "d8_data, _ = load_data_from_pth(d8_path, has_targets=False)\n",
    "d8hat_data, d8hat_targets = load_data_from_pth(d8hat_path)\n",
    "\n",
    "d9_data, _ = load_data_from_pth(d9_path, has_targets=False)\n",
    "d9hat_data, d9hat_targets = load_data_from_pth(d9hat_path)\n",
    "\n",
    "d10_data, _ = load_data_from_pth(d10_path, has_targets=False)\n",
    "d10hat_data, d10hat_targets = load_data_from_pth(d10hat_path)\n",
    "\n",
    "d11_data, _ = load_data_from_pth(d11_path, has_targets=False)\n",
    "d11hat_data, d11hat_targets = load_data_from_pth(d11hat_path)\n",
    "\n",
    "d12_data, _ = load_data_from_pth(d12_path, has_targets=False)\n",
    "d12hat_data, d12hat_targets = load_data_from_pth(d12hat_path)\n",
    "\n",
    "d13_data, _ = load_data_from_pth(d13_path, has_targets=False)\n",
    "d13hat_data, d13hat_targets = load_data_from_pth(d13hat_path)\n",
    "\n",
    "d14_data, _ = load_data_from_pth(d14_path, has_targets=False)\n",
    "d14hat_data, d14hat_targets = load_data_from_pth(d14hat_path)\n",
    "\n",
    "d15_data, _ = load_data_from_pth(d15_path, has_targets=False)\n",
    "d15hat_data, d15hat_targets = load_data_from_pth(d15hat_path)\n",
    "\n",
    "d16_data, _ = load_data_from_pth(d16_path, has_targets=False)\n",
    "d16hat_data, d16hat_targets = load_data_from_pth(d16hat_path)\n",
    "\n",
    "d17_data, _ = load_data_from_pth(d17_path, has_targets=False)\n",
    "d17hat_data, d17hat_targets = load_data_from_pth(d17hat_path)\n",
    "\n",
    "d18_data, _ = load_data_from_pth(d18_path, has_targets=False)\n",
    "d18hat_data, d18hat_targets = load_data_from_pth(d18hat_path)\n",
    "\n",
    "d19_data, _ = load_data_from_pth(d19_path, has_targets=False)\n",
    "d19hat_data, d19hat_targets = load_data_from_pth(d19hat_path)\n",
    "\n",
    "d20_data, _ = load_data_from_pth(d20_path, has_targets=False)\n",
    "d20hat_data, d20hat_targets = load_data_from_pth(d20hat_path)\n",
    "\n",
    "\n",
    "# Data Transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = CustomImageDataset(d1_data, d1_targets, transform=train_transform)\n",
    "d1hat_dataset = CustomImageDataset(d1hat_data, d1hat_targets, transform=test_transform)\n",
    "d2hat_dataset = CustomImageDataset(d2hat_data, d2hat_targets, transform=test_transform)\n",
    "d3hat_dataset = CustomImageDataset(d3hat_data, d3hat_targets, transform=test_transform)\n",
    "d4hat_dataset = CustomImageDataset(d4hat_data, d4hat_targets, transform=test_transform)\n",
    "d5hat_dataset = CustomImageDataset(d5hat_data, d5hat_targets, transform=test_transform)\n",
    "d6hat_dataset = CustomImageDataset(d6hat_data, d6hat_targets, transform=test_transform)\n",
    "d7hat_dataset = CustomImageDataset(d7hat_data, d7hat_targets, transform=test_transform)\n",
    "d8hat_dataset = CustomImageDataset(d8hat_data, d8hat_targets, transform=test_transform)\n",
    "d9hat_dataset = CustomImageDataset(d9hat_data, d9hat_targets, transform=test_transform)\n",
    "d10hat_dataset = CustomImageDataset(d10hat_data, d10hat_targets, transform=test_transform)\n",
    "d11hat_dataset = CustomImageDataset(d11hat_data, d11hat_targets, transform=test_transform)\n",
    "d12hat_dataset = CustomImageDataset(d12hat_data, d12hat_targets, transform=test_transform)\n",
    "d13hat_dataset = CustomImageDataset(d13hat_data, d13hat_targets, transform=test_transform)\n",
    "\n",
    "# For 14\n",
    "d14hat_dataset = CustomImageDataset(d14hat_data, d14hat_targets, transform=test_transform)\n",
    "# For 15\n",
    "d15hat_dataset = CustomImageDataset(d15hat_data, d15hat_targets, transform=test_transform)\n",
    "# For 16\n",
    "d16hat_dataset = CustomImageDataset(d16hat_data, d16hat_targets, transform=test_transform)\n",
    "# For 17\n",
    "d17hat_dataset = CustomImageDataset(d17hat_data, d17hat_targets, transform=test_transform)\n",
    "# For 18\n",
    "d18hat_dataset = CustomImageDataset(d18hat_data, d18hat_targets, transform=test_transform)\n",
    "# For 19\n",
    "d19hat_dataset = CustomImageDataset(d19hat_data, d19hat_targets, transform=test_transform)\n",
    "# For 20\n",
    "d20hat_dataset = CustomImageDataset(d20hat_data, d20hat_targets, transform=test_transform)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "d1hat_loader = DataLoader(d1hat_dataset, batch_size=32, shuffle=False)\n",
    "d2hat_loader = DataLoader(d2hat_dataset, batch_size=32, shuffle=False)\n",
    "d3hat_loader = DataLoader(d3hat_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "d4hat_loader = DataLoader(d4hat_dataset, batch_size=32, shuffle=False)\n",
    "d5hat_loader = DataLoader(d5hat_dataset, batch_size=32, shuffle=False)\n",
    "d6hat_loader = DataLoader(d6hat_dataset, batch_size=32, shuffle=False)\n",
    "d7hat_loader = DataLoader(d7hat_dataset, batch_size=32, shuffle=False)\n",
    "d8hat_loader = DataLoader(d8hat_dataset, batch_size=32, shuffle=False)\n",
    "d9hat_loader = DataLoader(d9hat_dataset, batch_size=32, shuffle=False)\n",
    "d10hat_loader = DataLoader(d10hat_dataset, batch_size=32, shuffle=False)\n",
    "d11hat_loader = DataLoader(d11hat_dataset, batch_size=32, shuffle=False)\n",
    "d12hat_loader = DataLoader(d12hat_dataset, batch_size=32, shuffle=False)\n",
    "d13hat_loader = DataLoader(d13hat_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# For 14\n",
    "d14hat_loader = DataLoader(d14hat_dataset, batch_size=32, shuffle=False)\n",
    "# For 15\n",
    "d15hat_loader = DataLoader(d15hat_dataset, batch_size=32, shuffle=False)\n",
    "# For 16\n",
    "d16hat_loader = DataLoader(d16hat_dataset, batch_size=32, shuffle=False)\n",
    "# For 17\n",
    "d17hat_loader = DataLoader(d17hat_dataset, batch_size=32, shuffle=False)\n",
    "# For 18\n",
    "d18hat_loader = DataLoader(d18hat_dataset, batch_size=32, shuffle=False)\n",
    "# For 19\n",
    "d19hat_loader = DataLoader(d19hat_dataset, batch_size=32, shuffle=False)\n",
    "# For 20\n",
    "d20hat_loader = DataLoader(d20hat_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Feature Extractor: ResNet50\n",
    "class ResNet50Extractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50Extractor, self).__init__()\n",
    "        resnet_model = models.resnet50(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet_model.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "    \n",
    "# Learning with Prototypes Model\n",
    "class LearningWithPrototypes(nn.Module):\n",
    "    def __init__(self, feature_extractor, num_classes, prototype_dim=2048):\n",
    "        super(LearningWithPrototypes, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.fc = nn.Linear(prototype_dim, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "        self.prototypes = nn.Parameter(torch.randn(num_classes, prototype_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        logits = self.fc(features)\n",
    "        return logits, features\n",
    "\n",
    "    def prototype_loss(self, features, labels):\n",
    "        distances = torch.cdist(features, self.prototypes)\n",
    "        return F.cross_entropy(-distances, labels)\n",
    "    \n",
    "    def update_prototypes(self, features, labels):\n",
    "        for class_idx in range(self.num_classes):\n",
    "            class_features = features[labels == class_idx]\n",
    "            if class_features.size(0) > 0:\n",
    "                self.prototypes.data[class_idx] = class_features.mean(dim=0)\n",
    "\n",
    "    def update_prototypes_with_ratp(self, features, labels, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Update prototypes with new high-confidence samples.\n",
    "\n",
    "        Parameters:\n",
    "        - features: Tensor of shape (batch_size, feature_dim), feature representations of the dataset.\n",
    "        - labels: Tensor of shape (batch_size,), corresponding pseudo-labels.\n",
    "        - alpha: Weight for blending old and new prototypes (default: 0.5).\n",
    "        \"\"\"\n",
    "        for class_idx in range(self.num_classes):\n",
    "            # Select features belonging to the current class\n",
    "            class_mask = (labels == class_idx)\n",
    "            class_features = features[class_mask]  # Shape: (num_samples_in_class, feature_dim)\n",
    "            \n",
    "            if class_features.size(0) > 0:\n",
    "                # Compute new centroid for this class (average over batch dimension)\n",
    "                new_centroid = class_features.mean(dim=0)  # Shape: (feature_dim,)\n",
    "                \n",
    "                # Blend the old and new prototypes\n",
    "                self.prototypes.data[class_idx] = (\n",
    "                    alpha * self.prototypes.data[class_idx] + (1 - alpha) * new_centroid\n",
    "                )\n",
    "                \n",
    "                \n",
    "                \n",
    "# Initialize Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "feature_extractor = ResNet50Extractor().to(device)\n",
    "model = LearningWithPrototypes(feature_extractor, num_classes=len(set(d1_targets))).to(device)\n",
    "\n",
    "\n",
    "print({\"started\"})\n",
    "\n",
    "# Load Pretrained Prototypes if Available\n",
    "if os.path.exists('/kaggle/input/weight-files/prototypes.pth'):\n",
    "    model.prototypes.data = torch.load('/kaggle/input/weight-files/prototypes.pth')\n",
    "    print(\"Loaded saved prototypes.\")\n",
    "\n",
    "# Loss Function, Optimizer, and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "alpha = 0.1\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "\n",
    "def self_train_update_with_t2pl(model, unlabeled_data, num, num_epochs=15, temperature=2.0, alpha=0.5, beta=0.7, top_percentage=0.3, top_5_percentage=0.3):\n",
    "    model.eval()\n",
    "    \n",
    "    # Step 1: Generate softmax outputs and select the top confident samples\n",
    "    softmax_outputs = []\n",
    "    representations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in DataLoader(CustomImageDataset(unlabeled_data, transform=test_transform), batch_size=32):\n",
    "            images = images.to(device)\n",
    "            outputs, features = model(images)\n",
    "            softmax_probs = F.softmax(outputs / temperature, dim=1)\n",
    "            softmax_outputs.append(softmax_probs.cpu().numpy())\n",
    "            representations.append(features.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all softmax outputs and features\n",
    "    softmax_outputs = np.concatenate(softmax_outputs, axis=0)\n",
    "    representations = np.concatenate(representations, axis=0)\n",
    "    \n",
    "    # Step 2: Select top confident samples based on a percentage of the data\n",
    "    top_k_count = int(top_percentage * softmax_outputs.shape[0])  # Calculate the top percentage dynamically\n",
    "    top_k_indices = np.argsort(np.max(softmax_outputs, axis=1))[-top_k_count:]\n",
    "    top_k_softmax = softmax_outputs[top_k_indices]\n",
    "    top_k_representations = representations[top_k_indices]\n",
    "    \n",
    "    # Step 3: Construct class centroids using the top samples\n",
    "    class_centroids = []\n",
    "    num_classes = softmax_outputs.shape[1]\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        class_samples = top_k_representations[np.argmax(top_k_softmax, axis=1) == class_idx]\n",
    "        if class_samples.shape[0] > 0:\n",
    "            centroid = class_samples.mean(axis=0)\n",
    "        else:\n",
    "            centroid = np.zeros_like(top_k_representations[0])\n",
    "        class_centroids.append(centroid)\n",
    "    \n",
    "    class_centroids = np.array(class_centroids)\n",
    "    \n",
    "    # Step 4: Calculate cosine similarity between centroids and representations of all unlabeled samples\n",
    "    similarities = np.dot(representations, class_centroids.T) / (np.linalg.norm(representations, axis=1)[:, None] * np.linalg.norm(class_centroids, axis=1))\n",
    "    \n",
    "    # Step 5: Select top half of the samples for pseudo-labeling\n",
    "    top_half_count = representations.shape[0] // 2  # Calculate the top 50% based on the number of samples\n",
    "    top_half_indices = np.argsort(np.max(similarities, axis=1))[-top_half_count:]\n",
    "    \n",
    "    # Step 6: Use kNN for final pseudo-label assignment\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "    knn.fit(top_k_representations, np.argmax(top_k_softmax, axis=1))\n",
    "    final_pseudo_labels = knn.predict(representations[top_half_indices])\n",
    "    \n",
    "    # Step 7: Select top 5% samples based on Euclidean distance\n",
    "    distances = knn.kneighbors(representations[top_half_indices], return_distance=True)[0]\n",
    "    top_5_count = int(top_5_percentage * top_half_count)\n",
    "    top_5_indices = np.argsort(distances.min(axis=1))[:top_5_count]\n",
    "    \n",
    "    # Filter `unlabeled_data` to include only samples in `top_5_indices`\n",
    "    filtered_unlabeled_data = [unlabeled_data[top_half_indices[idx]] for idx in top_5_indices]\n",
    "    \n",
    "    # Update pseudo dataset with new pseudo labels, only for the selected samples\n",
    "    pseudo_dataset = CustomImageDataset(filtered_unlabeled_data, final_pseudo_labels[top_5_indices], transform=train_transform)\n",
    "    pseudo_loader = DataLoader(pseudo_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Fine-tune model on the pseudo-labeled data\n",
    "    new_model = copy.deepcopy(model)\n",
    "    optimizer = optim.AdamW(new_model.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    # To track loss\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        running_loss = 0.0\n",
    "        new_model.train()\n",
    "        for images, labels in pseudo_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, features = new_model(images)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs, _ = model(images)\n",
    "            ce_loss = criterion(outputs, labels)\n",
    "            proto_loss = new_model.prototype_loss(features, labels)\n",
    "            dist_loss = distillation_loss(outputs, teacher_outputs, temperature)\n",
    "            loss = ce_loss + alpha * proto_loss + beta * dist_loss\n",
    "            print(f\"Epoch {epoch} has a loss of {loss}\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Average loss over the epoch\n",
    "        epoch_loss = running_loss / len(pseudo_loader.dataset)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "         # Step 3: Update Prototypes with high-confidence samples\n",
    "        with torch.no_grad():\n",
    "            all_features = []\n",
    "            all_labels = []\n",
    "            for images, labels in pseudo_loader:\n",
    "                images = images.to(device)\n",
    "                \n",
    "                # Extract features directly from feature extractor\n",
    "                features = new_model.feature_extractor(images).view(images.size(0), -1)\n",
    "                all_features.append(features)\n",
    "                all_labels.append(labels.to(device))\n",
    "            \n",
    "            all_features = torch.cat(all_features, dim=0)\n",
    "            all_labels = torch.cat(all_labels, dim=0)\n",
    "            \n",
    "            # Update prototypes with high-confidence samples\n",
    "            new_model.update_prototypes_with_ratp(all_features, all_labels, alpha=0.5)\n",
    "    with torch.no_grad():\n",
    "        weight_old = (num - 1) / num\n",
    "        weight_new = 1 / num\n",
    "        new_model.prototypes.data = (\n",
    "            weight_old * model.prototypes.data + weight_new * new_model.prototypes.data\n",
    "        )\n",
    "        print(\"Prototypes updated using weighted average.\")\n",
    "    # Save model after self-training\n",
    "    torch.save(new_model.state_dict(), f'/kaggle/working/files/model_f{num}_weights.pth')\n",
    "    print(\"Saved model after self-training with T2PL.\")\n",
    "    \n",
    " \n",
    "    \n",
    "    return new_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to Calculate Accuracy\n",
    "def calculate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs, _ = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "accuracy_matrix = []\n",
    "\n",
    "if os.path.exists('/kaggle/input/d10-weights/model_f10_weights.pth'):\n",
    "    model_f10 = copy.deepcopy(model)\n",
    "    model_f10.load_state_dict(torch.load('/kaggle/input/d10-weights/model_f10_weights.pth'))\n",
    "    print(\"Loaded self-trained model. for model 10\")\n",
    "else:\n",
    "    print(\"Model after training on dataset 10 is not availabale, please load it with the appropriate address,\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D11) using self-training and distillation\n",
    "if os.path.exists('/kaggle/working/files/model_f11_weights.pth'):\n",
    "    model_f11 = copy.deepcopy(model)\n",
    "    model_f11.load_state_dict(torch.load('/kaggle/working/files/model_f11_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f11 = self_train_update_with_t2pl(model_f10, d11_data, 11)\n",
    "\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f11_acc_d1hat = calculate_accuracy(model_f11,d1hat_loader)\n",
    "print({f11_acc_d1hat})\n",
    "f11_acc_d2hat = calculate_accuracy(model_f11,d2hat_loader)\n",
    "print({f11_acc_d2hat})\n",
    "f11_acc_d3hat = calculate_accuracy(model_f11,d3hat_loader)\n",
    "print({f11_acc_d3hat})\n",
    "f11_acc_d4hat = calculate_accuracy(model_f11,d4hat_loader)\n",
    "print({f11_acc_d4hat})\n",
    "f11_acc_d5hat = calculate_accuracy(model_f11,d5hat_loader)\n",
    "print({f11_acc_d5hat})\n",
    "f11_acc_d6hat = calculate_accuracy(model_f11,d6hat_loader)\n",
    "print({f11_acc_d6hat})\n",
    "f11_acc_d7hat = calculate_accuracy(model_f11,d7hat_loader)\n",
    "print({f11_acc_d7hat})\n",
    "f11_acc_d8hat = calculate_accuracy(model_f11,d8hat_loader)\n",
    "print({f11_acc_d8hat})\n",
    "f11_acc_d9hat = calculate_accuracy(model_f11,d9hat_loader)\n",
    "print({f11_acc_d9hat})\n",
    "f11_acc_d10hat = calculate_accuracy(model_f11,d10hat_loader)\n",
    "print({f11_acc_d10hat})\n",
    "\n",
    "f11_acc_d11hat = calculate_accuracy(model_f11,d11hat_loader)\n",
    "print({f11_acc_d11hat})\n",
    "accuracy_matrix.append([f11_acc_d1hat,f11_acc_d2hat,f11_acc_d3hat,f11_acc_d4hat,f11_acc_d5hat,f11_acc_d6hat,f11_acc_d7hat,f11_acc_d8hat,f11_acc_d9hat,f11_acc_d10hat,f11_acc_d11hat,None,None,None,None,None,None,None,None,None])\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D12) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f12_weights.pth'):\n",
    "    model_f12 = copy.deepcopy(model)\n",
    "    model_f12.load_state_dict(torch.load('/kaggle/input/weight-files/model_f12_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f12 = self_train_update_with_t2pl(model_f11, d12_data,12)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f12_acc_d1hat = calculate_accuracy(model_f12,d1hat_loader)\n",
    "print({f12_acc_d1hat})\n",
    "f12_acc_d2hat = calculate_accuracy(model_f12,d2hat_loader)\n",
    "print({f12_acc_d2hat})\n",
    "f12_acc_d3hat = calculate_accuracy(model_f12,d3hat_loader)\n",
    "print({f12_acc_d3hat})\n",
    "f12_acc_d4hat = calculate_accuracy(model_f12,d4hat_loader)\n",
    "print({f12_acc_d4hat})\n",
    "f12_acc_d5hat = calculate_accuracy(model_f12,d5hat_loader)\n",
    "print({f12_acc_d5hat})\n",
    "f12_acc_d6hat = calculate_accuracy(model_f12,d6hat_loader)\n",
    "print({f12_acc_d6hat})\n",
    "f12_acc_d7hat = calculate_accuracy(model_f12,d7hat_loader)\n",
    "print({f12_acc_d7hat})\n",
    "f12_acc_d8hat = calculate_accuracy(model_f12,d8hat_loader)\n",
    "print({f12_acc_d8hat})\n",
    "f12_acc_d9hat = calculate_accuracy(model_f12,d9hat_loader)\n",
    "print({f12_acc_d9hat})\n",
    "f12_acc_d10hat = calculate_accuracy(model_f12,d10hat_loader)\n",
    "print({f12_acc_d10hat})\n",
    "f12_acc_d11hat = calculate_accuracy(model_f12,d11hat_loader)\n",
    "print({f12_acc_d11hat})\n",
    "f12_acc_d12hat = calculate_accuracy(model_f12,d12hat_loader)\n",
    "print({f12_acc_d12hat})\n",
    "accuracy_matrix.append([f12_acc_d1hat,f12_acc_d2hat,f12_acc_d3hat,f12_acc_d4hat,f12_acc_d5hat,f12_acc_d6hat,f12_acc_d7hat,f12_acc_d8hat,f12_acc_d9hat,f12_acc_d10hat,f12_acc_d11hat,f12_acc_d12hat,None,None,None,None,None,None,None,None])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D13) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f13_weights.pth'):\n",
    "    model_f13 = copy.deepcopy(model)\n",
    "    model_f13.load_state_dict(torch.load('/kaggle/input/weight-files/model_f13_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f13 = self_train_update_with_t2pl(model_f12, d13_data, 13)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f13_acc_d1hat = calculate_accuracy(model_f13, d1hat_loader)\n",
    "print({f13_acc_d1hat})\n",
    "f13_acc_d2hat = calculate_accuracy(model_f13, d2hat_loader)\n",
    "print({f13_acc_d2hat})\n",
    "f13_acc_d3hat = calculate_accuracy(model_f13, d3hat_loader)\n",
    "print({f13_acc_d3hat})\n",
    "f13_acc_d4hat = calculate_accuracy(model_f13, d4hat_loader)\n",
    "print({f13_acc_d4hat})\n",
    "f13_acc_d5hat = calculate_accuracy(model_f13, d5hat_loader)\n",
    "print({f13_acc_d5hat})\n",
    "f13_acc_d6hat = calculate_accuracy(model_f13, d6hat_loader)\n",
    "print({f13_acc_d6hat})\n",
    "f13_acc_d7hat = calculate_accuracy(model_f13, d7hat_loader)\n",
    "print({f13_acc_d7hat})\n",
    "f13_acc_d8hat = calculate_accuracy(model_f13, d8hat_loader)\n",
    "print({f13_acc_d8hat})\n",
    "f13_acc_d9hat = calculate_accuracy(model_f13, d9hat_loader)\n",
    "print({f13_acc_d9hat})\n",
    "f13_acc_d10hat = calculate_accuracy(model_f13, d10hat_loader)\n",
    "print({f13_acc_d10hat})\n",
    "f13_acc_d11hat = calculate_accuracy(model_f13, d11hat_loader)\n",
    "print({f13_acc_d11hat})\n",
    "f13_acc_d12hat = calculate_accuracy(model_f13, d12hat_loader)\n",
    "print({f13_acc_d12hat})\n",
    "f13_acc_d13hat = calculate_accuracy(model_f13, d13hat_loader)\n",
    "print({f13_acc_d13hat})\n",
    "accuracy_matrix.append([f13_acc_d1hat, f13_acc_d2hat, f13_acc_d3hat, f13_acc_d4hat, f13_acc_d5hat, f13_acc_d6hat, f13_acc_d7hat, f13_acc_d8hat, f13_acc_d9hat, f13_acc_d10hat, f13_acc_d11hat, f13_acc_d12hat, f13_acc_d13hat,None,None,None,None,None,None,None])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D14) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f14_weights.pth'):\n",
    "    model_f14 = copy.deepcopy(model)\n",
    "    model_f14.load_state_dict(torch.load('/kaggle/input/weight-files/model_f14_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f14 = self_train_update_with_t2pl(model_f13, d14_data, 14)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f14_acc_d1hat = calculate_accuracy(model_f14, d1hat_loader)\n",
    "print({f14_acc_d1hat})\n",
    "f14_acc_d2hat = calculate_accuracy(model_f14, d2hat_loader)\n",
    "print({f14_acc_d2hat})\n",
    "f14_acc_d3hat = calculate_accuracy(model_f14, d3hat_loader)\n",
    "print({f14_acc_d3hat})\n",
    "f14_acc_d4hat = calculate_accuracy(model_f14, d4hat_loader)\n",
    "print({f14_acc_d4hat})\n",
    "f14_acc_d5hat = calculate_accuracy(model_f14, d5hat_loader)\n",
    "print({f14_acc_d5hat})\n",
    "f14_acc_d6hat = calculate_accuracy(model_f14, d6hat_loader)\n",
    "print({f14_acc_d6hat})\n",
    "f14_acc_d7hat = calculate_accuracy(model_f14, d7hat_loader)\n",
    "print({f14_acc_d7hat})\n",
    "f14_acc_d8hat = calculate_accuracy(model_f14, d8hat_loader)\n",
    "print({f14_acc_d8hat})\n",
    "f14_acc_d9hat = calculate_accuracy(model_f14, d9hat_loader)\n",
    "print({f14_acc_d9hat})\n",
    "f14_acc_d10hat = calculate_accuracy(model_f14, d10hat_loader)\n",
    "print({f14_acc_d10hat})\n",
    "f14_acc_d11hat = calculate_accuracy(model_f14, d11hat_loader)\n",
    "print({f14_acc_d11hat})\n",
    "f14_acc_d12hat = calculate_accuracy(model_f14, d12hat_loader)\n",
    "print({f14_acc_d12hat})\n",
    "f14_acc_d13hat = calculate_accuracy(model_f14, d13hat_loader)\n",
    "print({f14_acc_d13hat})\n",
    "f14_acc_d14hat = calculate_accuracy(model_f14, d14hat_loader)\n",
    "print({f14_acc_d14hat})\n",
    "accuracy_matrix.append([f14_acc_d1hat, f14_acc_d2hat, f14_acc_d3hat, f14_acc_d4hat, f14_acc_d5hat, f14_acc_d6hat, f14_acc_d7hat, f14_acc_d8hat, f14_acc_d9hat, f14_acc_d10hat, f14_acc_d11hat, f14_acc_d12hat, f14_acc_d13hat, f14_acc_d14hat,None,None,None,None,None,None])\n",
    "\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D15) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f15_weights.pth'):\n",
    "    model_f15 = copy.deepcopy(model)\n",
    "    model_f15.load_state_dict(torch.load('/kaggle/input/weight-files/model_f15_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f15 = self_train_update_with_t2pl(model_f14, d15_data, 15)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f15_acc_d1hat = calculate_accuracy(model_f15, d1hat_loader)\n",
    "print({f15_acc_d1hat})\n",
    "f15_acc_d2hat = calculate_accuracy(model_f15, d2hat_loader)\n",
    "print({f15_acc_d2hat})\n",
    "f15_acc_d3hat = calculate_accuracy(model_f15, d3hat_loader)\n",
    "print({f15_acc_d3hat})\n",
    "f15_acc_d4hat = calculate_accuracy(model_f15, d4hat_loader)\n",
    "print({f15_acc_d4hat})\n",
    "f15_acc_d5hat = calculate_accuracy(model_f15, d5hat_loader)\n",
    "print({f15_acc_d5hat})\n",
    "f15_acc_d6hat = calculate_accuracy(model_f15, d6hat_loader)\n",
    "print({f15_acc_d6hat})\n",
    "f15_acc_d7hat = calculate_accuracy(model_f15, d7hat_loader)\n",
    "print({f15_acc_d7hat})\n",
    "f15_acc_d8hat = calculate_accuracy(model_f15, d8hat_loader)\n",
    "print({f15_acc_d8hat})\n",
    "f15_acc_d9hat = calculate_accuracy(model_f15, d9hat_loader)\n",
    "print({f15_acc_d9hat})\n",
    "f15_acc_d10hat = calculate_accuracy(model_f15, d10hat_loader)\n",
    "print({f15_acc_d10hat})\n",
    "f15_acc_d11hat = calculate_accuracy(model_f15, d11hat_loader)\n",
    "print({f15_acc_d11hat})\n",
    "f15_acc_d12hat = calculate_accuracy(model_f15, d12hat_loader)\n",
    "print({f15_acc_d12hat})\n",
    "f15_acc_d13hat = calculate_accuracy(model_f15, d13hat_loader)\n",
    "print({f15_acc_d13hat})\n",
    "f15_acc_d14hat = calculate_accuracy(model_f15, d14hat_loader)\n",
    "print({f15_acc_d14hat})\n",
    "f15_acc_d15hat = calculate_accuracy(model_f15, d15hat_loader)\n",
    "print({f15_acc_d15hat})\n",
    "accuracy_matrix.append([f15_acc_d1hat, f15_acc_d2hat, f15_acc_d3hat, f15_acc_d4hat, f15_acc_d5hat, f15_acc_d6hat, f15_acc_d7hat, f15_acc_d8hat, f15_acc_d9hat, f15_acc_d10hat, f15_acc_d11hat, f15_acc_d12hat, f15_acc_d13hat, f15_acc_d14hat, f15_acc_d15hat,None,None,None,None,None])\n",
    "\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D16) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f16_weights.pth'):\n",
    "    model_f16 = copy.deepcopy(model)\n",
    "    model_f16.load_state_dict(torch.load('/kaggle/input/weight-files/model_f16_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f16 = self_train_update_with_t2pl(model_f15, d16_data, 16)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f16_acc_d1hat = calculate_accuracy(model_f16, d1hat_loader)\n",
    "print({f16_acc_d1hat})\n",
    "f16_acc_d2hat = calculate_accuracy(model_f16, d2hat_loader)\n",
    "print({f16_acc_d2hat})\n",
    "f16_acc_d3hat = calculate_accuracy(model_f16, d3hat_loader)\n",
    "print({f16_acc_d3hat})\n",
    "f16_acc_d4hat = calculate_accuracy(model_f16, d4hat_loader)\n",
    "print({f16_acc_d4hat})\n",
    "f16_acc_d5hat = calculate_accuracy(model_f16, d5hat_loader)\n",
    "print({f16_acc_d5hat})\n",
    "f16_acc_d6hat = calculate_accuracy(model_f16, d6hat_loader)\n",
    "print({f16_acc_d6hat})\n",
    "f16_acc_d7hat = calculate_accuracy(model_f16, d7hat_loader)\n",
    "print({f16_acc_d7hat})\n",
    "f16_acc_d8hat = calculate_accuracy(model_f16, d8hat_loader)\n",
    "print({f16_acc_d8hat})\n",
    "f16_acc_d9hat = calculate_accuracy(model_f16, d9hat_loader)\n",
    "print({f16_acc_d9hat})\n",
    "f16_acc_d10hat = calculate_accuracy(model_f16, d10hat_loader)\n",
    "print({f16_acc_d10hat})\n",
    "f16_acc_d11hat = calculate_accuracy(model_f16, d11hat_loader)\n",
    "print({f16_acc_d11hat})\n",
    "f16_acc_d12hat = calculate_accuracy(model_f16, d12hat_loader)\n",
    "print({f16_acc_d12hat})\n",
    "f16_acc_d13hat = calculate_accuracy(model_f16, d13hat_loader)\n",
    "print({f16_acc_d13hat})\n",
    "f16_acc_d14hat = calculate_accuracy(model_f16, d14hat_loader)\n",
    "print({f16_acc_d14hat})\n",
    "f16_acc_d15hat = calculate_accuracy(model_f16, d15hat_loader)\n",
    "print({f16_acc_d15hat})\n",
    "f16_acc_d16hat = calculate_accuracy(model_f16, d16hat_loader)\n",
    "print({f16_acc_d16hat})\n",
    "accuracy_matrix.append([f16_acc_d1hat, f16_acc_d2hat, f16_acc_d3hat, f16_acc_d4hat, f16_acc_d5hat, f16_acc_d6hat, f16_acc_d7hat, f16_acc_d8hat, f16_acc_d9hat, f16_acc_d10hat, f16_acc_d11hat, f16_acc_d12hat, f16_acc_d13hat, f16_acc_d14hat, f16_acc_d15hat, f16_acc_d16hat,None,None,None,None])\n",
    "\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D17) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f17_weights.pth'):\n",
    "    model_f17 = copy.deepcopy(model)\n",
    "    model_f17.load_state_dict(torch.load('/kaggle/input/weight-files/model_f17_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f17 = self_train_update_with_t2pl(model_f16, d17_data, 17)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f17_acc_d1hat = calculate_accuracy(model_f17, d1hat_loader)\n",
    "print({f17_acc_d1hat})\n",
    "f17_acc_d2hat = calculate_accuracy(model_f17, d2hat_loader)\n",
    "print({f17_acc_d2hat})\n",
    "f17_acc_d3hat = calculate_accuracy(model_f17, d3hat_loader)\n",
    "print({f17_acc_d3hat})\n",
    "f17_acc_d4hat = calculate_accuracy(model_f17, d4hat_loader)\n",
    "print({f17_acc_d4hat})\n",
    "f17_acc_d5hat = calculate_accuracy(model_f17, d5hat_loader)\n",
    "print({f17_acc_d5hat})\n",
    "f17_acc_d6hat = calculate_accuracy(model_f17, d6hat_loader)\n",
    "print({f17_acc_d6hat})\n",
    "f17_acc_d7hat = calculate_accuracy(model_f17, d7hat_loader)\n",
    "print({f17_acc_d7hat})\n",
    "f17_acc_d8hat = calculate_accuracy(model_f17, d8hat_loader)\n",
    "print({f17_acc_d8hat})\n",
    "f17_acc_d9hat = calculate_accuracy(model_f17, d9hat_loader)\n",
    "print({f17_acc_d9hat})\n",
    "f17_acc_d10hat = calculate_accuracy(model_f17, d10hat_loader)\n",
    "print({f17_acc_d10hat})\n",
    "f17_acc_d11hat = calculate_accuracy(model_f17, d11hat_loader)\n",
    "print({f17_acc_d11hat})\n",
    "f17_acc_d12hat = calculate_accuracy(model_f17, d12hat_loader)\n",
    "print({f17_acc_d12hat})\n",
    "f17_acc_d13hat = calculate_accuracy(model_f17, d13hat_loader)\n",
    "print({f17_acc_d13hat})\n",
    "f17_acc_d14hat = calculate_accuracy(model_f17, d14hat_loader)\n",
    "print({f17_acc_d14hat})\n",
    "f17_acc_d15hat = calculate_accuracy(model_f17, d15hat_loader)\n",
    "print({f17_acc_d15hat})\n",
    "f17_acc_d16hat = calculate_accuracy(model_f17, d16hat_loader)\n",
    "print({f17_acc_d16hat})\n",
    "f17_acc_d17hat = calculate_accuracy(model_f17, d17hat_loader)\n",
    "print({f17_acc_d17hat})\n",
    "accuracy_matrix.append([f17_acc_d1hat, f17_acc_d2hat, f17_acc_d3hat, f17_acc_d4hat, f17_acc_d5hat, f17_acc_d6hat, f17_acc_d7hat, f17_acc_d8hat, f17_acc_d9hat, f17_acc_d10hat, f17_acc_d11hat, f17_acc_d12hat, f17_acc_d13hat, f17_acc_d14hat, f17_acc_d15hat, f17_acc_d16hat, f17_acc_d17hat,None,None,None])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D18) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f18_weights.pth'):\n",
    "    model_f18 = copy.deepcopy(model)\n",
    "    model_f18.load_state_dict(torch.load('/kaggle/input/weight-files/model_f18_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f18 = self_train_update_with_t2pl(model_f17, d18_data, 18)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f18_acc_d1hat = calculate_accuracy(model_f18, d1hat_loader)\n",
    "print({f18_acc_d1hat})\n",
    "f18_acc_d2hat = calculate_accuracy(model_f18, d2hat_loader)\n",
    "print({f18_acc_d2hat})\n",
    "f18_acc_d3hat = calculate_accuracy(model_f18, d3hat_loader)\n",
    "print({f18_acc_d3hat})\n",
    "f18_acc_d4hat = calculate_accuracy(model_f18, d4hat_loader)\n",
    "print({f18_acc_d4hat})\n",
    "f18_acc_d5hat = calculate_accuracy(model_f18, d5hat_loader)\n",
    "print({f18_acc_d5hat})\n",
    "f18_acc_d6hat = calculate_accuracy(model_f18, d6hat_loader)\n",
    "print({f18_acc_d6hat})\n",
    "f18_acc_d7hat = calculate_accuracy(model_f18, d7hat_loader)\n",
    "print({f18_acc_d7hat})\n",
    "f18_acc_d8hat = calculate_accuracy(model_f18, d8hat_loader)\n",
    "print({f18_acc_d8hat})\n",
    "f18_acc_d9hat = calculate_accuracy(model_f18, d9hat_loader)\n",
    "print({f18_acc_d9hat})\n",
    "f18_acc_d10hat = calculate_accuracy(model_f18, d10hat_loader)\n",
    "print({f18_acc_d10hat})\n",
    "f18_acc_d11hat = calculate_accuracy(model_f18, d11hat_loader)\n",
    "print({f18_acc_d11hat})\n",
    "f18_acc_d12hat = calculate_accuracy(model_f18, d12hat_loader)\n",
    "print({f18_acc_d12hat})\n",
    "f18_acc_d13hat = calculate_accuracy(model_f18, d13hat_loader)\n",
    "print({f18_acc_d13hat})\n",
    "f18_acc_d14hat = calculate_accuracy(model_f18, d14hat_loader)\n",
    "print({f18_acc_d14hat})\n",
    "f18_acc_d15hat = calculate_accuracy(model_f18, d15hat_loader)\n",
    "print({f18_acc_d15hat})\n",
    "f18_acc_d16hat = calculate_accuracy(model_f18, d16hat_loader)\n",
    "print({f18_acc_d16hat})\n",
    "f18_acc_d17hat = calculate_accuracy(model_f18, d17hat_loader)\n",
    "print({f18_acc_d17hat})\n",
    "f18_acc_d18hat = calculate_accuracy(model_f18, d18hat_loader)\n",
    "print({f18_acc_d18hat})\n",
    "accuracy_matrix.append([f18_acc_d1hat, f18_acc_d2hat, f18_acc_d3hat, f18_acc_d4hat, f18_acc_d5hat, f18_acc_d6hat, f18_acc_d7hat, f18_acc_d8hat, f18_acc_d9hat, f18_acc_d10hat, f18_acc_d11hat, f18_acc_d12hat, f18_acc_d13hat, f18_acc_d14hat, f18_acc_d15hat, f18_acc_d16hat, f18_acc_d17hat, f18_acc_d18hat,None,None])\n",
    "\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D19) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f19_weights.pth'):\n",
    "    model_f19 = copy.deepcopy(model)\n",
    "    model_f19.load_state_dict(torch.load('/kaggle/input/weight-files/model_f19_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f19 = self_train_update_with_t2pl(model_f18, d19_data, 19)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f19_acc_d1hat = calculate_accuracy(model_f19, d1hat_loader)\n",
    "print({f19_acc_d1hat})\n",
    "f19_acc_d2hat = calculate_accuracy(model_f19, d2hat_loader)\n",
    "print({f19_acc_d2hat})\n",
    "f19_acc_d3hat = calculate_accuracy(model_f19, d3hat_loader)\n",
    "print({f19_acc_d3hat})\n",
    "f19_acc_d4hat = calculate_accuracy(model_f19, d4hat_loader)\n",
    "print({f19_acc_d4hat})\n",
    "f19_acc_d5hat = calculate_accuracy(model_f19, d5hat_loader)\n",
    "print({f19_acc_d5hat})\n",
    "f19_acc_d6hat = calculate_accuracy(model_f19, d6hat_loader)\n",
    "print({f19_acc_d6hat})\n",
    "f19_acc_d7hat = calculate_accuracy(model_f19, d7hat_loader)\n",
    "print({f19_acc_d7hat})\n",
    "f19_acc_d8hat = calculate_accuracy(model_f19, d8hat_loader)\n",
    "print({f19_acc_d8hat})\n",
    "f19_acc_d9hat = calculate_accuracy(model_f19, d9hat_loader)\n",
    "print({f19_acc_d9hat})\n",
    "f19_acc_d10hat = calculate_accuracy(model_f19, d10hat_loader)\n",
    "print({f19_acc_d10hat})\n",
    "f19_acc_d11hat = calculate_accuracy(model_f19, d11hat_loader)\n",
    "print({f19_acc_d11hat})\n",
    "f19_acc_d12hat = calculate_accuracy(model_f19, d12hat_loader)\n",
    "print({f19_acc_d12hat})\n",
    "f19_acc_d13hat = calculate_accuracy(model_f19, d13hat_loader)\n",
    "print({f19_acc_d13hat})\n",
    "f19_acc_d14hat = calculate_accuracy(model_f19, d14hat_loader)\n",
    "print({f19_acc_d14hat})\n",
    "f19_acc_d15hat = calculate_accuracy(model_f19, d15hat_loader)\n",
    "print({f19_acc_d15hat})\n",
    "f19_acc_d16hat = calculate_accuracy(model_f19, d16hat_loader)\n",
    "print({f19_acc_d16hat})\n",
    "f19_acc_d17hat = calculate_accuracy(model_f19, d17hat_loader)\n",
    "print({f19_acc_d17hat})\n",
    "f19_acc_d18hat = calculate_accuracy(model_f19, d18hat_loader)\n",
    "print({f19_acc_d18hat})\n",
    "f19_acc_d19hat = calculate_accuracy(model_f19, d19hat_loader)\n",
    "print({f19_acc_d19hat})\n",
    "accuracy_matrix.append([f19_acc_d1hat, f19_acc_d2hat, f19_acc_d3hat, f19_acc_d4hat, f19_acc_d5hat, f19_acc_d6hat, f19_acc_d7hat, f19_acc_d8hat, f19_acc_d9hat, f19_acc_d10hat, f19_acc_d11hat, f19_acc_d12hat, f19_acc_d13hat, f19_acc_d14hat, f19_acc_d15hat, f19_acc_d16hat, f19_acc_d17hat, f19_acc_d18hat, f19_acc_d19hat,None])\n",
    "\n",
    "\n",
    "\n",
    "# Train on Unlabeled Data (D20) using self-training and distillation\n",
    "if os.path.exists('/kaggle/input/weight-files/model_f20_weights.pth'):\n",
    "    model_f20 = copy.deepcopy(model)\n",
    "    model_f20.load_state_dict(torch.load('/kaggle/input/weight-files/model_f20_weights.pth'))\n",
    "    print(\"Loaded self-trained model.\")\n",
    "else:\n",
    "    model_f20 = self_train_update_with_t2pl(model_f19, d20_data, 20)\n",
    "    \n",
    "print(\"started_accuracy_computation\")\n",
    "f20_acc_d1hat = calculate_accuracy(model_f20, d1hat_loader)\n",
    "print({f20_acc_d1hat})\n",
    "f20_acc_d2hat = calculate_accuracy(model_f20, d2hat_loader)\n",
    "print({f20_acc_d2hat})\n",
    "f20_acc_d3hat = calculate_accuracy(model_f20, d3hat_loader)\n",
    "print({f20_acc_d3hat})\n",
    "f20_acc_d4hat = calculate_accuracy(model_f20, d4hat_loader)\n",
    "print({f20_acc_d4hat})\n",
    "f20_acc_d5hat = calculate_accuracy(model_f20, d5hat_loader)\n",
    "print({f20_acc_d5hat})\n",
    "f20_acc_d6hat = calculate_accuracy(model_f20, d6hat_loader)\n",
    "print({f20_acc_d6hat})\n",
    "f20_acc_d7hat = calculate_accuracy(model_f20, d7hat_loader)\n",
    "print({f20_acc_d7hat})\n",
    "f20_acc_d8hat = calculate_accuracy(model_f20, d8hat_loader)\n",
    "print({f20_acc_d8hat})\n",
    "f20_acc_d9hat = calculate_accuracy(model_f20, d9hat_loader)\n",
    "print({f20_acc_d9hat})\n",
    "f20_acc_d10hat = calculate_accuracy(model_f20, d10hat_loader)\n",
    "print({f20_acc_d10hat})\n",
    "f20_acc_d11hat = calculate_accuracy(model_f20, d11hat_loader)\n",
    "print({f20_acc_d11hat})\n",
    "f20_acc_d12hat = calculate_accuracy(model_f20, d12hat_loader)\n",
    "print({f20_acc_d12hat})\n",
    "f20_acc_d13hat = calculate_accuracy(model_f20, d13hat_loader)\n",
    "print({f20_acc_d13hat})\n",
    "f20_acc_d14hat = calculate_accuracy(model_f20, d14hat_loader)\n",
    "print({f20_acc_d14hat})\n",
    "f20_acc_d15hat = calculate_accuracy(model_f20, d15hat_loader)\n",
    "print({f20_acc_d15hat})\n",
    "f20_acc_d16hat = calculate_accuracy(model_f20, d16hat_loader)\n",
    "print({f20_acc_d16hat})\n",
    "f20_acc_d17hat = calculate_accuracy(model_f20, d17hat_loader)\n",
    "print({f20_acc_d17hat})\n",
    "f20_acc_d18hat = calculate_accuracy(model_f20, d18hat_loader)\n",
    "print({f20_acc_d18hat})\n",
    "f20_acc_d19hat = calculate_accuracy(model_f20, d19hat_loader)\n",
    "print({f20_acc_d19hat})\n",
    "f20_acc_d20hat = calculate_accuracy(model_f20, d20hat_loader)\n",
    "print({f20_acc_d20hat})\n",
    "accuracy_matrix.append([f20_acc_d1hat, f20_acc_d2hat, f20_acc_d3hat, f20_acc_d4hat, f20_acc_d5hat, f20_acc_d6hat, f20_acc_d7hat, f20_acc_d8hat, f20_acc_d9hat, f20_acc_d10hat, f20_acc_d11hat, f20_acc_d12hat, f20_acc_d13hat, f20_acc_d14hat, f20_acc_d15hat, f20_acc_d16hat, f20_acc_d17hat, f20_acc_d18hat, f20_acc_d19hat, f20_acc_d20hat])\n",
    "\n",
    "accuracy_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rand Mix using Ada In "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6049882,
     "sourceId": 9858288,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6051933,
     "sourceId": 9860877,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6052197,
     "sourceId": 9861222,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6170638,
     "sourceId": 10021063,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
